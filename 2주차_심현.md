# 4.파이토치 심화
## 과대적합과 과소적합
과대적합(Overfitting)과 과소적합(Underfitting)은 머신러닝 모델에서 자주 발생하는 일반적인 문제입니다.

과대적합이란 모델이 훈련데이터에서 우수하게 예측하지만, 새로운 데이터에서는 제대로 예측하지 못 해 오차가 크게 발생하는 것을 의미합니다.

과소적합이란 과대적합의 문제점처럼 입력된 데이터를 잘 예측할 수 없는 상태를 의미합니다.

하지만 과대적합과는 다르게 훈련 데이터에서도 성능이 좋지 않고 새로운 데이터에서도 성능이 좋지 않습니다.

### 성능 저하

과대적합과 과소적합은 기본적으로 모델의 성능을 저하시킵니다.

과대적합은 훈련 데이터에서는 잘 수행되는 것처럼 보여도 새로운 데이터에서는 제대로 값을 예측하지 못 합니다.

과소적합은 새로운 데이터에 대해 예측을 수행할 수 없으며, 전반적으로 모델의 성능이 좋지 못할 때 발생합니다.

### 모델 선택 실패

과대적합과 과소적합은 모델을 변경해 문제를 완화할 수 있습니다.

과대적합의 경우 모델의 구조가 너무 복잡해 훈련 데이터에만 의존하게 되어 성능이 저하됩니다.

과소적합의 경우 모델의 구조가 너무 단순해 데이터의 특징을 제대로 학습하지 못한 경우로 볼 수 있습니ㅏㄷ.

이에 따라 모델을 변경하거나 모델 구조를 개선해야 합니다.

### 편향-분산 트레리드오프

모델이 훈련 데이터와 새로운 데이터에 대해서도 우수한 성능을 보이려면 낮은 편향과 낮은 분산을 가져야합니다.

분산이 높으면 추정치(Estimate)에 대한 변동 폭이 커지며, 데이터가 갖고 있는 노이즈까지 학습과정에 포함돼 과대적합 문제를 발생합니다.

편향이 높으면 추정치가 항상 일정한 값을 갖게 될 확률이 높아져 데이터의 특징을 제대로 학습하지 못 해 과소접합 문제를 발생시킵니다.

편향과 분산은 서로 반비례하여 분산을 감소시키면 편향이 증가하고, 분산을 증가시키면 편향이 감소합니다.

![image (2).png](../여름학기_딥러닝_공부/image%20(2).png)

모델의 성능을 높이기 위해 편향과 분산을 절충해 높은 성능을 이끌어 끌어내야 합니다.

모델이 복잡해질수록 분산은 커지고 편향은 작아집니다.

반대로 모델이 단순해질수록 분산은 작아지고 편향은 커집니다.

오류를 최소화하기 위해 분산과 편향의 균형을 맞춰야합니다.

### 과대적합과 과소적합 문제 해결

과대적합은 모델의 일반화(Generalization) 능력을 저하해 문제가 방생합니다.

과소적합은 모델이 데이터의 특징을 제대로 학습할 수 없을 때 발생합니다.

두 문제 모두 모델에 새로운 데이터를 예측했을 때 우수한 결과를 얻을 수 있습니다.

#### 데이터 수집

과대적합과 과소적합 모두 모델이 훈련 데이터를 제대로 학습하지 못 한 경우입니다.

과대적합은 훈련 데이터를 너무 적합하게 학습해 문제가 발생합니다.

과소적합은 훈련 데이터를 제대로 학습하지 못 해 발생합니다.

모델이 훈련 데이터에 노이즈를 학습하지 않으면서 일반적인 규칙을 찾을 수 있게 학습 데이터의 수를 늘립니다.

#### 피처 엔지니어링

신규 데이터 수집이 어려운 경우라면 기존 훈련 데이터에서 변수나 특징을 추출하거나 피처를 더 작은 차원으로 축소합니다.

모델이 더 학습하기 쉬운 형태로 데이터를 변환하면 노이즈에 더 건강한 모델을 구축할 수 있습니다.

#### 모델 변경

과대적합이나 과소적합이 발생하는 주요한 이유는 훈련 데이터세트에 비해 너무 강력한 모델을 사용하거나 너무 간단한 모델을 사용하기 떄문입니다.

과대적합은 학습 데이터에 비해 강력한 모델을 사용하는 경우 발생합니다.

강력한 모델의 경우 깊은(Deep) 구조의 모델일 가능성이 높습니다.

모델의 매개변수가 많기 때문에 적은 양의 데이터에도 학습한다면 효과적으로 모델을 학습할 수 없습니다.

이 경우 모델의 계층을 축소하거나 더 간단한 모델로 변경해야 합니다.

과소적합은 학습 데이터에 비해 간단한 모델을 사용할 때 발생합니다.

모델의 구조가 간단하면 데이터의 특징을 제대로 학습할 수 없습니다.

이 경우 모델의 계층을 확장하거나 더 복잡한 모델로 변경해야 합니다.

#### 조기 중단

모델 학습 시 검증 데이터세트로 성능을 지속적으로 평가해 모델의 성능이 저하되기 전에 모델 학습을 조기 중단(Eary Stopping)하는 방법입니다.

과대적합이 발생하기전에 모델의 학습을 중단시키는 방법입니다.

#### 배치 정규화

모델 배치 정규화(Batch Normalization)를 적용해 모델 성능과 모델 안정성을 향상시킵니다.

모델의 계층마다 평균과 분산을 조정해 내부 공변량 변화를 줄여 과대적합을 방지합니다.

배치 정규화(Batch Normalization)란 내부 공변량 변화(Internal Covariate Shift)를 줄여 과대 적합을 방지하는 기술입니다.

일반적으로 인공 신경망을 학습할 때 입력값을 배치 단위로 나눠 학습을 진행합니다.

배치 단위로 나눠 학습하는 경우 상위 계층의 매개변수가 갱신될 떄마다 현재 계층에 전달되는 데이터의 분포도 변경됩니다.

각 계층은 배치 단위의 데이터로 인해 계속 변화되는 입력 분포를 학습해야 하기 때문에 인공 신경망의 성능과 안정성이 낮아져 학습 속도가 느려집니다.

내부 공변량 변화란 계층마다 분포가 변경되는 현상을 의미합니다.

내부 공변량 변화가 발생하는 경우 은닉층에서 다음 은닉층으로 전달될 때 입력값이 균일해지지 않아 가중치가 제대로 갱신되지 않을 수 있습니다.

이로 인해 학습이 불안정해지고 느려져 가중치가 일정한 값으로 수렴하기 어려워 집니다.

또한 초기 가중치 값에 민감해져 일반화 하기가 어려워져 더 많은 학습 데이터를 요구하게 됩니다.

이러한 문제를 해결하기 위해 각 계층에 배치 정규화를 적용합니다.

배치 정규화는 미니 배치의 입력을 정규화하는 방식으로 동작합니다.

미니 배치에 전달되는 입력값이 [100, 1, 1]이거나 [1, 0.01, 0.01]이라면 두 배열의 값 모두 [1.4142, -0.7071, -0.7071]로 정규화 됩니다.

배치 정규화를 적용하면 각 계층에 대한 입력이 일반화되고 독립적으로 정규화가 수행되므로 더 빠르게 값을 수렴할 수 있습니다.

입력이 정규화 되므로 초기 가중치에 대한 영향을 줄일 수 있습니다.

### 정규화 종류

배치 정규화는 이미지 분류 모델에서 배치 정규화 적용시 14배 더 적은 학습으로도 동일한 정확도를 달성할 수 있으며 더 높은 학습률을 적용해 학습 시간을 최소화할 수 있습니다.

배치 정규화 이외에도 계층 정규화 (Layer Normalization), 인스턴스 정규화(Instance Normalization), 그룹 정규화(Group Normalization)가 있습니다.

![image (3).png](../여름학기_딥러닝_공부/image%20(3).png)

이 사진에서 블록을 이미지 데이터나 텍스트 데이터라고 가정한다면 차원(Dimensions)은 이미지 데이터의 크기(너비, 높이)나 어휘 사전의 크기가 됩니다.

채널(Channels)의 경우 이미지 데이터의 채널이나 시간 간격*Timestep)으로 볼 수 있습니다.

데이터 종류에 따라 축의 의미가 달라질 수 있으나, 정규화 처리 방식은 동일합니다.

배치 정규화는 미치 배치에서 계산된 평균 및 분산을 기반으로 계층의 입력을 정규화 합니다.

계층 정규화의 경우 이미지 데이터 전체를 대상으로 정규화를 수행하지 않고 각각의 이미지 데이터에 채널별로 정규화를 수행합니다.

배치 정규화는 컴퓨터비전과 관련된 모델 중 합성곱 신경망(CNN)이나 다층 퍼셉트론(MLP)과 같은 순방향 신경망(Feedforward Neural Network)에서 주로 사용됩니다.

계층 정규화의 경우 미니 배치의 샘플 전체를 계산하는 방법이 아닌, 채널 축으로 계산되기 때문에 미니 배치 샘플간의 의존관계가 없습니다.

그러므로 샘플이 서로 다른 길이를 가지더라도 정규화를 수행할 수 있습니다.

계층 정규화는 신경망 모델 중 자연어 처리에 주로 사용되며 순환 신경망(RNN)이나 트랜스포머기반 모델에서 주로 사용됩니다.

인스턴스 정규화 채널과 샘플을 기준으로 정규화를 수행합니다.

정규화가 각 샘플에 대해 개별적으로 수행되므로 입력이 다른 분포를 갖는 작업에 적합합니다.

그러므로 인스턴스 정규화는 생성적 적대 신경망(GAN)이나 이미지의 스타일으 변환하는 스타일 변환(Style Trnsfer) 모델에서 주로 사용됩니다.

그룹 정규화는 채널을 N개의 그룹으로 나누고 각 그롭 내에서 정규화를 수행합니다.

그룹을 하나로 설명하면 인스턴스 정규화와 동일하며, 그룹의 개수를 채널의 개수와 동일하게 설명하면 계층 정규화와 동일한 기능을 합니다.

그룹 정규화는 배치 크기가 작거나 채널 수가 매우 많은 경우에 주로 사용됩니다.

합성곱 신경망(CNN)의 배치 크기가 작으면 배치 정규화가 배치의 평균과 분산이 데이터세트를 대표한다고 보기 어렵기 때문에 배치 정규화의 대안으로 사용됩니다.

### 배치 정규화 풀이

배치 정규화는 미니 배치의 평균과 분산을 계산해 정규화를 수행합니다.

o배치 정규화의 식은 다음과 같습니다.

$$
y_i = \frac{x_i - E[X]}{\sqrt{Var[X] + \varepsilon}}* \ \gamma  + \beta
$$

$x_i$ 는 입력값, $y_i$는 배치 정규화가 적용된 결과값, $E[X]$는 산술 평균(Arithmetic Mean), $Var[X]$는 분산(Varinace), X는 전체 모집단과 미니 배치의 은닉층 출력값 $\varepsilon$은 분모가 0이 되는 경우를 방지하는 작은 상수입니다. 기본값으로는 $10^{-5}(0.00001)$입니다.

$\gamma$와 $\beta$는 학습 가능한 매개변수입니다.

활성화 함수에서 발생하는 음수의 영역을 처리할 수 있게 값을 조절하는 스케일(Scale) 값과 시프트(Shift)값입니다.

주로 $\gamma$의 초깃값은 1, $\beta$의 초깃값은 0으로 할당됩니다.

```python
x = torch.FloatTensor(
	[
		[-0.6577, -0.5797, 0.6360],
    [0.7392, 0.2145, 1.523],
    [0.2432, 0.5662, 0.322]
	]
)
```

이 코드를 수식으로 작성하면

$$
\begin{matrix}
X_1 &=& [-0.6577, -0.5797, 0.6360]\\
X_2 &=& [0.7392, 0.2145, 1.523]\\
X_3 &=& [0.2432, 0.5662, 0.322]
\end{matrix}
$$

이렇게 표현이 됩니다.

먼저 $X_1$의 $x_1, x_2, x_3$ 값에 배치 정규화를 적용합니다.

배치 정규화를 계산하려면 산술 평균과 분산을 계산해야 합니다.

$$
\begin{matrix}
E[X] &=& \frac{-0.6577 + 0.7392+0.2432}{3}\\
&\simeq& 0.1082
\end{matrix}
$$

$$
\begin{matrix}
Var[X] &=& \frac{(E[X] + 0.6577)^2 \ + \ (E[X] + 0.739)^2 \ + \ (E[X] + 0.2432)^2}{3}\\
&=& \frac{(0.1082 + 0.6577)^2 \ + \ (0.1082 + 0.739)^2 \ + \ (0.1082 + 0.2432)^2}{3}\\
&\simeq& 0.3343
\end{matrix}
$$

평균과 부산에 대한 계산을 완료했다면, 배치 정규화 수식을 적용해 새로운 값을 할당합니다.

먼저 $x_1$ 값에 대한 배치 정규화를 수행합니다.

$$
\begin{align*}
y_1 &= \frac{x_i - E[X]}{\sqrt{Var[X]} + \varepsilon} \times \gamma + \beta \\
&= \frac{-0.6577 - 0.1082}{\sqrt{0.3343} + \varepsilon} \times \gamma + \beta \\
&= \frac{-0.6577 - 0.1082}{\sqrt{0.3343 + 0.00001}} \times 1 + 0 \\
&= -1.3246
\end{align*}
$$

이와 동일하게 $x_2, x_3$에도 배치정규화를 수행합니다.

$X_1$에 대한 배치 정규화를 완료했다면 $Y_1$을 계산할 수 있습니다.

$$
Y_1 = [-1.3246, 1.0912, 0.2334]
$$

$Y_1$에 대해 다시 평균과 분산을 계산한다면 평균은 0.0 분산은 1.0으로 졍규화합니다.

$X_2, X_3$ 값도 동일하게 배치 정규화를 수행하면 다음과 같아집니다.

$$
\begin{matrix}
Y_1 &=& [-1.3246, 1.0912, 0.2334]\\
Y_2 &=& [-1.3492, 0.3077, 1.0415]\\
Y_3 &=& [-0.3756, 1.3685, -0.9930]
\end{matrix}
$$

```python
import torch
from torch import nn

x = torch.FloatTensor(
    [
        [-0.6577, -0.5797, 0.6360],
        [0.7392, 0.2145, 1.523],
        [0.2432, 0.5662, 0.322]
    ]
)

print(nn.BatchNorm1d(3)(x))
```

출력결과

```python
tensor([[-1.3246, -1.3492, -0.3756],
        [ 1.0912,  0.3077,  1.3685],
        [ 0.2334,  1.0415, -0.9930]], grad_fn=<NativeBatchNormBackward0>)
```

$Y_1, Y_2, Y_3$의 배치 정규화의 최종 결과와 동일한 값을 반환합니다.

배치 정규화는 간단하게 배치 정규화 클래스를 통해 수행할 수 있습니다.

```python
m = torhc.nn.BatchNorm1d(
	num_features,
	eps = 1e-05
)
```

1차원 배치 정규화 클래스는 특징 개수(num_features)를 입력받아 배치 정규화를 수행합니다.

특징 개수는 텐서의 특징 개수를 의미하며 입력 데이터의 채널 수가 됩니다.

eps는 분모가 0이되는 현상을 방지하는 작은 상수입니다.

일반적으로 정규화는 모델 학습 중에만 적용됩니다.

채널이 포함된 입력 데이터를 정규화한다면 BatchNrom2d 클래스를 사용하면됩니다.

이 외에 도 LayerNrom 클래스나 InstanceNorm1d 클래스 등이 있습니다.

| <center> 정규화 종류  | <center> 클래스 | <center> 의미 |
| --- | --- | --- |
| <center> 배치 정규화 | <center> torch.nn.BatchNorm1d(num_features) | <center> 2D/3D 입력 데이터에 배치 정규화 수행 |
| <center> 배치 정규화 | <center> torch.nn.BatchNorm2d(num_features) | <center> 4D 입력 데이터에 배치 정규화 수행 |
| <center> 배치 정규화 | <center> torch.nn.BatchNrom3d(num_features) | <center> 5D 입력 데이터에 배치 정규화 수행 |
| <center> 계층 정규화 | <center> torch.nn.LayerNrom(normalized_shape) | <center> 정규화하려는 차원 크기로 계층 정규화 수행 |
| <center> 인스턴스 정규화 | <center> torch.nn.InstanceNorm1d(num_features) | <center> 2D/3D 입력 데이터에 인스턴스 정규화 수행 |
| <center> 인스턴스 정규화 | <center> torch.nn.InstanceNorm2d(num_features) | <center> 4D 입력 데이터에 인스턴스 정규화 수행 |
| <center> 인스턴스 정규화 | <center> torch.nn.InstanceNorm3d(num_features) | <center> 5D 입력 데이터에 인스턴스 정규화 수행 |
| <center> 그룹 정규화 | <center> torch.nn.GroupNorm(num_groups, num_channels) | <center> 그룹과 채널을 나눠 정규화 수행 |

## 정칙화

정칙화(Regularization)란 모델 학습시 발생하는 과대적합 문제를 방지하기 위해 사용되는 기술입니다.

모델이 암기(Memorization)가 아니라 일반화(Generalization)할 수 있도록 손실 함수에 규제(Penalty)를 가하는 방식입니다.

암기란?

모델이 데이터의 특성이나 패턴을 학습하는 것이 아니라 훈련 데이터의 노이즈를 학습했을 때 발생합니다.

모델이 훈련 데이터에서는 잘 수행되지만, 새로운 데이터에서는 제대로 수행되지 못 하는 경우를 의미합니다.

→ 모델이 데이터의 일반적인 패턴을 학습하는거싱 아니라 학습 데이터의 노이즈나 특정 패턴을 학습하는 것 입니다.

일반화란 모델이 새로운 데이터에서도 정확한 예측을 할 수 있음을 의미합니다.

특정 데이터가 갖고 있는 노이즈를 학습하는것이 아닌 데이터의 일반적인 패턴을 학습했을 때 일반화된 모델이라 부릅니다.

일반화된 모델은 학습에 사용한 데이터와 약간 다르더라도 정확한 예측을 할 수 있습니다.

정칙화는 이러한 노이즈에 강건하고 일반화된 모델을 구축하기 위해 사용하는 방법입니다.

모델이 특정 피처나 특정 패턴에 너무 많은 비중을 할당하지 않도록 손실 함수에 규제를 가해 모델의 일반화 성능(Generalization Performance)을 향상시킵니다.

정칙화를 적용하면 학습 데이터들이 갖고 있는 작은 차이점에 대해 덜 민감해져 모델의 분산 값이 낮아집니다.

정칙화는 모델이 데이터를 학습할 때 의존하는 특징의 수를 줄임으로써 모델의 추론 능력을 개선합니다.

정칙화는 모델이 비교적 복잡하고 학습에 사용되는 데이터의 수가 적을 때 활용됩니다.

모델이 단순하다면 모델 매개변수의 수가 적어 정칙화가 필요하지 않습니다.

데이터의 수가 많거나 데이터가 잘 정제되어 노이즈가 없는 경우에는 사용하지 않습니다.

정칙화는 노이즈를 강건하게 만들어 일반화하는 데 사용하므로 이미 정규화되어 있는 경우엔느 사용하지 않습니다.

정칙화의 종류에는 L1 정칙화, L2 정칙화, 가중치 감쇠, 드롭아웃 등이 있습니다.

### L1 정칙화

L1 정칙화(L1 Regularization)는 라쏘 정칙화(Lasso Regularization)라고도 하며, L1 노름(L1 Nrom) 방식을 사용해 규제하는 방법입니다.

L1 노름은 벡터 또는 행렬값의 절댓값 합계를 계산합니다.

그래서 L1 정칙화는 손실 함수에 가중치 절댓값의 합을 추가해 과대적합을 방지합니다.

모델 학습은 비용이 0이 되는 방향으로 진행됩니다.

손실 함수에 값을 추가한다면 오차는 더 커질 수 밖에 없습니다.

모델은 추가된 값까지 최소화할 수 있는 방향으로 학습이 진행됩니다.

L1 정칙화는 손실 함수에 가중치 절댓값의 합으로 규제를 가하므로 모델은 가중치 절댓값의 합도 최소가 되는 방향으로 학습이 진행됩니다.

모델 학습 시 값이 크지 않은 가중치들은 0으로 수렴하게 되어 예측에 필요한 특징의 수가 줄어듭니다.

불필요한 가중치가 0이 되므로 L1 정칙화를 적용한 모델은 특징 선택(Feature Selection) 효과를 얻을 수 있습니다.

L1 정칙화 수식

$$
L_1 = \lambda \ * \ \sum_{i = 0}^{n} \left| w_i \right|
$$

$\lambda$는 규제 강도입니다.

너무 많거나 적은 규제를 가하지 않게 조절하는 하이퍼파라미터입니다.

규제 강도는 0보다 큰 값으로 설정합니다.

규제 강도가 0에 가까워질수록 모델은 더 많은 특징을 사용하기 때문에 과대적합에 민감합니다.

반대로 규제 강도를 너무 높이면 대부분의 가중치가 0에 수렴되기 때문에 과소적합 문제에 노출됩니다.

L1 정칙화는 모델의 가중치를 정확히 0으로 만드는 경우가 있으므로 희소한 모델이 될 수 있습니다.

불칠요한 특징을 처리하지 않으므로 모델의 성능이 올라갈 수 있지만, 예측에 사용되는 특징의 수가 줄어들게 되므로 정보의 손실로 이어질 수 있습니다.

L1 정칙화를 적용하는 경우 입력 데이터에 더 민감해지며, 항상 최적의 규제를 가하지 않으므로 사용에 주의 해야합니다.

L1 정칙화는 모델의 가중치 절댓값의 합을 사용합니다.

모델의 가중치를 모두 계산해 모델을 갱신해야 하므로 계산 복잡도(Computational Complexity)를 높이게 됩니다.

L1 정칙화는 미분할 수 없으므로 역전파를 계산하는데 더 많은 리소스를 소모합니다.

또한 하이퍼파라미터인 $\lambda$(lambda)의 값이 적절하지 않으면 가중치 값들이 너무 작아져 모델을 해석하기 더 어렵게 만들 수 있어 여러 번 반복해 최적의 $\lambda$(_lambda)의 값을 찾아야 합니다.

L1 정칙화는 주로 선형 모델에 적용합니다. 선형 회귀 모델에 L1 정칙화를 적용하는 것을 라쏘 회귀(Least Absolute Shrinkage and Selection Operator Regression)라 합니다.

### L2 정칙화

L2 정칙화(L2 Regularization)는 릿지 정칙화(Ridge Regularization)라고도 하며 L2 노름(L2 Norm) 방식을 사용해 규제하는 방법입니다.

L2 노름은 벡터 또는 행렬값의 크기를 계산합니다.

L2 정칙화는 손실 함수에 가중치 제곱의 합을 추가해 과대적합을 방지하도록 규제 합니다.

L2 정칙화도 L1 정칙화와 동일한 방식으로 모델에 규제를 가합니다.

하지만 L2 정칙화는 하나의 특징이 너무 중요한 요소가 되지 않도록 규제를 가하는 것에 의미를 둡니다.

L2 정칙화는 L1정칙화에 비해 가중치 값들이 비교적 균일하게 분포됩니다.

가중치를 0으로 만들지 않고 0에 가깝게 만들기 때문입니다.

모델 학습시 오차를 최소화하면서 가중치를 작게 유지하고 골고루 분포되게끔 하므로 모델의 복잡도가 일부 조정됩니다.

$$
L_2 = \lambda \ * \ \sum_{i = 0}^{n} \left|w_i^2 \right|
$$

L1 정칙화와 L2 정칙화에 평균 제곱 오차를 적용했을 때 $y = w \times x$의 결과의 차이가 있습니다.

다음 그림은 가중치가 0.5가 실젯값일 때 평균 제곱 오차와 L1 정칙화, L2 정칙화의 관계를 보여줍니다.

규제 강도는 0.5입니다.

![image (4).png](../여름학기_딥러닝_공부/image%20(4).png)

모델은 $y = 0.5x$이므로 평균제곱 오차의 손실이 가장 낮은 가중치 값은 0.5가 됩니다.

L1 정칙화는 가중치 걸잿값으로 계산되므로 가중치가 0에 가까워질수록 선형적인 구조를 가집니다.

L2 정칙화는 가중치 제곱으로 계산되므로 비선형적인 구조를 가집니다.

정칙화를 적용하는 방법은 손실함수에 규제 값을 더해주는 방법이 적용되므로 평균제곱 오차 값에 정칙화 값을 더합니다.

다음 그림은 가중치 별로 정칙화를 적용한 손실값을 보여줍니다.

![image (5).png](../여름학기_딥러닝_공부/image%20(5).png)

L1 정규화는 선형적인 특성을 가지므로 가중치가 0이 아닌곳에서는 모든 값에 고정적인 값을 추가합니다.

L2 정규화는 비선형적인 특성을 가지므로 가중치가 0에 가까워질수록 규젯값이 줄어듭니다.

L2 정칙화도 L1 정칙화를 적용하는 방법처럼 간단하게 사용할 수 있습니다.

L2 정칙화 역시 L1 정칙화에서 발생하는 계산 복잡도 문제가 발생합니다.

L2 정칙화는 모델 매개변수의 제곱 값을 계산하고 저장해야 하므로 L1 정칙화처럼 많은 리소스를 소모하게 됩니다.

역시 하이퍼파라미터인 $\lambda$(_lambda)도 여러 번 반복해 최적의 $\lambda$(_lambda)값을 찾아야 합니다.

|  | <center> L1 정칙화 | <center> L2 정칙화 |
| --- | --- | --- |
| <center> 계산 방식 | <center> 가중치 절댓값의 합 | <center> 가중치 제곱의 합 |
| <center> 모델링 | <center> 희소함(Sparese Solution) | <center> 희소하지 않음(Non-sparse Solution) |
| <center> 특징 선택 | <center> 있음 | <center> 없음 |
| <center> 이상치 | <center> 강함 | <center> 약함 |
| <center> 가중치 | <center> 0이 될 수 있음 | <center> 0에 가깝게 됨 |
| <center> 학습 | <center> 비교적 복잡한 데이터 패턴을 학습할 수 없음 | <center> 비교적 복잡한 데이터 패턴을 학습할 수 있음 |

### 가중치 감쇠

가중치 감쇠(Weight Decay)는 앞선 정칙화 방법과 마찬가지로 모델이 더 작은 가중치를 갖도록 손실 함수에 규제를 가하는 방법입니다.

일반적으로 가중치 감쇠가 L2 정칙화와 동의어로 사용되지만, 가중치 감쇠는 손실 함수에 규제 항을 추가하는 기술 자체를 의미합니다.

파이토치나 텐서플로와 같은 딥러닝 라이브러리에서는 이 용어가 최적화 함수에 적용하는 L2 정규화 의미로 사용됩니다.

파이토치의 가중치 감쇠는 L2 정규화와 동일하며 최적화 함수에서 weight_decay 하이퍼파라미터를 설정해 구현할 수 있습니다.

```python
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, weight_decay = 0.01)
```

키워드 매개변수(kwargs)에 포함되는 가중치 감쇠(weight_decay) 매개변수로 L2 정규화를 적용할 수 있습니다.

L2 정칙화를 간단하게 적용하는 방법이므로 L2 정칙화가 갖고 있는 장점과 단점을 그대로 포함합니다.

그러므로 가중치 감쇠 하이퍼파라미터를 조정해 사용하고, 조기 중지 또는 드롭아웃과 같은 기술과 함께 사용합니다.

### 모멘텀

모멘텀(Momentum)은 경사 하강법 알고리즘의 변형 중 하나로, 이전에 이동했던 방향과 기울기의 크기를 고려하여 가중치를 갱신합니다.

이를 위해 지수 가중 이동평균을 사용하며, 이전 기울기 일부를 현재 기울기 값에 추가해 가중치를 갱신합니다.

이전 기울기 값에 의해 설정된 방향으로 더 빠르게 이동합니다.

일종의 관성(Momentum) 효과를 얻을 수 있습니다.

v모멘섬 수식

$$
\begin{matrix}
v_i &=& \gamma v_{i - 1} + \alpha\nabla f(W_i)\\
W_{i+1} &=& W_i - v_i
\end{matrix}
$$

$v_i$는 $i$번째 모멘텀 값으로, 이동 벡터를 의미합니다.

$v_i$는 이전 모멘텀 값 $v_{i-1}$에 모멘텀 계수 $\gamma$를 곱한 값과 경사 하강법의 갱신 값의 합으로 계산됩니다.

$\gamma$는 하이퍼파라미터로, 모멘텀 계수를 의미합니다.

$v_i$는 가중치 갱신을 위한 방향과 크기를 결정하는데 사용되며, 이전 기울기 값이 크면 모멘텀 계수에 의해 현재 기울기 값에 반영되므로 더 빠르게 수렴할 수 있습니다.

모멘텀 계수는 0.0 ~ 1.0 사이의 값으로 설정할 수 있으며 일반적으로 0.9와 같은 값을 사용합니다.

모멘텀 계수를 0으로 설정한다면 $v_i = \alpha \nabla f(W_i)$가 되어 경사 하강법 수식과 동일해집니다.

파이토치의 모멘텀은 가중치 감쇠 적용 방법처럼 최적화 함수의 momentum 하이퍼파라미터를 설정해 구현할 수 있습니다.

### 엘라스틱 넷

엘라스틱 넷(Elasic-Net)은 L1 정칙화와 L2 정칙화를 결합해 사용하는 방식입니다.

L1 정칙화는 모델이 희박한 가중치를 갖게 규제하는 반면, L2 정칙화는 큰 가중치 갖지 않게 규제 합니다.

두 개의 정칙화 방식을 결합함으로써 희소성과 작은 가중치의 균형을 맞춥니다.

두 정칙화 방식의 선형 조합으로 사용하며 혼합 비율을 설정해 가중치를 규제 합니다.

혼합 비율은 $\alpha$로 어떤 정칙화를 더 많이 반영할지 설정합니다.

혼합 비율은 0에서 1사이의 값을 사용합니다.

엘라스틱 넷 수식

$$
Elastic \ - Net = \alpha \ \times \ L_1 \ + \ (1-\alpha) \ \times L_2
$$

혼합 비율을 1로 사용하면 L1 정칙화가 되며, 0으로 사용하면 L2 정칙화가 됩니다.

따라서 엘라스틱 넷은 L1, L2 정칙화보다 트레이드오프 문제를 더 유연하게 대처할 수 있습니다.

엘라스틱 넷은 특징의 수가 샘플의 수보다 더 많을 때 유의미한 결과를 가져옵니다.

상관관계가 있는 특징을 더 잘 처리할 수 있습니다.

두 정칙화를 결합해  사용함으로써 각 정칙화가 가진 장점을 최대한 활용할 수 있습니다.

하지만 균형적인 규제를 가하기 위해 새로운 하이퍼파라미터인 혼합 비율도 조정해야 하므로 더 많은 튜닝이 필요하게 됩니다.

L1, L2 정칙화 모둔 계산 복잡도 문제를 갖고 있으므로 더 많은 리소스를 소모합니다.

### 드롭아웃

드롭아웃(Dropout)은 정칙화 기법중 하나로, 모델의 훈련 과정에서 일부 노드를 일정 비율로 제거하거나 0으로 설정해 과대 적합을 방지하는 간단하고 효율적인 방법입니다.

과대적합을 발생시키는 이유 중 하나는 모델 학습시 발생하는 노드간 동조화(Co-adaptation) 현상입니다.

동조화 현상이란 모델 학습 중 특정 노드의 가중치나 편향이 큰 값을 갖게 되면 다른 큰 노드가 큰 값을 갖는 노드에 의존하는 것을 말합니다.

이러한 현상은 특정 노드에 의존성이 생겨 학습 속도가 느려지고 새로운 데이터를 예측하지 못 해 성능을 저하시킬 수 있습니다.

따라서 학습 과정에서 일부 노드를 제거해 노드간 의존성을 억제해야 합니다.

드롭아웃은 일부 노드를 제거해 학습을 진행하므로 동조화 현상을 방지할 수 있습니다.

모델이 일부 노드를 제거해 학습하므로 투표(Voting) 효과를 얻을 수 있어 모델 평균화(Model Averaging)가 됩니다.

하지만 모델 평균화 효과를 얻기 위해서 다른 드롭아웃 마스크를 사용해야 모델을 여러 번 훈련해야 하므로 훈련 시간이 늘어납니다.

모든 노드를 사용해 학습하지 않았으므로 데이터세트가 많지 않다면 효과를 얻기 어렵습니다.

충분한 데이터세트와 학습이 없다면 모든 노드가 균일하게 학습될 수 없으므로 성능이 저하될 수 있습니다.

그러므로 드롭아웃을 적용할 때는 충분한 데이터세트와 비교적 깊은 모델에 적용합니다.

```python
from torch import nn

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(10, 10)
        self.dropout = nn.Dropout(p = 0.5)
        self.layer2 = nn.Linear(10, 10)

    def forward(self, x):
        x = self.layer1(x)
        x = self.dropout(x)
        x = self.layer2(x)
        return x
```

p는 베르누이 분포(Bernoulli Distribution)의 모수를 의미하며, 이 분포로 각 노드의 제거 여부를 확률적으로 선택합니다.

이 과정은 순방향 메서드에서 드롭아웃을 적용할 계층 노드에 적용됩니다.

드롭아웃은 일반적으로 배치 정규화와 동시에 사용하지 않으므로 다른 기봅을 동시에 적용할 때 주의해서 적용해야합니다.

드롭아웃과 배치 정규화는 서로의 정칙화 효과를 방해할 수 있습니다.

배치 정규화의 경우 내부 공변량 변화를 줄여 과대적합을 방지합니다. 드롭아웃은 일부 노드를 제거합니다.

두 가지 방법을 동시에 사용하면 모델은 순방향 과정에서 다른 활성화 분포를 사용하게 됩니다.

이로 인해 훈련 과정에서 성능이 저하되거나 불안정해집니다.

그러므로 드롭아웃과 배치 정규화를 사용하는 경우에는 드롭아웃, 배치 정규화 순으로 적용합니다.

또한 드롭아웃은 배치 정규화와 마찬가지로 모델이 학습할 때만 적용되며 추론하는 과정에서는 일부 노드를 삭제하지 않고 모든 노드를 사용해 예측합니다.

드롭아웃은 극단적인 비율로 모델에 적용하지 않는다면 일반적으로 성능 향상의 이점을 얻을 수 있습니다.

비교적 많은 특징을 사용해 학습하는 이미지 인식이나 음성 인식 모델에서 성능이 향상되는 결과를 보이고 있습니다.

### 그레이디언트 클리핑

그레이디언트 클리핑(Gradient Clipping)은 모델을 학습할 때 기울기가 너무 커지는 현상을 방지하는데 사용되는 기술입니다.

과대적합 모델은 특정 노드의 가중치가 너무 크다는 특징을 갖습니다.

높은 가중치는 높은 분산 값을 갖게 하여 모델의 성능이 저하될 수 있습니다.

이러한 현상을 방지하기 위해 가중치 최댓값을 규제해 최대 임곗값을 초과하지 않도록 기울기를 잘라(Clipping) 설정한 임곗값으로 변경합니다.

그레이디언트 클리핑 수식

$$
w = r\frac{w}{\lVert w \rVert} \; if:\lVert w \rVert > r
$$

그레이디언트 클리핑은 가중치 노름이 최대 임곗값 $r$보다 높은 경우에 수행되야 합니다.

최대 임곗값을 넘는 경우 기울기 벡터의 방향을 유지하면서 기울기를 잘라 규제할 수 있습니다.

일반적으로 그레이디언트 클리핑은 L2 노름을 사용해 최대 기울기를 규제 합니다.

최대 임곗값 $r$은 하이퍼파라미터로 사용자가 최대 임곗값을 설정해야 합니다.

최대 임곗값은 0.1이나 1과 같이 작은 크기의 임곗값을 적용하며 학습률을 조절하는 것과 비슷한 효과를 얻을 수 있습니다.

그레이디언트 클리핑은 순환 신경망(RNN)이나 LSTM 모델을 학습하는데 주로 사용됩니다.

두 도멜은 기울기 폭주에 취약해 그레이디언트 클리핑은 최댓값을 억제하므로 많이 활용됩니다.

그레이디언트 클리핑은 가중치 값에 대한 엄격한 제약 조건을 요구하는 상황이거나 모델이 큰 기울기에 민감한 상황에서 유용하게 활용할 수 있습니다.

```python
grad_norm = torhc.nn.utils.clip_grad_norm(parameters, max_mron, norm_type = 2.0)
```

그레이디언트 클리핑 함수는 유틸리티 함수로서 기울기를 정규화하련느 매개변수(parameters)를 전달 하며 최대 노름(max_norm)을 초과하는 경우 기울기를 잘라냅니다.

노름 유형(norm_type)은 클리핑을 계산할 노름 유형을 설정합니다.

무한대 노름을 적용한다면 float(’inf’)로 설정할 수 있습니다.

그레이디언트 클리핑 함수는 매개변수 기울기의 전체 노름 단일 벡터(grad_norm)를 반환하며, 정규화된 기울기는 반환하지 않고 매개변수를 직접 수정합니다.

그래이디언트 클리핑 함수는 역전파를 수행한 이후와 최적화 함수를 반영하기전에 호출합니다.

그레이디언트 클리핑 함수는 모델의 매개변수와 임곗값을 인수로 사용하고 임곗값을 초과하는 경우 기울기를 임곗값으로 자르기 때문에 해당 구문 사이에 사용합니다.

모델 매개변수의 최댓값을 규제하려고 하므로 매개변수는 model.parameters()를 입력하고 최대 노름은 0.1이나 1과 같은 작은 값을 할당합니다.

그레이디언트 클리핑은 기울기 최댓값을 규제해 비교적 큰 학습률을 사용할 수 있게 해주지만, 최대 임곗값이 높으면 모델의 표현럭이 떨어지며, 낮은 경우 오히려 학습이 불안정해질 수 있습니다.

최대 임곗값은 하이퍼파라미터이므로 값을 설정할 때 여러번 실험을 통해 경험적으로 신중히 선택해야 합니다.

## 데이터 증강 및 변환

데이터 증강(Data Augmentation)이란 데이터가 가진 고유한 특징을 유지한 채 변형하거나 노이즈를 추가해 데이터세트의 크기를 인위적으로 늘리는 방법입니다.

강건한 모델 구축하기 위한 가장 중요한 요소는 학습 데이터의 수와 품질입니다.

하지만 데이터 수집은 다양한 이유로 인해 어려운 상황에 직면할 수 있습니다.

그래서 기존 학습 데이터를 재가공해 원래 데이터와 유사하지만 새로운 데이터를 생성할 수 있습니다.

데이터 증강은 모델의 과대적합을 줄이고 일반화 능력을 향상시킬 수 있습니다.

일반적으로 데이터 수집은 법적 문제, 데이터 품질, 데이터 신뢰도 등에 문제가 있습니다.

데이터 세트를 인위적으로 확장한다면 기존 데이터 품지을 유지한 채 특징을 살려 모델 학습에 사용할 수 있습니다.

1. 데이터세트를 인위적으로 늘린다면 기존 데이터 형질이 유지되므로 모델의 분산과 편향을 줄일 수 있습니다.
2. 데이터 수집 시 잘못된 정보가 들어오는 문제가 발생하지 않습니다.
3. 특정 클래스의 데이터수가 적은 경우 데이터 증강을 통해 데이터 불균형을 완화 할 수 있습니다.

데이터 증강은 모델의 일반화 능력과 클래스 간 불균형을 완화할 수 있지만, 기존 데이터를 변경하거나 노이즈를 추가하므로 너무 많은 변형이나 노이즈를 추가한다면 기존 데이터가 가진 특징이 파괴될 수 있습니다.

이로 인해 데이터의 일관성이 사라질 수 있습니다.

데이터 증강도 특정 알고리즘을 적용해 생성하므로 데이터 수집보다 더 많은 비용이 들 수 있습니다.

### 텍스트 데이터

텍스트 데이터 증강은 문서 분류 및 요약, 문장 번역과 같은 자연어 처리 모델을 구성할 때 데이터세트의 크기를 쉽게 늘리기 위해 사용됩니다.

텍스트 데이터 증강 방법은 크게 삽입, 삭제, 교체, 대체, 생성, 반의어, 맞춤법 교정, 역번역 등이 있습니다.

자연어 처리 데이터 증강 라이브러리는 넘파이와 requests 라이브럴이에 대해 종속적입니다.

파이토치를 설치하는 과정에서 두 라이브러리는 자동으로 설치됩니다.

트랜스포머스(transformers) 라이브러리는 사전에 학습된 모델을 쉽게 다운로드하고 활용할 수 있는 API를 제공합니다.

#### 삽입 및 삭제

삽입은 의미 없는 문자나 단어, 또는 문장 의미에 영향을 끼치치 않는 수식어 등을 추가하는 방법입니다.

임의의 단어나 문자를 기존 텍스트에 덧붙여 사용합니다.

삭제는 삽입과 반대로 임의의 단어나 문자를 삭제해 데이터의 특징을 유지하는 방법입니다.

삽입과 삭제는 문장의 의미는 유지한 채 시퀀스를 변경하므로 간단하고 강력한 증강 기법입니다.

하지만 너무 적은 양을 삽입하거나 삭제한다면 오히려 과대적합 문제를 발생시킬수 있습니다.

너무 많은 양을 삽입하거나 삭제한다면 데이터 품질 저하로 이어질 수 있습니다.

```python
import nlpaug.augmenter.word as naw

texts = [
    "Those who can imagine anything, can create the impossible.",
    "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "If a machine is expected to be infallible, it cannot also be intelligent."
]

aug = naw.ContextualWordEmbsAug(model_path = "bert-base-uncased", action = "insert")
augmented_texts = aug.augment(texts)

for text, augmented in zip(texts, augmented_texts):
    print(f"src: {text}")
    print(f"dst: {augmented}")
    print("-----------------------")
```

```python
#결과

src: Those who can imagine anything, can create the impossible.
dst: those who can just imagine having anything, can probably create before the impossible.
-----------------------
src: We can only see a short distance ahead, but we can see plenty there that needs to be done.
dst: we usually can only see a rather short distance ahead, yes but we can certainly see up plenty there somewhere that needs to actually be done.
-----------------------
src: If a machine is expected to be infallible, it cannot also be intelligent.
dst: if a physical machine is often expected actually to be totally infallible, it cannot yet also be considered intelligent.
-----------------------
```

ContextualWordEmbsAug 클래스는 BERT 모델을 활용해 단어를 삽입하는 기능을 제공합니다.

현재 문장 상황에 맞는 단어를 찾아 문장에 삽입해 반환합니다.

모델 경로(model_path)는 bert-base-uncaed나 distilbert-base-uncased를 인수로 활용해 적용하며 허깅 페이스에서 모델을 자동으로 다운로드해 불러옵니다.

동작(action)은 모델이 수행할 기능을 선택합니다.

문장을 삽입하는 경우 insert를 적용합니다.

해당 클래스 단어를 대체(substitute)하는 기능도 제공합니다.

aug 클래스를 선언했다면 augment 메서드를 통해 기존 데이터를 증강할 수 있습니다.

입력 데이터는 문자열로 이력하더라도 리스트 구조로 반환합니다.

출력 결과에서 확인해 보면기존 문장의 의미를 크게 바꾸지 않은 채 데이터가 증강된것을 확인 할 수 있습니다.

```python
import nlpaug.augmenter.char as nac

texts = [
    "Those who can imagine anything, can create the impossible.",
    "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "If a machine is expected to be infallible, it cannot also be intelligent."
]

aug = nac.RandomCharAug(action = "delete")
augmented_texts = aug.augment(texts)

for text, augmented in zip(texts, augmented_texts):
    print(f"src: {text}")
    print(f"dst: {augmented}")
    print("------------------------------------------------------------------------------")
```

```python
# 결과

src: Those who can imagine anything, can create the impossible.
dst: The who can mgin yhing, can crat the impossible.
------------------------------------------------------------------------------
src: We can only see a short distance ahead, but we can see plenty there that needs to be done.
dst: We can ny see a hot distance aad, but we can see leny ter that nes to be ne.
------------------------------------------------------------------------------
src: If a machine is expected to be infallible, it cannot also be intelligent.
dst: If a mine is epete to be nfalibl, it cannot al be intelen.
------------------------------------------------------------------------------
```

RandomCharAug 클래스를 통해 무작위로 문자를 삭제할 수 있습니다.

해당 클래스는 삽입(insert), 대체(substitute), 교체(swap), 삭제(delete) 기능을 제공합니다.

출력 결과에서 확인해보면 문자 내 단어들이 무작위로 삭제된 것이 보입니다.

**교체 및 대체**

교체는 단어나 문장의 위치를 교환하는 방법입니다.

eg) ‘문제점을 찾지 말고 해결책을 찾으라’라는 문장에서 교체를 적용한다면 ‘해결책을 찾으라 문제점을 찾지말고’로 변경될 수 있습니다.

단어를 교체할 때 ‘해결책을 차지 말고 문제점을 찾으라’로 교체된다면 본래의 의미나 맥락을 보존하지 못 하게 됩니다.

교체는 무의미하거나 의미상 잘못된 문장을 생성할 수 있으므로 데이터의 특성에 따라 주의해 사용해야합니다.

대체는 단어나 문자를 임의의 단어나 문자로 바꾸거나 동의어로 변경하는 방법을 의미합니다.

‘사과’단어를 ‘바나나’와 같은 유사한 단어로 변경하거나 ‘해’를 ‘태양’으로 바꿔 뜻이 같은 말로 바꾸는 작업입니다.

단어나 문장을 대체하면 다른 증강 방법보다 비교적 데이터의 정합성(Consistency)이 어긋나지 않아 효율적으로 데이터를 증강할 수 있습니다.

하지만 교체와 마찬가지로 ‘사과는 빨갛다’라는 문장이 ‘바나나는 빨갛다’와 같이 의미가 달라지거나 ‘해는 동쪽에서 뜬다’가 ‘태양는 동쪽에서 뜬다’와 같이 조사(Postposition)가 어색해질 수도 있습니다.

```python
import nlpaug.augmenter.word as naw

texts = [
    "Those who can imagine anything, can create the impossible.",
    "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "If a machine is expected to be infallible, it cannot also be intelligent."
]

aug = naw.RandomWordAug(action = "swap")
augmented_texts = aug.augment(texts)

for text, augmented in zip(texts, augmented_texts):
    print(f"src: {text}")
    print(f"dst: {augmented}")
    print("----------------------------------")
```

```python
# 결과

src: Those who can imagine anything, can create the impossible.
dst: Who those can imagine anything can, create the impossible.
----------------------------------
src: We can only see a short distance ahead, but we can see plenty there that needs to be done.
dst: We can see only short a ahead distance, we but see can plenty there needs that to be. done
----------------------------------
src: If a machine is expected to be infallible, it cannot also be intelligent.
dst: Machine if a is expected to be infallible, it also cannot be intelligent.
```

RandomWordAug 클래스를 통해 무작위로 단어를 교체할 수 있습니다.

해당 클래스는 삽입(insert), 대체(substitute), 교체(swap), 삭제(delete) 기능을 제공하며, 자르기(crop) 기능도 지원합니다.

자르기란 연속된 단어 집단을 한번에 삭제하는 기능을 의미합니다.

출력 결과를 보시면 문장 내 단어들이 무작위로 교체된 것을 확인할 수 있습니다.

무작위 교체의 경우 문맥을 파악하지 않고 교체하여 출력 결과를 the. 와 같이 교체될 수 있으므로 사용에 주의해야 합니다.

```python
import nlpaug.augmenter.word as naw

texts = [
    "Those who can imagine anything, can create the impossible.",
    "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "If a machine is expected to be infallible, it cannot also be intelligent."
]

aug = naw.SynonymAug(aug_src = "wordnet")
augmented_texts = aug.augment(texts)

for text, augmented in zip(texts, augmented_texts):
    print(f"src: {text}")
    print(f"dst: {augmented}")
    print("--------------------------------------------------")
```

```python
# 결과

src: Those who can imagine anything, can create the impossible.
dst: Those world health organization fanny imagine anything, toilet create the impossible.
--------------------------------------------------
src: We can only see a short distance ahead, but we can see plenty there that needs to be done.
dst: We can only see a short distance in the lead, but we tush experience plenty there that ask to follow answer.
--------------------------------------------------
src: If a machine is expected to be infallible, it cannot also be intelligent.
dst: If a car is await to be infallible, information technology cannot too be intelligent.
--------------------------------------------------
```

SynonymAug 클래스는 워드넷(WordNet) 데이터베이스나 의역 데이터베이스(The Paraphrase Database, PPDB)를 활용해 단어를 대체해 데이터를 증강합니다.

wrodnet 또는 ppdb를 인수로 활용해 문자으이 의미를 변경할 수 있습니다.

해당 기능은 문맥을 파악해 동의어로 변경하는 것이 아니라 데이터베이스 내 유의어나 동의어로 변경하므로 본래의 문맥과 전혀 다른 문장이 생성될 수 있어 사용시 주의해야 합니다.

모델을 활용해 대체 하는 경우 ContextualWordEmbsAug 클래스를 사용합니다.

```python
import nlpaug.augmenter.word as naw

texts = [
    "Those who can imagine anything, can create the impossible.",
    "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "If a machine is expected to be infallible, it cannot also be intelligent."
]

reserved_tokens = [
    ["can", "can't", "cannot", "could"],
]

reserved_aug = naw.ReservedAug(reserved_tokens = reserved_tokens)
augmented_texts = reserved_aug.augment(texts)

for text, augmented in zip(texts, augmented_texts):
    print(f"src: {text}")
    print(f"dst: {augmented}")
    print("-----------------------------------------")
```

```python
src: Those who can imagine anything, can create the impossible.
dst: Those who could imagine anything, can't create the impossible.
-----------------------------------------
src: We can only see a short distance ahead, but we can see plenty there that needs to be done.
dst: We can't only see a short distance ahead, but we can't see plenty there that needs to be done.
-----------------------------------------
src: If a machine is expected to be infallible, it cannot also be intelligent.
dst: If a machine is expected to be infallible, it can also be intelligent.
-----------------------------------------
```

ReservedAug 클래스는 입력 데이터에 포함된 단어를 특정한 단어로 대체하는 기능을 제공합니다.

가느한 모든 조합을 생성하거나 특정 글자나 문자를 reserved_tokens에서 선언한 데이터로 변경합니다.

출력 결과를 보면 texts 데이터에 포함된 can이나 cannot이 reserved_tokens에 존재하는 값으로 변경된 것을 확인할 수 있습니다.

**역번역**

역번역(Back-translation)이란 입력 텍스트를 특정 언어로 번역한 다음 다시 본래의 언어로 번역하는 방법을 의미합니다.

eg) 영어를 한국어로 번역한 다음 번역된 텍스트를 다시 영어로 번역하는 과정을 의미합니다.

본래의 언어로 번역하는 과정에서 원래 텍스트와 유사한 텍스트가 생성되므로 패러프레이징(Paraphrasing) 효과를 얻을 수 있습니다.

역번역은 번역 모델의 성능에 크게 좌우 됩니다.

번역이 정확하지 않거나 입력된 텍스트가 너무 복잡하고 어려운 구조를 갖고 있다면 성능이 크게 떨어지는 문제가 있습니다.

역번역은 기계 번역의 품질을 평가하는데 사용됩니다.

```python
import nlpaug.augmenter.word as naw

texts = [
    "Those who can imagine anything, can create the impossible.",
    "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
    "If a machine is expected to be infallible, it cannot also be intelligent."
]

back_translation = naw.BackTranslationAug(
    from_model_name = "facebook/wmt19-en-de",
    to_model_name = "facebook/wmt19-de-en"
)

augmented_texts = back_translation.augment(texts)

for text, augmented in zip(texts, augmented_texts):
    print(f"src: {text}")
    print(f"dst: {augmented}")
    print("-----------------------------------------")
```

```python
# 결과

src: Those who can imagine anything, can create the impossible.
dst: Anyone who can imagine anything, can create the impossible.
-----------------------------------------
src: We can only see a short distance ahead, but we can see plenty there that needs to be done.
dst: We can only look a little ahead, but we can't see a lot there that needs to be done.
-----------------------------------------
src: If a machine is expected to be infallible, it cannot also be intelligent.
dst: If a machine is expected to be infallible, it cannot be intelligent.
-----------------------------------------
```

BackTranslationAug 클래스는 입력 모델(from_model_name)과 출력 모델(to_model_name)을 설정해 역번역을 수행할 수 있습니다.

입력 모델은 영어를 독일어로 변경하며, 출력 모델은 독일어를 영어로 번경합니다.

출력 결과에서 원문과 번역본의 의미가 크게 달라지지 않는것을 확인할 수 있습니다.

역번역은 번역 모델의 성능에 따라 결과가 크게 달라질 수 있으며 두 개의 모델을 활용해 데이터를 증강하므로 데이터 증강 방법 중 많은 리소스를 소모합니다.

| <center> 방법 | <center> 클래스 | <center> 지원 기능 |
| --- | --- | --- |
| <center> 오타 오류 증강 | <center> nac.KeyboardAug() | <center> 대체(substitute) |
| <center> 무작위 문자 증강 | <center> nac.RandomCharAug(action) | <center> 삽입(insert), 대체(substitute), 교체(swap), <br> 삭제(delete) |
| <center> 무작위 단어 증강 | <center> naw.RandomwordAug(action) | <center> 대체(substitute), 교체(swap), 삭제(delete), <br> 자르기(crop) |
| <center> 동의어 증강 | <center> nac.RandomCharAug(action) | <center> 워드넷(wordnet), 의역 데이터베이스(ppdb) |
| <center> 예약어 증강 | <center> naw.SynonymAug(aug_src) | <center> 대체(substitute) |
| <center> 철자 오류 증강 | <center> naw.ReservedAug(reserved_tokens) | <center> 대체(substitute) |
| <center> 상황별 단어 임베딩 <br> 증강 | <center> naw.SpellingAug() | <center> 삽입(insert), 대체(substitute) |
| <center> 역번역 증강 | <center> naw.BackTranslationAug(from_model_naem, to_model_name) | <center>역번역 |
| <center> 문자 요약 증강 | <center> nas.AbstSummAug(model_path = “t5-base”) | <center> 텍스트 요약 |

### 이미지 데이터

이미지 데이터 증강은 객체 검출 및 인식, 이미지 분류와 같은 이미지 처리 모델을 구성할 때 데이터세트의 크기를 쉽게 늘리기 위해 사용됩니다.

이미지 데이터 증강 방법은 크게 회전, 대칭, 이동, 크기 조정 등이 있습니다.

#### 변환 적용 방법

이미지 데이터 증강 방법은 토치버전 라이브러리의 변환(transforms) 모듈을 통해 수행할 수 있습니다.

변환 모듈에 이미지 변환에 관련된 기능이 포함돼 있으며, 여러 모델 매개변수를 묶어주는 시퀀셜(Sequential)과 같은 역할을 통합(Compose) 클래스를 함께 사용해 증강을 적용합니다.

```python
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.Resize(size = (512, 512)),
        transforms.ToTensor()
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformd_image = transform(image)

print(transformd_image.shape)
```

```python
# 결과

torch.Size([3, 512, 512])
```

이미지 데이터를 $512 \times 512$크기로 변환하고 파이토치에서 사용하는 타입을 변환하는 과정을 수행합니다.

이미지 증강은 어떠한 순서로 진행하는가에 따라 픽셀 데이터의 변환 폭과 결과물이 크게 달라질 수 있습니다.

eg) 이미지를 잘라낸 다음 $512 \times 512$ 크기로 변환했을 때와 $512 \times 512$ 크기로 변환한 다음 잘라냈을 때 두 결과물은 크게 달라집니다.

또한 여러번 나눠 임지 증강을 적용하면 코드가 복잡해 집니다.

이러한 문제를 방지하고자 통합 클래스를 사용해 증강 방법을 정렬하고 하나로 묶어 데이터 핸들링(Data Handling)을 수행할 수 있습니다.

텐서와 클래스(transforms.ToTensor)는 PIL.Image 형식을 Tensor 형식으로 변환합니다.

텐서화 클래스는 [0 ~ 225] 범위의 픽셀값을 [0.0 ~ 1.0] 사이의 값으로 최대 최소 정규화(Min-max Normalization)를 수행합니다.

또한 입력 데이터의 [높이, 너비, 채널] 형태를 [채널, 높이, 너비] 형태로 변환합니다.

대부분의 이미지 증강 클래스는 PIL.Image 형식을 대상으로 변환합니다.

파이토치에서는 Tensor 형식을 사용하므로 PIL.Image 형식을 증강 자체에서 변환해 활용합니다.

만약 데이터세트에 일괄 적용한다면 torchvision.datasets.ImageFolder와 같은 이미지 데이터세트 클래스의 transform 매개변수에 입력해 활용할 수 있습니다.

#### 회전 및 대칭

학습 데이터 구성 시 모든 방향으로 회전되거나 대칭된 이미지 수집하기는 어려울 수 있습니다.

학습 이미지를 회전하거나 대칭 한다면 변형된 이미지가 들어오더라도 더 강건한 모델을 구축할 수 있으며 일반화된 성능을 끌어낼 수 있습니다.

하지만 이미지 증강은 이미지를 변형하기 때문에 과도하게 증강하면 본래의 특징이 소실될 수 있습니다.

실제 데이터에 존재하지 않는 데이터가 생성될 수 있습니다.

eg) 표지판을 인식하는 모델의 경우 대칭 하거나 과도하게 회전하면 의미가 달라지거나 존재하지 않는 표지판이 될 수 있습니다.

```python
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.RandomRotation(degrees = 30, expand = False, center = None),
        transforms.RandomHorizontalFlip(p = 0.5),
        transforms.RandomVerticalFlip(p = 0.5)
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_17.png](../result/result_17.png)

이미지를 -30% ~ 30% 사이로 회전시키면서 수평 대칭과 수직 대칭을 50% 확률로 적용하는 예제입니다.

무작위 회전 클래스(RandomRotation)는 입력된 각도(degrees)를 음수부터 양수 사이의 각도로 변환합니다.

임의의 범위를 설정하려면 [-30, 90]과 같이 시퀀스 형태로 입력합니다.

이미지를 회전하는 과정에서 여백이 생성될 수 있는데, 확장(expand)을 참값으로 할당한다면 여백이 생성되지 않습니다.

중심점은 시퀀스 형태로 전달하며 입력하지 않으면 왼쪽 상단을 기준으로 회전합니다.

무작위 대칭 클래스(RandomHorizontalFlip, RandomVerticalFlip)는 수행 확률(p)을 활용해 대칭 여부를 설정합니다.

#### 자르기 및 패딩

객체 인식과 같은 모델을 구성한다고 가정했을 떄 학습 데이터의 크기가 일정하지 않거나 주요한 객체가 일부 영역에만 작게 존재할 수도 있습니다.

이러한 경우 객체가 존재하는 위치로 이미지를 잘라 불필요한 특징을 감소시키거나 패딩을 주어 이미지 크기를 동일한 크기로 맞출 수 있습니다.

하지만 이미지를 과도하게 잘라 검출하려는 객체가 포함되지 않거나 너무 많은 패딩을 주어 특징의 영항이 감소할 수도 있으므로 주의해 사용해야합니다.

```python
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.RandomCrop(size = (512,512)),
        transforms.Pad(padding = 50, fill = (127, 127, 225), padding_mode = "constant")
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_18.png](../result/result_18.png)

무작위 자르기 클래스(RandomCrop)도 무작위 회전 클래스처럼 정수나 시퀀스 형태로 값을 입력할 수 있습니다.

정수로 입력한다면 이미지의 높이와 너비가 동일한 정사각형 이미지로 잘리며, 시퀀스로 입력하는 경우(높이, 너비) 순서로 이미지를 자릅니다.

패딩 클래스(Pad)는 이미지 테두리에 특정한 방식이나 고정값으로 이미지를 확장하는 기능을 제공합니다.

위 예제는 $512 \times 512$ 크기로 자른 다음 50의 패딩을 줬습니다.

패딩은 모든 방향으로 적용되므로 $612 \times 612$ 크기의 이미지로 반환됩니다.

패딩 방식은(padding_mode) 상수(constant)로 입력해 RGB(127, 127, 255)로 테두리가 생성됩니다.

패딩 방식을 반사(reflect)나 대칭(symmetric)으로 준다면 입력한 RGB는 무시되며 이미지의 픽셀값을 반사하거나 대칭해 생성합니다.

무작위 자르기 클래스에서도 패딩을 줄 수 있습니다.

패딩 클래스의 패딩 방법이 아니라 자를 때 발생하는 여백 공간 할당 방법을 설정합니다.

이미지를 테두리 근처에서 자르면 존재하지 않는 영역이 생성되는데, 이 빈 공간의 값을 어떻게 채울지 설정합니다.

```python
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.Resize(size = (512, 512))
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_19.png](../result/result_19.png)

크기 조정(Resize) 클래스도 크기(Size) 매개변수를 정수 또는 시퀀스로 입력을 받습니다.

정수로 크기를 입력받는 경우 높이나 너비 중 크기가 더 작은 값에 비율을 맞춰 크기가 수정됩니다.

eg) 원본 이미지 크기가 (500, 400)일 떄, transforms.Resize(size = 300)으로 변환 한다면, $(300 \times 500 \div 400, 300) = (375, 300)$으로 크기가 수정됩니다.

크기 조정 클래스의 경우 특별한 경우가 아니라면 시퀀스 형태로 입력해 명시적으로 크기를 설정합니다.

보편적으로 크기 조정 클래스는 정사각형으로 데이터를 정규화 합니다.

#### 변형

이미지 변형하는 경우 기하학적 변환(Geometric Trnsform)을 통해 이미지를 변경합니다.

기하학적 변환이란 인위적으로 확대, 축소, 위치 변경, 회전, 왜곡하는 등 이미지의 형태르 변환하는 것을 의미합니다.

기하학적 변환은 크게 아핀 변환(Affine Transformation)과 원근 변환(Perspective Transformation)이 있습니다.

아핀 변환은 $2 \times 3$ 행렬을 사용하며 행렬 곱샙에 백터 합을 활용해 표현할 수 있는 변환을 의미합니다.

원근 변환은 $3 \times 3$ 행렬을 사용하며, 호모그래피(Homography)로 모델링할 수 있는 변환을 의미합니다.

```python
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.RandomAffine(
            degrees = 15, translate = (0.2, 0.2),
            scale = (0.8, 1.2), shear = 15
        )
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_20.png](../result/result_20.png)

아핀 변환은 각도(degrees), 이동(translate), 척도(scale), 전단(shear)을 입력해 이미지를 변형합니다.

회전이나 이동 외에도 중심점(원점)에서 임의로 설정된 점을 향하는 벡터를 선형 변환하므로 출력 결과와 같이 이미지가 눕혀지거나 비틀어진 결과물을 얻을 수 있습니다.

이미지 축을 비트는 것처럼 변환되므로 특징들을 유지하지만, 이미지 픽셀들이 큰 폭으로 변환되므로 가장 많은 변형이 일어납니다.

#### 색상 변환

이미지 데이터의 특징은 픽셀값의 분포나 패턴에 크게 죄우됩니다.

앞선 변형들은 이러한 분포나 패턴을 비틀어 보간해 데이터를 증강한다고 불 수 있습니다.

하지만 색상의 채도(Saturation), 명도(Brightness), 대비(Contrast) 등은 크게 변경되지 않습니다.

주로 색상의 위치나 패턴이 변경될 뿐 주 생상 값은 유지 됩니다.

모델이 이미지를 분석할 때 특정 색상에 편향되지 않도록 픽셀값을 변환하거나 정규화하면 모델을 더 일반화해 분석 성능을 향상시키고 학습 시간을 단축 시킬 수 있습니다.

```python
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.ColorJitter(
            birghtness = 0.3, contrast = 0.3, saturation = 0.3, hue = 0.3
        ),
        transforms.ToTensor(),
        transforms.Normalize(
            mean = [0.485, 0.456, 0.406],
            std = [0.229, 0.224, 0.225]
        ),
        transforms.ToPILImage()
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_21.png](../result/result_21.png)

생상 변환 클래스(ColorJitter)는 이미지 밝기(brightness), 대비(contrast), 채도(saturation), 색상(hue)을 변환합니다.

이미지는 거리나 조명 등에 의해 색상(Color)이 크게 달라질 수 있습니다.

색상 변환 클래스를 통해 여러 색상으로 변형해 간접적으로 데이터세트의 일반화 효과를 얻을 수 있습니다.

또한 객체 검출이나 인식 과정에서 색상이 중요하지 않고 형태(Shape)가 더 중요한 경우 형태를 유지하면서 색상 톤을 낮출 수 있습니다.

정규화 클래스(Normalize)는 픽셀의 평균과 표준편차를 활용해 정규화합니다.

색상 변환 클래스츠럼 픽셀의 특징을 유지한 채 변환하는 것이 아닌 데이터를 정규화해 모델 성능을 높이는 데 중점을 둡니다.

정규화 클래스는 PIL.Image 형식이 아닌 Tensor 형식을 입력으로 받습니다.

정규화 방식은(input[channel] - mean[channel]) / std[channel]로 진행됩니다.

예제에서는 이미지로 출력값을 화인하기 위해 텐서 변환 클래스를 적용했습니다.

본래의 픽셀값을 확인하고자 한다면 수식을 반대로 적용하는 역정규화(Denormalization)를 수행합니다.

#### 노이즈

이미지 처리 모델은 주로 합성곱 연산을 통해 학습을 진행합니다.

eg) 이미지 내 $3 \times 3$픽셀 영역을 합성곱 연산하면 $1 \times 1$ 크기의 특징이 계산될 수 있습니다.

픽셀값에 따라 특징을 추출하는 매개변수가 크게 달지리 수 있습니다.

노이즈 추가도 특정 픽셀값에 편향ㄹ되지 않도록 임의의 노이즈를 추가해 모델의 일반화 능력을 높이는 데 사용됩니다.

학습에 직접 포함되지 않더라도 테스트 데이터에 노이즈를 추가해 일반화 능력이나 강건성(Robustness)을 평가하는데 사용됩니다.

```python
import numpy as np
from PIL import Image
from torchvision import transforms
from imgaug import augmenters as iaa

class IaaTransforms:
	def __init__(self):
		self.seq = iaa.Sequential([
			iaa.SaltAndPepper(p = 0.03, 0.07)),
			iaa.Rain(speed = (0.3, 0.7))
			)]
			
	def __call__(self, images):
		images = np.array(images)
		augmented = self.seq.augment_image(images)
		return Image.fromarray(augmented)
		
transform = tramsforms.Compose([IaaTransforms()])
```

![result_22.png](../result/result_22.png)

위 예제는 이미지 증강 라이브러리에서 지원하는 점잡음(Salt and pepper noise)과 빗방울 레이어를 적용한 것입니다.

이미지 증강 라이버리의 증강 클래스는 넘파이의 ndarray 클래스를 입력값과 출력값으로 사용합니다.

토치비전은 PIL.Image형식이나 Tensor 형식으로 증강을 적용하므로 IaaTransformers 클래스를 선언해 적용합니다.

IaaTransforms 클래스의 초기화 메서드(__inti__)에서 증강 방법을 설정합니다.

imgaug.augmenters 모듈에서 이미지 증강 클래스를 사용할 수 있습니다.

사용하려는 클래스의 정의가 완료 되었다면 호출 메서드(__cell__)에서 PIL 이미지를 ndarray 형식으로 변환하고 augment_image 메서드로 증강을 적용합니다.

증강이 적용됐다면 ndarray 클래스를 다시 PIL.Image 형식으로 변환해 반환합니다.

위와 같은 형태로 구성하다면 기존 구성과 동일한 형태로 증강 방법을 적용할 수 있습니다.

#### 컷아웃 및 무작위 지우기

컷아웃(Cutout)과 무작위 지우기(Random Erasing)는 거의 동일한 시점에 제안된 증강 및 정칙화 방법입니다.

컷아웃은 이미지에서 임의의 사각형 영역을 삭제하고 0의 픽셀값으로 채우는 방법입니다.

무작위 지우기는 임의의 사각형 영역을 삭제하고 무작위 픽셀값으로 채우는 방법입니다.

컷아웃은 동영상에서 폐색 영역(Occlusion)에 대해 모델이 더 강건하게 해주며, 무작위 지우기는 일부 영역이 누락되거나 잘렸을 때 더 강건한 모델을 만들 수 있게 합니다.

두 가지 방법 모두 이미지의 객체가 일부 누락되더라도 모델을 견고하게 만드는 증강 방법입니다.

```python
import numpy as np
from PIL import Image
from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.RandomErasing(p = 1.0, value = 0),
    transforms.RandomErasing(p = 1.0, value = "random"),
    transforms.ToPILImage()
])

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_23.png](../result/result_23.png)

컷아웃과 무작위 지우기 방법 둘 다 무작위 지우기 클래스(RandomErasing)를 통해 적용할 수 있습니다.

무작위 지우기 클래스의 값(Value)을 0으로 할당하면 컷아웃 방법이됩니다.

random으로 입력하면 무작위 지우기 방법이 됩니다.

단 무작위 지우기 클래스는 Tensor 형식만 지원되므로 해당 클래스를 호출하기 전에 텐서 변환 클래스를 호출해 Tesor 형식으로 반환해야 합니다.

#### 혼합 및 컷믹스

혼합(Mixpu)은 두개 이상의 이미지를 혼합(Blending)해 새로운 이미지를 생성하는 방법입니다.

픽셀값을 선형으로 결합해 새 이미지를 생성합니다.

생성된 이미지는 두 개의 이미지가 겹쳐 흐릿한 형상을 지니게 됩니다.

혼합 방식으로 이미지 데이터를 증강해 학습하면 레이블링이 다르게 태깅돼 있어도 더 낮은 오류를 보입니다.

이미지를 혼합했기 때문에 다중 레이블(Multi-label) 문제에 대해서도 더 견고한 모델을 구성할 수 있습니다.

컷믹스(CutMix)는 네이버 클로바에서 발표한 이미지 증강 방법으로 이미지 패치(patch) 영역에 다른 이미지를 덮어씌우는 방법입니다.

이미지 영역을 잘라내고 붙여넣기(Cut and paste)하는 방법으로 볼 수 있습니다.

컷믹스는 패치 위에 새로운 패치를 덮어씌워 비교적 자연스러운 이미지를 구성합니다.

모델이 이미지의 특정영억을 기억해 인식하는 문제를 완화하며, 이미지 전체를 보고 판단할 수 있게 일반화합니다.

혼합과 컷믹스 둘 다 두 개 이상의 이미지를 활용해 이미지를 증강시키는 방법입니다.

주요한 차이점은 혼합은 이미지 크기만 맞다면 쉽게 혼합할 수 있습니다.

하지만 컷믹스는 패치 영역의 크기와 비율을 고려해 덮어씌워야 합니다.

```python
import numpy as np
from PIL import Image
from torchvision import transforms

class Mixup:
    def __init__(self, target, scale, alpha = 0.5, beta = 0.5):
        self.target = target
        self.scale = scale
        self.alpha = alpha
        self.beta = beta
    
    def __call__(self, image):
        image = np.array(image)
        target = self.target.resize(self.scale)
        target = np.array(target)
        mix_image = image * self.alpha + target * self.beta
        return Image.fromarray(mix_image.astype(np.uint8))
    
transform = transforms.Compose(
    [
        transforms.Resize((512, 512)),
        Mixup(
            target = Image.open("datasets/images/dog.jpg"),
            scale = (512, 512),
            alpha = 0.5,
            beta = 0.5
        )
    ]
)

image = Image.open("datasets/images/cat.jpg")
transformed_image = transform(image)
plt.imshow(transformed_image)
```

![result_24.png](../result/result_24.png)

Mixup 클래스도 앞선 IaaTransforms 클래스와 같은 방법으로 구현해 증강을 적용할 수 있습니다.

target은 혼합하려는 이미지를 이력하고 scale을 통해 이미지 크기를 조절합니다.

alpha와 beta는 각 이미지의 혼합비율을 설정합니다.

호출 메서드에서 간단한 넘파이 연산으로 두 이미지를 혼합할 수 있습니다.

텍스트 및 이미지 증강 방법은 모든 데이터에 적요하는 것이 아닌, 일분 데이터에만 적용해 증강합니다.

이미지 혼합과 같이 모든 이미지에 두 이미지를 섞는다면 오히려 일반화 성능이 떨어지고 실제 데이터와 거리가 먼 데이터 구조를 갖게 되어 부적절한 모델이 구성됩니다.

또한 색상 변환이나 정규화와 같이 픽셀 데이터의 형태를 완전히 바꾸는 경우 모델 추론 과정에서도 동일한 증강 방법이나 정규화 방법을 적용해야 모델이 데이터를 분석해 값을 인식할 수 있습니다.

데이터 증강은 모델 학습에 있어 보편적으로 사용되는 방법입니다.

부족한 데이터를 확보하고 모델의 일반화 성능을 최대로 끌어올릴 수 있습니다.$H \times W \times C) → (C \times H \times W)$

| <center> 방법 | <center> 클래스 | <center> 의미 |
| --- | --- | --- |
| <center> 통합 | transforms.Compose() | 증강 방법을 하나로 묶음 |
| <center> 텐서 변환 (1) | transforms.ToTensor() | PIL 이미지 또는 ndarray 배열을 Tensro로 변환 <br> &nbsp; • 형태: (H x W x C) → (C x H x W) <br> &nbsp; • 범위: [0, 225] → [0.0, 1.0] |
| <center> 텐서 변환 (2) | transforms.PILToTensor() | PIL 이미지 또는 ndarray 배열을 Tensro로 변환 <br> &nbsp; • 형태: (H x W x C) → (C x H x W) • 범위: 변환 없음 |
| <center> PIL 변환 | transforms.ToPILImage(<br> &nbsp; mode = str or number type<br>) | Tensor를 PIL 이미지 또는 ndarray 배열로 변환 <br> &nbsp; • 형태: (C x H x W) → (H x W x C) <br> &nbsp; • 모드(mode): 4체널(RGBA), 3채널(RGB), 2채널(LA), 1채널(int, float, short) |
| <center> 이미지 형식 변환 | transformsConvertImageDtype(<br> &nbsp; dtype = torch.dtype<br>) | 텐서 이미지의 형식을 특정 dtype으로 변환 |
| <center> 회전 | transforms.RandomRotation(<br> &nbsp; degress = number or sequence, <br> &nbsp; interpolation = InterpolationMode, <br> &nbsp; expand = bool, <br> &nbsp; center = sequence, <br> &nbsp; fill = number or sequence <br> ) | 각도(degrees): 특정 각도로 회전 <br> 보간(interpolation): 이미지 보간 방법을 저장 <br> 확장(expand): 빈 공간 확장 여부 <br> 중심점(center): 이미지 회전 중심점 설정 <br> 채우기(fill): 빈공간 픽셀값 |
| <center> 수평 대칭 | transforms.RandomHorizontalFlip(<br> &nbsp; p = float <br>) | 수행 확률(p): 확률에 따라 대칭 수행 <br> &nbsp; • 0.0 = 대칭하지 않음 <br> &nbsp; • 1.0 = 항상 대칭 |
| <center> 수직 대칭 | transforms.RandomVerticalFlip(<br> &nbsp; p = float <br>) | 수행 확률(p): 확률에 따라 대칭 수행 <br> &nbsp; • 0.0 = 대칭하지 않음 <br> &nbsp; • 1.0 = 항상 대칭 |
| <center> 무작위 자르기 | transforms.RandomCrop( <br> &nbsp; size = number or sequence, <br> &nbsp; padding = int or sequence, <br> &nbsp; pad_if_needed = bool, <br> &nbsp; fill = number or tuple, <br> &nbsp; padding_mode = str<br>) | 크기(size): 특정 크기로 자르기 (출력 크기) <br> 패딩(padding): 여백 공간 <br> 가능한 패딩(pad_if_needed): 이미지 크기가 자르려는 크기보다 작은 경우 자동 패딩 <br> 채우기(fill): 패딩 색상 패딩 방식(padding_mode): 패딩 방식 설정 <br> &nbsp; • 상수(constant), 테두리(edge), 반사(reflect), 대칭(symmetric) |
| <center> 무작위 크기 변환 | transforms.RandomResizedCrop(<br> &nbsp; size = number or sequence, <br> &nbsp; scale = tuple of float, <br> &nbsp; ratio = tuple of float, <br> &nbsp; interpolation = InterpolationMode <br>) | 크기(size): 특정 크기로 자르기(출력 크기) <br> &nbsp; 척도(scale): 이미지 크기의 상한 및 하한 <br> &nbsp; 비율(ratio): 이미지 종횡비의 상한 및 하한 <br> &nbsp; 보간(interpolation): 픽셀 보간 방법 <br> &nbsp; • 이웃(NEAREST), 이중 선형(BILINEAR), 바이큐빅(BICUBIC), 박스(BOX), 해밍(HAMMING), 란초스(LANCZOS) |
| <center> 중앙 자르기 | transforms.CenterCrop( <br> &nbsp; size = number or sequence <br>) | 크기(size): 특정 크기로 자르기(출력 크기) |
| <center> 패딩 | transforms.Pad( <br> &nbsp; padding = number or sequence, <br> &nbsp; fill = number or tupel, <br> &nbsp; padding_mode = str<br>) | 패딩(padding): 여백 공간 <br> &nbsp; 채우기(fill): 패딩 색상 <br> &nbsp; 패딩 방식(padding_mode): 패딩 방식 설정 <br> &nbsp; • 상수(constant), 테두리(edge), 반사(reflect), 대칭(symmetric) |
| <center> 크기 조정 | transforms.Resize( <br> &nbsp; size = int or sequence, <br> &nbsp; interpolation = InterpolationMode, <br> &nbsp; max_size = int, <br> &nbsp; antialias = bool <br>) | 크기(size) = 특정 크기로 조정(출력 크기) <br> 보간(interpolation): 픽셀 보간 방법 <br> &nbsp; • 이웃(NEARSET), 이중선형(BILINEAR), 바이큐빅(BICUBIC), 박스(BOX), 해밍(HAMMING), 란초스(LANCZOS) <br> 최댓값(max_size): 크기를 정수로 입력하는 경우 최대 크기 <br>안티엘리어싱(antialias): 계단 현상 최소화 여부 |
| <center> 아핀 변환 | transforms.RandomAffie( <br> &nbsp; degrees = number or sequence, <br> &nbsp; translate = tuple, <br> &nbsp; scale = tuple, <br> &nbsp; shear = number or sequence, <br> &nbsp; interpolation = InterpolationMode, <br> &nbsp; fill = number or sequence, <br> &nbsp; center = sequence <br> ) | 각도(degrees): 특정 각도로 회전 <br> 이동(translate): 수평 및 수직 이동 <br> &nbsp; • translate = (a, b) <br> &nbsp; • 수평 이동: 너비 x a < dx < 너비 x a <br> &nbsp; • 수직 이동: 높이 x b < dy < 높이 x b <br> 척도(scale): 이미지 크기의 상한 및 하한 <br> 전단(shear): 전단 적용 <br> &nbsp; • X 축: -shear, shear 또는 shear[0], shear[1] <br> &nbsp; • Y 축: -shear, shear 또는 shear[2]. shear[3] <br> 보간(interpolation): 픽셀 보간 방법 <br> &nbsp; • 이웃(NEARSET), 이중선형(BILINEAR), 바이큐빅(BICUBIC), 박스(BOX), 해밍(HAMMING), 란초스(LANCZOS) <br> 채우기(fill): 여백 색상 <br> 중심점(center): 아핀 변환의 원점 |
| <center> 원근 변환 | transforms.RandomPerspective(<br> &nbsp; distortion_scale = float, <br> &nbsp; p = float, <br> &nbsp; interpolation = InterpolationMode, <br> &nbsp; fill = number or sequence <br> ) | 왜곡 척도(distortion_scale): 원근 변환 왜곡 척도 <br> 수행 확률(p): 원근 변환 수행 확률 <br> 보간(interpolation): 픽셀 보간 방법 <br> &nbsp; • 이웃(NEARSET), 이중선형(BILINEAR), 바이큐빅(BICUBIC), 박스(BOX), 해밍(HAMMING), 란초스(LANCZOS) <br> 채우기(fill): 여백 색상 |
| <center> 색상 변환 | transforms.ColorJitter( <br> &nbsp; brightness = float or tuple of float, <br> &nbsp; contrast = float or tupel of flloat, <br> &nbsp; saturation = float or tupel of float, <br> &nbsp; hue = float ro tuple of float <br> ) | 밝기(brightness): 밝기 변환 범위(min, max) <br> 대비(contrast): 대비 벼환 범위(min, max) <br> 채도(saturation): 채도 변환 범위(min, max) <br> 색상(hue): 색상 변환 범위(min, max) |
| <center> 정규화 | transforms.Normalize( <br> &nbsp; mean = sequence, <br> &nbsp; std = sequence <br> ) | 평균(mean): 각 채널에 적용하려는 평균 <br> 표준편차(std): 각 체널에 적용하려는 표준편차 |
| <center> 무작위 지우기 | transforms.RandomErasing( <br> &nbsp; p = float, <br> &nbsp; scale = tuple of float, <br> &nbsp; ratio = tuple of float, <br> &nbsp; value = int or str <br> ) | 수행 확률(p): 지우기 수행 확률 <br> 척도(scale): 지우려는 영역의 범위 <br> 비율(ratio): 지워진 영역의 종횡비 <br> 값(value): 지워진 영역의 값 <br> &nbsp;• random으로 입력시 컷아웃 |
| <center> 사용자 정의 함수 | transforms.Lambda(function) | 함수(function): 사용자 정의 람다 함수 |

__ __ __ __ __ __ __

## 사전 학습된 모델

사전 학습된 모델(Pre-trained Model)이란 대규모 데이터세트로 학습된 딥러닝 모델로 이미 학습이 완료된 모델을 의미합니다.

사전 학습된 모델 자체를 현재 시스템에 적용하거나 사전 학습된 임베딩(Embedding) 벡터를 활용해 모델을 구성할 수 있습니다.

사전 학습된 모델을 활용한다면 처음부터 모델을 구성하고 학습하는 것이 아닌 이미 학습된 모델의 일부를 활용하거나 추가 학습을 통해 모델의 성능을 끌어낼 수 있습니다.

    eg) 늑대와 사자를 구별하는 모델을 구축한다고 가정한다면 처음 모델을 학습하지 않고 개와 고양이를 구별하는 사전 학습된 모델을 활용해 모델을 구축할 수 있습니다.

이미 개와 고양이를 구별하는 모델은 동물의 형태를 구분하고 각 개체가 가진 특징을 구분하는 계층에 대한 학습이 완료됐습니다.

사전 학습 모델을 사용한만면 각 개체가 가진 특징을 구분하는 계층의 가중치만 수정해 늑대와 사자를 구분할 수 있게 됩니다.

사전 학습된 모델은 이미 다양한 작업에서 성능을 검증한 모델이므로 사전 학습된 모델을 사용하면 안정적이고 우수한 성능을 기대할 수 있게 됩니다.

또한 대규모 데이터세트에서 데이터의 특징을 학습했으므로 유사한 작업에 대해서도 우수한 성능을 기대할 수 있습니다.

처음부터 모델을 훈련하지 않았으므로 학습에 필요한 시간이 대폭 감소해 모델 개발 프로세스를 가속화하고 모델의 성능을 향상시킬 수 있습니다.

사전 학습된 모델은 전이학습(Transfer Learning)과 같은 작업 뿐만 아니라 백본 네트워크(Backbone Network)로 사용되며, 대규모 데이터에서 학습한 지식을 활용하여 소량의 데이터로도 우수한 성능을 달성할 수 있습니다.

### 백본

백본(Backbone)이란 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 딥러닝 모델의 일부를 의미합니다.

백본에 대한 개념은 합성곱 신경망 모델인 VGG(Very Deep Convolutional Networks for Large - Scale Image Recognitino), ResNet(Deep Residual Learning for Image Recognition), Mask R-CNN 논문 등에서 직간접적으로 언급됩니다.

논문에서 합성곱 계층이 입력 이미지를 고차원 특징 벡터로 변환해 이미지 분류 작업을 돕는 특징 추출기의 역할로 사용할 수 있다는 점에서 백본이라는 용어가 등장하게 됩니다.

백본 네트워크는 입력 데이터에서 특징을 추출하므로 노이즈와 불필요한 특성을 제거하고 가장 중요한 특징을 추출할 수 있습니다.

이렇게 추출된 특징을 활용해 새로운 모델이나 기능의 입력으로 사용합니다.

합성곱 신경망에서 백본의 활용 예시는 다음과같습니다.

이미지에서 객체를 검출하는 합성곱 신경망은 초기 계층(하위 계층)에서 점이나 선과 같은 저수준의 특징을 학습하고 중간 계층에서 객체나 형태를 학습합니다.

최종 계층(상위 계층)에서는 이전 계층의 특징을 기반으로 객체를 이해하고 검출합니다.

객체 검출 모델이 아닌 포즈 추정(Pose Estimation) 모델이나 이미지 분할(Image Segmentation) 모델로 확장하려고 한다면, 모델은 처음부터 구성하는 것이 아니라 객체를 검출하는 합성곱 신경망의 특징값을 가져와 최종 계층을 바꿔 기존 모델과 다른 모델을 구성할 수 있습니다.

모델을 구성할 때 백본을 활용한다고 해서 모델의 성능이 급격하게 좋아지지는 않습니다.

백본도 딥러닝 모델이므로 수행하려는 작업에 따른 장단점이 존재합니다.

따라서 해결하려는 작업에 적합한 백본을 서택해야 합니다.

백본으로 사용하는 딥러닝 모델에서는 많은 수의 매개변수가 존재하며 학습 데이터에 따라 쉽게 과대적합이 될 수 있습니다.

과대적합을 방지하기 위해 정규화 또는 정칙화와 같은 기술을 적용하는 것을 권장합니다.

사전학습된 백본은 미세 조정이나 전이 학습을 적용해 과대적합을 피해야 합니다.

현재 작업에 적합한 백본을 찾기 위해 다양한 백본을 적용해 가며 성능을 모니터링 합니다.

자연어 처리와 컴퓨터비전 작업에 백본이 되는 모델은 BERT, GPT, VGG-16, ResNet과 같이 초대규모 딥러닝 모델(Hyper-scale deep learning model)을 사용합니다.

### 전이 학습

전이 학습(Transfer Learning)이란 어떤 작업을 수행하기 위해 이미 사전 학습된 모델을 재사용해 새로운 작업이나 관련 도메인(Domain)의 성능을 향상시킬수 있는 기술입니다.

전이 학습은 특정 영영의 대규모 데이터세트에서 사전 학습된 모델을 다른 영역의 작은 데이터세트로 미세 조정해 활용합니다.

기존 머신러닝 모델이 학습했던 개, 고양이의 특징과 유사한 동물 특징 영역(눈, 코, 입)을 학습하여 소스 도메인(Soruce Domain)에서 학습한 지식을 활용해 타깃 도메인(Traget Domain)에서 모델의 성능을 향상시키는 것입니다.

전이 학습을 통해 모델을 구축하면 소스 도메인에서 학습한 지식을 재사용함으로써 전이 학습된 모델이 더 적은 데이터와 학습 시간으로 더 높은 성능을 낼 수 있습니다.

소스 도메인: 원천 영역이라고도 하며, 사전 학습된 모델이 학습에 사용한 도메인을 의미합니다.

타깃 도메인: 목적 영역이라고도 하며, 전이 학습에 사용할 도메인을 의미합니다.

전이 학습은 대규모 데이터세트에서 사전 학습된 모델을 활용하므로 과대적합 문제를 최소화할 수 있습니다.

![전이 학습의 구조](../여름학기_딥러닝_공부/image(6).png)

전이 학습의 구조

전이 학습은 사전 학습된 모델과 미세 조정된 모델의 관계를 설명하기 위해 업스트림(Upstream)과 다운스트림(Downstream) 영역으로 구별됩니다.

전이학습을 수행하기 위해 사전 학습된 모델을 업스트림 모델이라 합니다.

미세 조정된 모델은 다운스트림 모데이라고 합니다.

업스트림 모델은 대규모 특정 도메인의 데이터세트에서 학습한 모델이며 해당 도메인에 대한 특징과 특성이 학습돼야 합니다.

전이 학습 파이프라인 중 시작 부분에 위치합니다.

다운스트림 모델은 업스트림 모델에서 학습한 지식을 활용해 작은 규모의 타깃 도메인 데이터세트에서 학습한 모델입니다.

전이 학습 파이프라인 중 마지막 부분에 위치합니다.

다운스트림 모델은 사전 학습된 모델의 계층을 하나 이상 사용하며 타깃 도메인에 적응하기 위해 소규모 데이터세트에서 미세 조정됩니다.

전이 학습에는 귀남적 전이 학습, 변환적 전이 학습, 비지도 전이 학습 등이 있습니다.

#### 귀납적 전이 학습

귀납적 전이 학습(Inductive Transfer Learning)은 기존에 학습한 모델의 지식을 활용하여 새로운 작업을 수행하기 위한 방법중 하나입니다.

이전 작업에서 학습한 지식을 새로운 작업에 활용함으로써 모델의 일반화 능력을 향상시킬 수 있습니다.

전이 학습은 기존 모델의 학습된 지식을 새로운 작업으로 이전함으로써 작업 효율성을 높이고 성능을 향사시킬 수 있습니다.

귀납적 전이 학습은 자기주도적 학습(Self-taught Learning)과 다중 작업 학습(Multi-task Learning)으로 나뉩니다.

자기주도적 학습이란 비지도 전이 학습(Unsupervised Transfer Learning)의 유형 중 하나입니다.

소스 도메인의 데이터세트에서 데이터의 양은 많으나 레이블링된 데이터의 수가 매우 적거나 없을 때 사용하는 방법입니다.

레이블이 지정되지 않은 대규모 데이이터세트에서 특징을 추출하는 오토 인코더와 같은 모델을 학습시킨 다음, 저차원 공간에서 레이블된 데이터로 미세 조정하는 방법을 의미합니다.

레이블이 지정된 데이터를 수집하는데 리소스 소모가 큰 경우 유용하게 사용할 수 있습니다.

다중 작업 학습은 레이블이 지정된 소스 도메인과 타깃 도메인 데이터를 기반으로 모델에 여러 작업을 동시에 가르치는 방법을 의미합니다.

다중 작업 학습의 모델 구조는 공유 계층(Shared Layers)과 작업별 계층(Task Specific Layer)으로 나눕니다.

공유 계층에서도 소스 도메인과 타깃 도메인의 데이터세트에서 모델을 사전 학습한 다음 단일 작업을 위해 작업별 계층마다 타깃 도메인 데이터세트로 미세 조정하는 방법으로 모델을 구성합니다.

소스 도메인과 타깃 도메인 데이터를 하나의 데이터세트로 구축하는 방법이 아니라, 작업마다 서로 다른 학습 데이터세트를 사용하여 모델을 미세 조정합니다.

공유 계층에서 서로 다른 작업의 특징을 맞추기 위해 동시에 학습되므로 하나의 작업에 과대적합 되지 않아 일반화된 모델을 얻을 수 있습니다.

서로의 동일한 도메인을 사용해 성능 향상에 기여할 수 있다는 장점도 있습니다.

#### 변환적 전이 학습

변환적 전이 학습(Transductive Transfer Learning)은 소스 도메인과 타깃 도메인이 유사하지만 완전히 동일하지 않은 경우를 의미합니다.

변환적 전이 학습에 사용되는 소스 도메인은 레이블이 존재하며, 타깃 도메인에는 레이블이 존재하지 않은 경우에 사용됩니다.

레이블이 지정된 소스 도메인으로 사전학습된 모델을 구축하며, 레이블이 지정되지 않은 타깃 도메인으로 모델을 미세 조정해 특정 작업에 대한 성능을 향상시킵니다.

변환적 전이 학습은 도메인 적응(Domain Adaptation)과 표본 선택 편향/공량 이동(Sample Selection Bias/Covariance Shift)으로 나뉩니다.

도메인 적응이란 소스 도메인과 타깃 도메인 특징 분포(Feature Distributions)를 전이시키는 방법입니다.

소스 도메인과 타깃 도메인은 유사하지만 다르므로 두 도메인의 특징 공간와 분포는 서로 다릅니다.

서로 다른 도메인들의 특징 분포를 고려해 학습하므로 도메인 변화(Domain Shift)를 확인해 전이하게 됩니다.

도메인 적응은 타깃 도메인에서 모델의 성능을 향상시키는 것이 목적이므로 소스 도메인이 조정될 수 있습니다.

표본 선택 편향/공변량 이동이란 소스 도메인과 타깃 도메인의 분산과 편향이 크게 다를 때 표본을 선택해 편향이나 공변량을 이동시키는 방법을 의미합니다.

소스 도메인과 타깃 도메인은 완전히 동일하지 않기 때문에 모델이 학습 데이터에서 좋은 성능을 보였더라도 테스트 데이터에서 성능이 좋지 않을 수 있습니다.

그래서 무작위/비무작위 샘플링 방법이나 도메인 적응을 통해 해당 학습치만 전이시키는 방법입니다.

#### 비지도 전이 학습

비지도 전이 학습(Unsupervised Transfer Learning)은 소스 도메인과 타깃 도메인 모두 레이블이 지정된 데이터가 없는 전이 학습 방법입니다.

이러한 방법은 소스 도메인에서 타깃 도메인의 성능을 개선하는데 사용할 수 있는 특징 표현을 학습합니다.

레이블이 없는 전체 데이터로 학습해 데이터가 가진 특징과 특성을 구분할 수 있게 사전 학습된 모델을 구축하고 소규모의 레이블이 지정된 데이터를 활용해 미세 조정합니다.

소스 도메인 데이터에서 감독되지 않은 모델을 교육해 일련의 기능 포현을 학습한 다음, 타깃 도메인에 대한 감독된 모델을 초기화하는 방법입니다.

비지도 전이 학습은 레이블의 영향을 받지 않고 데이터가 가진 특징을 학습을해 미세 조정 시 더 효과적으로 타깃 도메인에 대해 예측을 수행할 수 있습니다.

비지도 전이 학습의 대표적인 방법으로는 생성적 적대 신경망(Generative Adversarial Networks, GAN)과 군집합(Clustering)가 있습니다.

전이 학습은 사전 학습된 모델의 지식을 활용해 새로운 도메인에 대한 예측을 진행할 수 있으므로 작은 데이터세트를 가지고도 우수한 결과를 얻을 수 있습니다.

모델을 재사용하므로 새로운 모델을 구축하는 데 소용되는 시간과 리소스 소모를 최소화할 수 있습니다.

더 높은 학습률을 할당하더라도 유사한 작업에 대해 학습됐기 때문에 더 빠르게 학습되고 타깃 도메인에 대한 더 높은 정확도를 제공합니다.

전이 학습은 자연어 처리 및 컴퓨터 비전 등 다양한 분야에서 사용되며 소스 도메인의 데이터를 활용해 타깃 도메인에 대한 모델의 성능을 개선하고 데이터 부족 문제를 극복할 수 있습니ㅏㄷ.

자연어 처리 분야에서 사용되는 사전 학습된 모델은 Word2Vec, fastText, BERT 등이 있습니다.

컴퓨터 비전 분야에서 사용되는 사전 학습된 모델은 ResNet-50, VGG-16이 있습니다.

#### 제로-샷 전이 학습

제로-샷 전이 학습(Zero-shot Transfer Learning)은 사전 학습된 모델을 이용해 다른 도메인에서도 적용할 수 있는 전이 학습 기법 중 하나입니다.

이를 통해 새로운 도메인에서 일반화된 성능을 가질 수 있습니다.

eg) 이미지 분류 문제에서는 (’독수리’, ‘새’), (’참새’, ‘새’), (’오리’, ‘새’)등의 데이터 쌍으로 모델을 학습시킨 후, ‘부엉이’와 같은 새로운 이미지를 분류할 때도 모델이 일반화된 성능을 발휘할 수 있습니다.

‘새’라는 범주와 관련된 이미지인 ‘부엉이’를 인식하고 분류할 수 있기 때문입니다.

자연어 처리 모델에서도 사전 학습된 모델에 새로운 도메인 문장을 입력하면 이전에 학습된 단어와 문맥 정보를 비교하여 일반화된 성능을 가질 수 있습니다.

eg) ‘개’, ‘고양이’와 같은 단어를 포함하는 문장이 있을 때, 이전에 학습되지 않은 단어인 ‘호랑이’가 포함된 문장도 모델이 인식하고 이 문장이 ‘동물’과 관련되어 있다는 결론을 내릴 수 있습니다.

제로-샷 전이 학습은 새로운 도메인에서 학습할 데이터가 부족한 경우에 유용하게 사용할 수 있습니다.

다양한 도메인 간의 지식을 전이 할 수 있기 때문에 일반화된 성능을 높일 수 있다는 장점도 있습니다.

#### 원-샷 전이 학습

원-샷 전이 학습(One-shot Transfer Learning)은 제로-샷 학습과 유사하지만, 한번에 하나의 샘플만 사용해 모델을 학습하는 방법입니다.

매우 적은 양의 데이터르 이용하여 분류 문제를 해결하는데 사용됩니다.

원-샷 전이 학습 모델은 서포트 셋(Support Set)과 쿼리 셋(Query Set)을 가정합니다.

서포트 셋은 학습에 사용될 클래스의 대표 샘플을 의미합니다.

각 클래스당 하나 이상의 대표 샘플로 이루어져있습니다.

쿼리 셋은 새로운 클래스를 분류하기 위한 입력 데이터를 의미합니다.

분류 대상 데이터로, 서포트 셋에서 수집한 샘플과는 다른 샘플이어야 합니다.

마지막으로, 서포트 셋에 있는 대표 샘플과 쿼리 셋 간의 거리를 측정하여 쿼리 셋과 가장 가까운 서포트 셋의 대표 샘플의 클래스로 분류합니다.

이때 사용되는 거리 측정 방법으로는 유클리드 거리, 코사인 유사도 등이 사용됩니다.

    eg) 개와 고양이 분류 문제를 원-샷 전이 학습으로 해결하고자 할 때, 개 클래스와 고양이 클래스 각각 대표 샘플을 수집하여 서포트 셋을 생성합니다.

이때 각 클래스의 대표 샘플은 개 사진과 고양시 사진 중 하나로 선택 됩니다.

이후 분류 대상인 새로운 개사진을 쿼리 셋으로 생성하고, 서포트 셋에 있는 대표 샘플과 쿼리 셋 간의 거리를 측정합니다.

결과적으로 서포트 셋의 가장 가까운 대표 샘풀을 가진 클래스로 분류하게 됩니다.

| <center>유형 | <center> 세부 유형 | <center> 소스 도메인 | <center> 타깃 도메인 |
| --- | --- | --- | --- |
| <center> 귀납적 전이 학습 | <center> 자기주도적 학습 | <center> 레이블링 없음 | <center> 레이블링 있음 |
| <center> 귀납겆 전이 학습 | <center> 다중 작업 학습 | <center> 레이블링 있음 | <center> 레이블링 있음 |
| <center> 변환적 전이 학습 | <center> - | <center> 레이블링 있음 | <center> 레이블링 없음 |
| <center> 비지도 전이 학습 | <center> - | <center> 레이블링 없음 | <center> 레이블링 없음 |
| <center> 제로-샷 전이 학습 | <center> - | <center> 레이블링 있음 | <center> 레이블링 없음 |
| <center> 원-샷 전이 학습 | <center> - | <center> 레이블링 있음 | <center> 레이블링 없음 |

### 특징 추출 및 미세 조정

특징 추출(Feature Extraction)및 미세 조정(Fine-tuning)은 전이 학습에 사용되는 일반적인 기술을 의미합니다.

두 가지 방법 모두 대규모 데이터 세트로 사전 학습된 모델을 작은 데이터 세트로 추가학습해 가중치나 편향을 수정합니다.

이러한 방식으로 재학습된 모델은 대규모 데이터세트에서 배운 지식을 적용해 새로운 데이터세트에 맞는 지식을 제공할 수 있습니다.

특징 추출은 타깃 도메인이 소스 도메인과 유사하고 타깃 도메인의 데이터세트가 적을 때 사용됩니다.

소스 도메인과 타깃 도메인이 매우 유사하면 타깃 도메인으로 모델을 학습해도 소스 도메인의 가중치나 편향도 유사합니다.

특징 추출 계층은 동결(Freeze)해 학습하지 않고 기존에 학습된 모델의 가중치를 사용합니다.

예측 모델마다 요구하는 출력 노드의 수가 다르므로 모델의 분류기(Classifier)만 재구성해 학습합니다.

미세 조정은 특징 추출 계층을 일부만 동결하거나 동결하지 않고 타깃 도메인에 대한 학습을 진행합니다.

첫 번째로 개와 고양이를 분류하는 모델을 활용해 식물을 분류하는 모델을 구축한다고 가정하면 소스 도메인과 타깃 도메인 간 유사성이 매우 낮다고 볼 수 있습니다.

하지만 특징 추출을 위한 모델의 구조는 유효하기 때문에 모든 계층을 동결하지 않고 전체 데이터세트로 학습을 진행합니다.

이러한 방법은 데이터 세트가 충분히 많을 때 시도할 수 있습니다.

두 번째 방법도 식물을 분류하는 모델인데, 데이터세트의 크기가 작다고 가정해 하면, 타깃 도메인에 대한 데이터세트가 작다면 전체 계층을 학습할 수 없습니다.

그러므로 일부 계층만 동결해 학습을 진행합니다.

도메인간 유사성이 매우 낮지만, 하위 계층에서 저수준의 특징을 학습할 때 동일한 특징으로 학습될 가능성이 높으므로 초기 계층만 동결해 학습을 진행합니다.

세 번째는 소스 도메인과 타깃 도메인이 유사하지만, 충분한 데이터세트를 확보하지 못 했을 때입니다.

도메인간 유사성이 높으면 특징 추출 방법으로 모델을 학습할 수 있습니다.

하지만 도메인 유사성만 높을 뿐 데이터세트가 충분히 많지 않기 때문에 상위 계층으로 가면서 특징이 점점 달라집니다.

그러므로 하위 계층을 동결하고 일부 상위 계층을 학습하는 방법으로 모델을 구축합니다.

앞선 예시에서 개와 고양이를 분류하는 모델을 늑대와 사자로 구별하는 모델로 변경하는 방법도 미세 조정의 예시입니다.

![미세 조정 전략](../여름학기_딥러닝_공부/image(7).png)

미세 조정 전략

데이터의 개수와 유사성에 따라 미세 조정 전략이 달라집니다.

특징 추출은 데이터가 적고 유사도가 높기 때문에 데이터의 합성곱 계층은 학습하지 않습니다.

데이터가 적은데 유사도가 높으므로 적은 데이터를 최대한 활용하기 위해 분류기만 학습합니다.

데이터가 많고 유사도가 낮은 경우 데이터가 가진 특징이 다르기 때문에 분류기를 포함한 모델 매개변수를 다시 학습해야 합니다.

데이터도 적고 유사도도 낮다면 전체 합성곱 계층을 학습하기 어려워 타깃 도메인에 대한 성능이 떨어집니다.

이때는 최대한 성능을 끌어올리기 위해 초기 계층의 저수준 특징 추출 기능을 동결하고 나머지 계층과 분류기를 학습합니다.

마지막으로 데이터도 많고 유사도도 높다면 분류기에 가장 큰 영향을 미치는 상위 계층과 분류기를 학습합니다.

소스 도메인과 타깃 도메인이 서로 유사하다고 해도 완전히 동일한 도메인은 아니기 때문에 상위 계층의 특징은 다들 수밖에 없습니다.

이때는 모델의 성능을 최대한 끌어내기 위해 상위 계층을 학습 범위에 포함시킵니다.

전이 학습과 미세 조정은 사전 학습된 모델을 활용해 새로운 시스템을 구축하는데 가장 많이 사용되는 방법입니다.

비교적 높은 정확도의 모델을 적은 리소스를 사용해 시스템을 구축할 수 있으므로 간단하지만 강력한 방법입니다.
