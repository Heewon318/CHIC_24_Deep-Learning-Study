# 자연어
자연어 처리를 위해 해결해야하는 문제
- 모호성
- 가변성
- 구조

해결 과정
1. 토큰화
- 토크나이저 사용(공백 분할, 정규표현식 적용, 어휘 사전 적용- OOV 문제 고려, 머신러닝 활용)

🤩 OOV 문제 해결 - BPE 알고리즘
- 서브 워드 분리
- 하나의 단어를 더 작은 서브 워드들의 조합으로 구성
- 신조어, 희귀 단어 문제 해결
- 참고링크: https://f7project.tistory.com/286

### 단어 토큰화
- 단어 단위로 토큰화(띠어쓰기, 문장 부호, 대소문자 기준)
- 가장 일반적임
- split method 사용
```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized = review.split()
print(tokenized)
#공백을 기준으로 분리함
```
1. 최고! vs 최고!!

느낌표 하나 차이로 다른 의미

2. cg vs cg.

cg가 단어 사전에 있더라도 cg., cg는, cg도는 OOV가 됨.
- 한국어는 접사, 문장 부호, 오타, 띄어쓰기 오류 등에 취약

### 글자 토큰화
글자 단위로 문장을 나눔(비교적 작은 단어 사전 구축) -> 컴퓨터 자원 SAVE

```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized2 = list(review)
print(tokenized2)
#한 글자씩으로 나뉨.
```
list를 사용하면 한글의 경우 한 글자씩, 영어의 경우 알파벳 단위로 나뉨

### 자소 단위 토큰화

```
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
```
자음 모음 단위로 분리됨.
이러면 단어 토큰화의 단점인 "cg." vs "cg" 문제도 해결할 수 있고, 단어 사전도 줄일 수 있다. 

🤨단점
1. 개별 토큰은 의미 없기에 LLM이 각 토큰의 의미를 조합해서 결과를 도출해야함. 이 때 문장 생성이나 개체명 인식 등을 할 때 다의어나 동음이의어가 많은 도메인에서의 구별 어려움
2. 모델 입력 시퀀스의 길이가 길어질수록 연산량 증가

예시> 앞서 사용된 review의 경우
- 단어 토큰화: 13개
- 글자 토큰화: 53개
- 자소 단위 토큰화: 101개

### 형태소 토큰화
- 품사 태깅을 통해 문맥을 고려해 더 정확한 분석 가능
1. KoNLPy
- Okt, Kkma, Komoran, Hannanum 등
#### Okt
okt.nouns, okt.phrases, okt.morphs, okt.pos
#### NLTK
영어 자연어 처리에 최적화됨.

### 하위 단어 토큰화
😊장점
인간이 자연어를 이해하는 방식과 가장 유사함.

🤨단점
전문용어, 고유어 많은 데이터의 취약

# 임베딩
컴퓨터는 텍스트 벡터화 과정이 필요
- 원-핫 인코딩
: 문서에 등장하는 각 단어를 고유한 색인 값으로 매핑, 해당 색인 위치는 1, 나머지는 0
- 빈도 벡터화
: 해당 단어의 빈도로 표시

장점: 단어/문장을 벡터 형태로 변환 쉽고 간단

단점: 벡터가 sparse함, 차원의 저주, 의미를 내포하고 있지 않음.

🙄차원의 저주??
- 관측치보다 변수의 수가 더 많아지는 경우 발생
- 차원이 늘어나면서 sparse해짐.(빈공간이 늘어남)
- knn 알고리즘에는 특히 안 좋음.
---
- 워드 임베딩
: 단어를 고정된 길이의 실수 벡터로 표현하는 방법, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계를 추론
- 동적 임베딩: 다의어나 문맥 정보를 다루기 쉽게 함, 인공 신경망 활용

### 언어 모델
- 자기회귀 언어 모델
:입력된 문장들의 조건부 확률을 이요해 다음에 올 단어 예측
- 통계적 언어 모델
: 언어의 통계적 구조를 이용해 문장이나 단어의 시퀀스를 생성 또는 분석함.
### N-gram
텍스트에서 n개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정함.(가장 기초적인 통계적 언어 모델)
- 입력 텍스트를 하나의 토큰 단위로 분석하지 않고 N개의 토큰을 묶어서 분석함.
- 유니그램, 바이그램, 트라이그램, N-gram
### TF-IDF
텍스트 문서에서 특정 단어의 중요도를 계산하는 방법, 문서 내 단어의 중요도를 평가하는 데 사용되는 통계적인 가중치를 의미함.

: BoW(문서나 문장을 단어의 집합으로 표현하는 방법)에 가중치를 부여하는 방법

#### 단어 빈도 TF
: 문서 내에서 특정 단어의 빈도수를 나타내는 값
- TF값이 높을 수록 해당 단어가 특정 문서에서 중요한 역할을 한다고 생각할 수도 있지만, 단어 자체가 특정 문서 내에서 자주 사용되는 단어이므로 전문 용어나 관용어로 간주할 수 있음.
-문서 길이가 길어질수록 TF 값도 높아짐.

#### 문서 빈도 DF
: 한 단어가 얼마나 많은 문서에 나타나는지를 의미함.
- 특정 단어가 많은 문서에 나타나면 문서 집합에서 단어가 나타나는 횟수를 계산함.
- DF 값이 높으면 특정 단어가 많은 문서에서 등장한다고 볼 수 있음. 해당 단어는 일반적으로 널리 사용되며, 중요도가 낮을 수 있음.
- DF 값이 낮으면 적은 수의 문서에만 등장한다는 뜻. 특정 문맥에서만 사용되는 단어일 가능성 높음 -> 중요도 높을 수 있음

#### 역문서 빈도 IDF
: 전체 문서 수를 문서 빈도로 나눈 다음에 로그를 취한 값
- 문서 내에서 특정 단어가 얼마나 중요한지를 나타냄.
- 문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는 의미가 됨.
->문서 빈도의 역수를 취해 단어의 빈도수가 적을수록 IDF 값이 커지게 보정함.
- 문서에서 특정 단어의 등장 횟수가 적으면 IDF는 상대적으로 커짐

#### TF-IDF
: 문서 빈도와 역문서 빈도를 곱한 값
TF x IDF

- 문서 내에 단어가 자주 등장하지만 전체 문서 내에 적게 등장하는 경우 TF-IDF 값은 커진다. -> 특이한 단어일 경우 값이 커짐
- 전체 문서에서 자주 등장할 확률이 높은 관사나 관용어 등의 가중치는 낮아짐.
- 사이킷런 사용함.

### 빈도 기반 벡터화의 단점
- 문장의 순서나 문맥 고려 X
-> 문장 생성과 같이 순서가 중요한 작업에는 부적합함

---
언어 모델의 역할은?
1. 예측: 감정 분석 등
2. 생성: 모델에 절이나 구 입력 시 다음 단어 예측, 문장 생성

통계적 언어 모델 vs 자기회귀 언어 모델
- 두개가 같은 거임.

자기회귀 언어 모델 vs N-gram
- 자기회귀는 이전에 등장한 모든 단어를 고려하며, 확률을 차례로 곱해나감.
- 엔그램은 모든 토큰을 사용하지 않고 사용자가 지정한 n - 1개의 토큰을 고려함
---
# Word2Vec
- 2013년 구글에서 공개한 임베딩 모델, 단어 간의 유사성을 측정하기 위해 분포가설을 기반으로 개발됨.
- 분포 가설: 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높음.
- 즉, 단어 간의 동시 발생 확률 분포를 이용해 단어 간의 유사성을 측정함.
### 단어 벡터화
1. 희소 표현
- 원-핫 인코딩, TF-IDF 등 빈도 기반 방법
- data가 sparse할 수밖에 없는 듯.
- 공간적 낭비 발생, 단어간의 유사성 반영 못 함, 벡터 간의 유사성 계산 많은 비용 발생
2. 밀집 표현
- Word2Vec
- 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기는 커지지 않음.
- 효율적인 공간 활용이 가능함.
- 단어 간의 거리를 효과적으로 계산할 수 있음
-CBoW, Skip-gram 사용

#### CBoW
- 주변에 있는 단어를 가지고 중간에 있는 단어 예측
- 중심 단어: 예측 대상
- 주변 단어: 예측에 사용되는 단어들
- 윈도: 중심 단어 예측을 위해 주변 몇 개의 단어를 고려할지를 정해야하는데 이 범위를 뜻함.
- 슬라이딩 윈도: 학습을 위해 윈도를 이동해 가며 학습하는 것
- CBoW는 슬라이딩 윈도를 통해 한 번의 학습으로 여러 개의 중심 단어와 그에 맞는 주변 단어를 학습할 수 있음.
- 입력 값: 입력 단어의 원-핫 벡터
- 하나의 윈도에서 하나의 학습 데이터가 만들어짐.

#### Skip-gram
- CBoW와 반대
- 중심단어를 입력받아 주변 단어를 예측
- 하나의 윈도에서 여러 학습 데이터가 나옴.
- 이게 더 학습 데이터가 많다보기 CBoW보다는 성능이 좋음.

# 계층적 소프트맥스
:출력층을 이진 트리 구조로 표현하여 연산 수행
- 자주 등장할 수록 상위 노드, 드물게 등장할 수록 하위 노드
- 일반 소프트맥스 연산 보다 더 빠르고 효율적
- 소프트맥스 연산의 시간 복잡도에서 log2를 씌운 값이 됨.
### 네거티브 샘플링
- Word2Vec 모델에서 사용되는 확률적 샘플링 기법
- 전체 단어 집합에서 일부 단어 샘플링하여 오답 단어로 사용
- 학습 윈도 내에 등장하지 않는 단어를 n개 추출 후 정답 단어와 함께 소프트맥스 연산 수행
- 전체 단어의 확률을 계산할 필요가 없어 모델을 효율적으로 학습할 수 있음.
- n은 보통 5~20정도
---
### 소프트맥스 word2vec vs 네거티브 word2vec
- word2vec의 등장 배경: one-hot은 단어 벡터간 유의미한 유사도를 계산할 수 없음. 원핫은 0과 1로만 구성되어서 데이터가 sparse해짐.

->단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 해야함 = word2vec, 확률로 표현함.

- 네거티브는 입력 단어의 임베딩과 해당 단어가맞는지를 판단해 1과 0으로 내적 연산을 수행함. 이 값을 시그모이드 함수에 넣어 확률값으로 변환.
- 레이블이 1이면 확률값을 높이고, 0이면 낮춘다.
 


304쪽부터 공부해야함
