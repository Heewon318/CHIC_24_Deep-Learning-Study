# 자연어
자연어 처리를 위해 해결해야하는 문제
- 모호성
- 가변성
- 구조

해결 과정
1. 토큰화
- 토크나이저 사용(공백 분할, 정규표현식 적용, 어휘 사전 적용- OOV 문제 고려, 머신러닝 활용)

🤩 OOV 문제 해결 - BPE 알고리즘
- 서브 워드 분리
- 하나의 단어를 더 작은 서브 워드들의 조합으로 구성
- 신조어, 희귀 단어 문제 해결
- 참고링크: https://f7project.tistory.com/286

### 단어 토큰화
- 단어 단위로 토큰화(띠어쓰기, 문장 부호, 대소문자 기준)
- 가장 일반적임
- split method 사용
```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized = review.split()
print(tokenized)
#공백을 기준으로 분리함
```
1. 최고! vs 최고!!

느낌표 하나 차이로 다른 의미

2. cg vs cg.

cg가 단어 사전에 있더라도 cg., cg는, cg도는 OOV가 됨.
- 한국어는 접사, 문장 부호, 오타, 띄어쓰기 오류 등에 취약

### 글자 토큰화
글자 단위로 문장을 나눔(비교적 작은 단어 사전 구축) -> 컴퓨터 자원 SAVE

```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized2 = list(review)
print(tokenized2)
#한 글자씩으로 나뉨.
```
list를 사용하면 한글의 경우 한 글자씩, 영어의 경우 알파벳 단위로 나뉨

### 자소 단위 토큰화

```
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
```
자음 모음 단위로 분리됨.
이러면 단어 토큰화의 단점인 "cg." vs "cg" 문제도 해결할 수 있고, 단어 사전도 줄일 수 있다. 

🤨단점
1. 개별 토큰은 의미 없기에 LLM이 각 토큰의 의미를 조합해서 결과를 도출해야함. 이 때 문장 생성이나 개체명 인식 등을 할 때 다의어나 동음이의어가 많은 도메인에서의 구별 어려움
2. 모델 입력 시퀀스의 길이가 길어질수록 연산량 증가

예시> 앞서 사용된 review의 경우
- 단어 토큰화: 13개
- 글자 토큰화: 53개
- 자소 단위 토큰화: 101개

### 형태소 토큰화
- 품사 태깅을 통해 문맥을 고려해 더 정확한 분석 가능
1. KoNLPy
- Okt, Kkma, Komoran, Hannanum 등
#### Okt
okt.nouns, okt.phrases, okt.morphs, okt.pos
#### NLTK
영어 자연어 처리에 최적화됨.

### 하위 단어 토큰화
😊장점
인간이 자연어를 이해하는 방식과 가장 유사함.

🤨단점
전문용어, 고유어 많은 데이터의 취약

# 임베딩
컴퓨터는 텍스트 벡터화 과정이 필요
- 원-핫 인코딩
: 문서에 등장하는 각 단어를 고유한 색인 값으로 매핑, 해당 색인 위치는 1, 나머지는 0
- 빈도 벡터화
: 해당 단어의 빈도로 표시

장점: 단어/문장을 벡터 형태로 변환 쉽고 간단

단점: 벡터가 sparse함, 차원의 저주, 의미를 내포하고 있지 않음.

🙄차원의 저주??
- 관측치보다 변수의 수가 더 많아지는 경우 발생
- 차원이 늘어나면서 sparse해짐.(빈공간이 늘어남)
- knn 알고리즘에는 특히 안 좋음.
---
- 워드 임베딩
: 단어를 고정된 길이의 실수 벡터로 표현하는 방법, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계를 추론
- 동적 임베딩: 다의어나 문맥 정보를 다루기 쉽게 함, 인공 신경망 활용
