# 자연어
자연어 처리를 위해 해결해야하는 문제
- 모호성
- 가변성
- 구조

해결 과정
1. 토큰화
- 토크나이저 사용(공백 분할, 정규표현식 적용, 어휘 사전 적용- OOV 문제 고려, 머신러닝 활용)

🤩 OOV 문제 해결 - BPE 알고리즘
- 서브 워드 분리
- 하나의 단어를 더 작은 서브 워드들의 조합으로 구성
- 신조어, 희귀 단어 문제 해결
- 참고링크: https://f7project.tistory.com/286

### 단어 토큰화
- 단어 단위로 토큰화(띠어쓰기, 문장 부호, 대소문자 기준)
- 가장 일반적임
- split method 사용
```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized = review.split()
print(tokenized)
#공백을 기준으로 분리함
```
1. 최고! vs 최고!!

느낌표 하나 차이로 다른 의미

2. cg vs cg.

cg가 단어 사전에 있더라도 cg., cg는, cg도는 OOV가 됨.
- 한국어는 접사, 문장 부호, 오타, 띄어쓰기 오류 등에 취약

### 글자 토큰화
글자 단위로 문장을 나눔(비교적 작은 단어 사전 구축) -> 컴퓨터 자원 SAVE

```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized2 = list(review)
print(tokenized2)
#한 글자씩으로 나뉨.
```
list를 사용하면 한글의 경우 한 글자씩, 영어의 경우 알파벳 단위로 나뉨

### 자소 단위 토큰화

```
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
```
자음 모음 단위로 분리됨.
이러면 단어 토큰화의 단점인 "cg." vs "cg" 문제도 해결할 수 있고, 단어 사전도 줄일 수 있다. 

🤨단점
1. 개별 토큰은 의미 없기에 LLM이 각 토큰의 의미를 조합해서 결과를 도출해야함. 이 때 문장 생성이나 개체명 인식 등을 할 때 다의어나 동음이의어가 많은 도메인에서의 구별 어려움
2. 모델 입력 시퀀스의 길이가 길어질수록 연산량 증가

예시> 앞서 사용된 review의 경우
- 단어 토큰화: 13개
- 글자 토큰화: 53개
- 자소 단위 토큰화: 101개

### 형태소 토큰화
- 품사 태깅을 통해 문맥을 고려해 더 정확한 분석 가능
1. KoNLPy
- Okt, Kkma, Komoran, Hannanum 등
#### Okt
okt.nouns, okt.phrases, okt.morphs, okt.pos
#### NLTK
영어 자연어 처리에 최적화됨.

### 하위 단어 토큰화
😊장점
인간이 자연어를 이해하는 방식과 가장 유사함.

🤨단점
전문용어, 고유어 많은 데이터의 취약
