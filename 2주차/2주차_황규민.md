## 과대적합과 과소적합(Over fitting, Under fitting)

과대적합 : 훈련데이터에선 우수하게 예측하지만 새로운 데이터에 제대로 예측하지 못하는 현상

과소적합 : 훈련데이터에서도 성능이 좋지 않다

공통점: **성능 저하**

- 모델 선택 실패
    - 과대, 모델구조 복잡해 훈련 데이터에 의존
    - 과소, 모델구조 단순해 특징 학습을 제대로 못함
- 편향-분산 트레이드오프
    - 모델이 훈련과 새로운데이터에 우수한 성능을 보이려면 낮은 편향과 낮은 분산을 가져야 함.
    - 분산이 높으면 추정치(Estimate)에 대한 변동이 커짐, 데이터가 갖고 있는 노이즈까지 학습 
    과정에 포함하게 되 과대적합 문제 발생
    - 편향이 높으면 추정치가 항상 일정한 값을 갖게 될 확률이 높아져 데이터의 특징을 제대로 학습하지 못함
        - 편향과 분산은 서로 반비례, 모델이 복잡해질수록 분산은 커지고 편향은 작아진다.

### 과대적합과 과소적합 문제 해결

과대적합은 모델의 일반화(Generalizatiion)능력을 저하해 발생, 과서적합은 모델이 데이터 특징을 제대로 학습할 수 없을 때 발생

- 데이터 수집 : 학습 데이터의 수를 늘린다.
- 피처 엔지니어링 : 학습하기 쉬운 형태로 데이터를 변환
- 모델 변경 : 과대 적합 경우 간단한 모델로 과소의 경우 더 복잡한 모델로 변경
- 조기 중단 : 과대 적합이 발생하기 전에 모델 학습을 중단
- 배치 정규화 : 모델의 계층 마다 평균과 분산을 조정해 내부 공변량을 줄여 과대 적합을 방지
- 가중치 초기화 : 적절한 초기 가중치를 설정해 과대적합을 방지
- 정칙화(Regularization) : 목적 함수에 페널티를 부여하는 방법

## 배치 정규화(Batch Normalization)

내부 공변량 변화(INternal Covariate Shift)를 줄여 과대 적합을 방지하는 기술

상위 계층의 매개변수가 갱신될 때 마다 현재 계층에 전달되는 데이터의 분포로 변경

- 일반적으로 입력값을 배치 단위로 나눠 학습을 진행

내부 공변량이란 계층마다 입력 분포가 변경되는 현상을 의미

이 문제를 해결하기 위해 각 계층에 배치 정규화를 적용

- 미니 배치의 입력을 정규화 하는 방식으로 동작
    - 입력이 일반화 되고 독립적으로 정규화가 수행되 더 빠르게 값을 수렴

### 정규화 종류

배치 정규화 : 미니 배치에 계산된 평균 및 분산을 기반으로 계층의 입력을 정규화, CNN,MLP,FNN

계층 정규화(Layer Normalization) : 이미지 데이터 전체를 대상으로 하지 않고 각각의 이미지 데이터에 채널별로 정규화를 수행, RNN,TS

- 채널 축으로 계산하기 때문에 미니배치 샘플간의 의존관계가 없음, 샘플이 서로 다른길이를 가져도 정규화 수행가능

인스턴스 정규화(Instance Normalization): 채널과 샘플을 기준으로 정규화, 입력이 다른 분포를 갖는 작업에 적합, GAN,(Style Transfer모델)

그룹 정규화(Group Normalization): 채널을 N개의 그룹으로나누고 각 그룹내에서 정규화, 배치 정규화의 대안

### 배치 정규화

$y_i=\dfrac{x_i-E[X]}{\sqrt{Var[X]+\epsilon}}*\gamma+\beta$

$E[X]$ : 산술 평균, $Var[X]$ : 분산, $\epsilon$ : 분모가 0이 되는 현상을 방지하는 작은 상수, 
$\gamma,\beta$ : 매개변수 활성화 함수에서 발생하는 스케일(Scale), 시프트(Shift) 값

## 가중치 초기화(Weight Initialzation)

모델의 초기 가중치 값을 설정하는 것, 적절한 초깃값을 설정한다면 기울기 폭주나 기울기 소실문제를 완화할 수 있다.

### 상수 초기화

대표적으로 0, 1, 특정값, 단위행렬, 디랙 델타 함수(Dirac Delta Function)

- 간단하고 계산 비용이 거의 들지 않음, 일반적으로 사용되지 않는 초기화 방법
    - 배열 구조의 가중치에서 문제 발생, 대칭 파괴(Breaking Symmetry)현상으로 모델 학습 어려움

### 무작위 초기화

초기 가중치 값을 무작위 값이나 특정 분포 형태로 초기화 하는 것

대표적으로 무작위, 균등 분포(Unifomr Distribution), 정규 분포(Normal Distribution), 잘린 정규 분포(Truncated Normal Distribution), 희소 정규 분포 초기화(Sparse Normal Distribution Initialization)

- 네트워크가 할습할 수 있고 대칭 파괴 문제를 방지할 수 있음, 간단하고 많이 사용하는 방법
    - 계층이 적거나 하나만 있는 경우에는 보편적, 계층이 많아질수록 기울기 소실 현상 발생
        - 결론적으로 상수 초기화와 같은 문제 발생

### 제이비어 & 글로럿 초기화(Xavier & Glorot Initialization)

균등 분포나 정규 분포를 사용해 가중치를 초기화 하는 방법

확률 분포 초기화 방법과 차이점은 동일한 표준 편차를 사용하지 않고 은닉층의 노드 수에 따라 다른 표준 편차를 할당한다는 점

이전 계층의 노드 수와 다음 계층으 노드 수에 따라 표준 편차가 계산됨

시그모이드, 하이퍼볼릭 탄전트 활성화 함수로 사용되는 네트워크에 효과적

### 카이밍&허 초기화(Kaiming, He)

균등 분포나 정규 분포를 사용해 가중치를 초기화하는 방법, 글로럿과 차이점은 **현재 계층**의 입력 뉴런 수를 기반으로만 가중치를 초기화

각 노드의 출력 분산이 입력 분산과 동일하게 만들어 ReLU 함수의 죽은 뉴런 문제를 최소화,

ReLU함수를 사용하는 네트워크에서 효과적

### 직교 초기화(Orthogoanl)

특잇값 분해(singular value Decomposition, SVD)를 활용해 자신을 제외한 모든 열, 행 벡터들과 
직교이면서 동시에 단위 벡터인 행렬을 만드는 방법, LSTM GRU 같은 순환 신경망(RNN) 주로 사용

직교 행렬의 고윳값의 절댓값은 1이기 때문에 행렬 곱을 여러 번 수행해도 기울기 소실,폭주 문제 방지

가중치 행렬의 고윳값이 1에 가까워지도록 해 RNN에서 기울기가 소실 문제를 방지하는데 사용

- 모델이 특정 초기화 값에 지나치게 민감해지므로 순방향 신경망에는 사용하지 않음

## 정칙화(Regularization)

모델 학습 시 발생하는 과대적합 문제를 방지하기 위해 사용,  손실함수에  규제(Penalty)주는 방법

- 일반화 성능을 향상시킴
- 작은 차이에 덜 민감해져 분산 값이 낮아짐
- 데이터 학습시 읜존하는 특징의 수를 줄여 모델의 추론 능력을 개선

일반화란 새로운 데이터에서도 정확한 예측을 할 수 있음을 의미

### L1 정칙화(L1 Regularization)

라쏘 정규화라고도 함, L1 Norm 방식을 사용해 규제하는 방법, 벡터 또는 행렬값의 절댓값 합계 계산

- 손실함수에 가중치 절대값의 합을 추가해 과대적합을 방지
- 학습은 비용이 0이 되는 방향으로 진행, 손실함수에 값을 추가하면 오차가 더 커짐
    - 모델의 가중치 절댓값의 합도 최소가 되는 방향으로 진행 (작은 값은 0으로 수렴)

$L_1=\lambda*\displaystyle\sum^n_{i=0}\left|w_i\right|$

$\lambda$:규제 강도 0이상의 값: 0에 가까워질수록 모델은 더 많은 특징을 사용

- 모델의 가중치를 0으로 만드는 경우로 희소한 모델이 될 수 있음
- 가중치 절댓값의 합을 사용해 계산 복잡도를 높이게 됨
- 미분할수 없어 역전파 계산에 더 많은 리소스 소모
- 주로 선형 모델에 적용, 선형 회귀모델에 적용해 라쏘회귀

### L2 정칙화(L2 Regularization)

릿지 정규화라고도 함, L2 Norm 방식을 사용해 규제하는 방법, 벡터 또는 행렬 값의 크기를 계산

- 손실함수에 가중치 제곱의 합을 추가해 과대적합을 방지
- L1 과 다른점 하나의 특징이 너무 중요한 요소가 되지 않도록 규제
- 가중치를 0으로 만들지 않고 0에 가깝게 만듬
- 계산 복잡도 문제는 여전히 있음

$L_2=\lambda*\displaystyle\sum^n_{i=0}\left|w^2_i\right|$

정칙화를 적용하는 방법은 손실 함수에 규제 값을 더해주는 방법으로 적용되므로 평균 제곱 오차 값에 정칙화 값을 더한다.

조기 중지(early stop),드롭아웃(drop out)과 같은 기술과 함께 사용, 주로 심층 신경망 모델에서 사용, 선형 회귀 모델에서 사용할 때 릿지회귀라 함

### 가중치 감쇠(Weight Decay)

더 작은 가중치를 갖도록 손실함수에 규제를 가하는 방법, 손실함수에 규제 항을 추가하는 기술 자체를 의미

L2 정칙화를 간단하게 적용하는 방법, 갖고 있는 장점과 단점 그대로 포함

### 모멘텀(Momentum)

경사 하강법 알고리즘의 변형, 이전에 이동했던 방향과 기울기의 크기를 고려해 가중치를 갱신

- 지수 가중 이동평균 사용, 이전 기울기 값의 일부를 현재 기울기 값에 추가해 가중치 갱신
    - 일종의 관성효과를 얻을 수 있음

$v_i=\gamma v_{i-1}+\alpha\nabla f(W_i)$,  $W_{i+1}=W_i-v_i$ $v_i$: 이동벡터

$v_{i-1}$: 이전 모멘텀 값, 감마: 모멘텀 계수 0.0~1.0 주로 0.9 사용

### 엘라스틱 넷(Elastic-Net)

L1과 L2 정칙화를 결합해 사용하는 방식, 희소성과 작은 가중치의 균형을 맞춤

- L1 희박한 가중치를 갖게 규제
- L2큰 가중치를 갖지 않게 규제

두 정칙화 방식의 선형 조합으로 사용하며 혼합 비율을 설정해 가중치를 규제, 혼합 비율은 $\alpha$로 설정

- 0~1 값 사용

$Elastic-Net=\alpha\times L_1+(1-\alpha)\times L_2$

각 장점을 최대한 활용할 수 있지만, 혼합비율 조정을 위해 더 많은 튜닝 또 더 많은 리소스가 필요

### 드롭아웃(Drop out)

모델 훈련과정에서 일부 노드를 일정 비율로 제거하거나 0으로 설정해 과대적합을 방지하는 간단하고 효율적인 방법

과대적합이 발생하는 이유 중 노드 간 동조화현상, 특정 노드에 의존성이 생겨 일부 제거해 억제

- 모델 평균화 효과를 얻기 위해 여러번 훈련해야 해서 시간이 늘어남
- 모든 노드를 사용하지 않기 때문에 데이터세트가 많아야 효과
    - 모든 노드가 균일하게 학습될 수 없음

충분한 데이터세트와 깊은 모델에 적용, 일반적으로 배치 정규화와 동시에 사용하지 않음

- 서로 정칙화 효과를 방해할 수 있음
- 고로 드롭아웃 후 배치 정규화 순으로 적용
    - 둘다 모델 학습할 때만 적용, 추론하는 과정에서는 삭제하지 않고 모든 노드를 사용해 예측

### 그레디언트 클리핑(Gradient Clipping)

기울기가 커지는 현상을 방지하는데 사용되는 기술, 높은 가중치는 높은 분산 값을 갖아 성능 저하

가중치의 최댓값을 규제, 최대 임곗값을 초과하지 않도록 기울기를 잘라 설정한 임곗값으로 변경

$w=\gamma\dfrac{w}{\|w\|}\space\space if:\|w\|>\gamma$

가중치 노름이 최대 임곗값 감마보다 높은 경우에 수행

최대 임곗값을 넘는 경우 기울기 벡터의 방향을 유지하며 기울기를 잘라 규제함 
일반적으로 그레디언트 클리핑은 L2 노름을 사용해 최대 기울기를 규제함

감마값을 하이퍼파라미터로 최대 임곗값 설정, 0.1 or 1과 같은 작은 크기의 임곗값을 적용함

- 학습률을 조절하는 것과 비슷한 효과를 얻을 수 있음

RNN이나 LSTM모델을 학습하는 데 주로 사용, 두 모델은 기울기 폭주에 취약해 유용함

- 가징치 값에 대한 엄격한 제약 조건을 요구, 큰 기울기에 민감한 상황에 유용

매개변수 기울기의 전체 노름 단일 벡터를 반환, 정규화된 기울기는 반환하지 않고 매개변수를 직접 수정함

역전파를 수행한 이우 최적화 함수를 반영하기 전에 호출

- 모델의 매겨변수와 임곗값을 인수로 사용, 임겠값을 초과하는 경우 기울기를 임곘값으로 자르기때문에 구문 사이에 사용

## 데이터 증강 및 변환(Data Augmentation)

데이터가 가진 **고유한 특징을 유지한 채** 변형하거나 노이즈를 추가해 데이터세트의 크기를 인위적으로 늘리는 방법, 과대적합을 줄이고 일반화 능력을 향상시킬수 있음

- 데이터의 형질이 유지되 모델의 분산과 편향을 줄인다
- 데이터 수집 시 잘못된 정보가 들어오는 문제가 발생하지 않는다
- 특정 클래스의 데이터수가 적은 경우 데이터 증강을 통해 데이터 블균형을 완화함
- 기존 데이터가 가진 특징을 파괴하지 않게 사용하는 것이 중요
- 특정 알고리즘을 적용해 생성하므로 데이터 수집보다 더 많은 비용이 들 수도 있음

### 텍스트 데이터

방법: 삽입, 삭제, 교체, 대체, 생성, 반의어, 맞춤법 교정, 역번역 등

자연어 처리 데이터 증강(NLPAUG)라이브러리를 활용, 음성 데이터 증강도 지원

삽입 : 의미 없는 문자나 단어 또는 문장 의미에 영향을 끼치지 않는 수식어 등을 추가하는 방법

삭제 : 삽입과 반대로 임의의 단어나 문자를 삭제해 데이터의 특징을 유지하는 방법

- 두 방법 모두 문장의 의미는 유지한 채 시퀀스를 변경하는 간단하고 강력한 증강 기법
    - 적절한 양을 사용해야 함

교체 : 단어나 문자의 위치를 교환하는 방법, 데이터의 특성에 따라 주의해 사용

대체 : 단어나 문자를 임의의 단어나 문자로 바꾸거나 도의어로 변경하는 방법을 의미

- 데이터의 정합성이 어긋나지 않아서 효율적으로 데이터를 증강
    - 조사가 어색해질 수도 있음

역번역: 입력 텍스트를 특정 언어로 번역한 다음 다시 본래 언어로 변역하는 방법

- 패러프레이징(Paraphrasing)효과를 얻을 수 있다
- 번역 모델의 성능에 크게 좌우 됨

### 이미지 데이터

이미지 처리 모델을 구성할 때 데이터세터의 크기를 쉽게 늘리기 위해 사용

방법: 회전, 대칭, 이동, 크기조정 등 torchvision 라이브러리로 사용

회전 및 대칭 : 변형된 이미지가 들어오더라도 더 강건한 모델을 구축 가능

- 과도하게 증강되지 않도록

자르기 및 패딩: 이미지를 잘라 불필요한 특징을 감소, 패딩을 주어 이미지 크기를 동일한 크기

크기 조정: 학습 데이터에 사용되는 이미지 크기 조정

변형: 기하학적 변환(Geometric Transform)을 통해 이미지 변형

- 아핀 변환(Affine Transformation) : 2*3 행렬을 사용하며, 행렬 곱셈에 합을 활용해 표현할 수 있는 변환을 의미
- 원근 변환(Perspective Transformation) : 3*3행렬을 사용하며, Homography로 모델링할 수 있는 변환을 의미

색상 변환: 일반화 효과를 얻고, 형태가 더 중요한 경우 형태를 유지하면서 색상 톤을 낮춤

노이즈: 주로 합성곱 연산을 통해 진행, 3*3 픽셀 영역을 합성곱 연산하면 1*1 크기의 특징이 계산 됨

- 픽셀값에 따라 특징을 추출하는 매개변수가 달라질 수 있음
- 일반화 성능을 높이고 강건성(Robustness)을 평가하는데 사용

컷아웃(Cutout) 및 무작위 지우기(Random Erasing)

컷아웃: 임의의 사각형 영역을 삭제하고 0의 픽셀 값으로 채우는 방법

- 동영상에서 폐색영역(Occlusion) 대해 모델이 더 강건하게 됨

무작위 지우기: 임의의 사각형 영역을 삭제하고 무작위 픽셀값으로 채우는 방법

- 일부 영역이 누락되거나 잘렸을 때 더 강ㅇ건한 모델을 만듬
    - 두 가지 방법 모두 이미지의 객체가 일부 누락되더라도 모델을 견고하게 만드는 증강 방법

혼합(Mixup) 및 컷믹스(CutMix)

혼합 : 두 개 이상의 이미지를 혼합해 새로운 이미지를 생성하는 방법, 픽셀값을 선형으로 결합

컷믹스: 이미지 패치 영역에 다른 이미지를 덮어씌우는 방법, 이미지의 특정 영역을 기억해 인식하는 문제를 완화 (일부 데이터에만 적용)

### 사전 학습된 모델(Pre-trained Model)

이미 학습이 완료된 모델을 의미, 임베딩 벡터를 활용해 모델을 구성할 수 있음.

대규모 데이터에서 학습한 지식을 활용하여 소량의 데이터로도 우수한 성능을 달성할 수 있다.

### 백본(Backbone)

입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 딥러닝 모델의 일부를 의미

- 모델을 처음부터 구성하는 것이 아니라 객체를 검출하는 합성곱 신경망의 특징값을 가져와 최종계층을 바꿔 기존모델과 다른 모델을 구성
- 과대적합을 방지하기 위해 정규화 또는 정칙화 같은 기술을 적용하는 것을 권장
- 다양한 백본을 적용해가며 성능을 모니터링
- 초 대규모 딥러닝 모델을 사용

### 전이 학습(Transfer Learning)

이미 사전에 학습된 모델을 재사용해 새로운 작업이나 관련 도메인의 성능을 향상시키는 기술

- 소스 도메인에서 학습한 지식을 재사용함으로써 더 적은 데이터와 학습시간으로 더 높은 성능
    - 업스트림: 사전 학습된 모델
    - 다운스트림: 미세 조정된 모델

귀납적(Inductive) 전이 학습 : 기존에 학습한 모델의 지식을 활용해 새로운 작업을 수행

- 자기주도적(Self-taught) 학습: 비지도 전이학습의 한 유영으로 레이블링 된 데이터의 수가 적거나 없을 때 사용
    - 오토 인코더로 특징 추출 후 저차원 공간에서 레이블링된 데이터로 미세 조정하는 방법
- 다중 작업(Multi-task) 학습: 레이블이 지정된 소스 도메인과 타깃 도메인 데이터를 기반으로 모델에 여러 작업을 동시에 가르치는 방법
    - 공유 계층(Shared Layers)과 작업별 계층(Task Specific Layer) 으로 나뉨
        - 데이터세트에서 모델을 사전 학습한 다음 단일 작업을 위해 작업별 계층마다 타깃 도메인 데이터세트로 미세 조정하는 방법으로 모델을 구성, 작업마다 서로 다른 학습 데이터세트 사용
            - 서로 다른 작업의 특징을 맞추기 위해 동시에 학습해, 하나의 작업에 과대적합을 방지

변환적(Transductive) 전이 학습 : 소스 도메인과 타깃 도메인이 유사한 경우, 소스도메인은 레이블이 존재 타깃 도메인은 레이블이 존지하지 않는 경우 사용, 소스 도메인으로 사전 학습된 모델을 구축해 타깃 도메인으로 모델을 미세 조정해 특정 작업에 대한 성능을 향상

- 도메인 적응(Domain Adaptation)과 포본 선택 편향/공변량 이동(Sample Selection Bias/Covariance Shift)
    - 도메인 적응: 소스 도메인과 타깃 도메인의 특징 분포를 전이시키는 방법
        - 두 도메인은 유사하지만 다르므로 특징 공간과 분포는 서로 다름, 특징 분포를 고려해 학습하므로 도메인 변화를 확인해 전이하게 됨, 타깃 도메인에서 모델 성능을 향상시키는 것이 목적으로 소스 도메인이 조정될 수 있음
    - 포본 선택편향/공변량 이동 : 두 도메인의 분산과 편향이 크게 다를 때 표본을 선택해 편향이나 공변량을 이동시키는 방법, 두 도메인이 완전히 동일하지 않기 때문에 학습 데이터에서 좋은 성능을 보였더라도 테스트 데이터에서 성능이 좋지 않을 수 있음
        - 무작위/비무작위 샘플링 방법이나 도메인 적응을 통해 해당 학습치만 전이하는 방법

비지도(Unsupervised) 전이 학습 : 두 도메인 모두 레이블이 없는 전이 학습 방법

- 전체 데이터로 학습해 데이터가 가진 특징과 특성을 구분할 수 있게 사전 학습된 모델을 구축하고 소규모 레이블이 지정된 데이터를 활용해 미세 조정함. 소스 도메인 데이터에서 감독되지 않은 모델을 교육해 일련의 기능 표현을 학습한 다음, 타깃 도메인에 대한 감독된 모델을 초기화 하는 방법
    - 레이블의 영향을 받지 않고 데이터가 가진 특징을 학습했으므로 미세 조정 시 더 효과적인 타깃 도메인에 대해 예측을 수행할 수 있다.
        - 생성적 적대 신경망(Generative Adversarial Networks, GAN)
        - 군집화(Clustering)

유사한 작업에 대해 학습됐기 때문에 더 빠르게 학습되고 타깃 도메인에 대한 더 높은 정확도를 제공

제로-샷(Zero-shot) 전이 학습 : 다른 도메인에도 적용할 수 있는 전이 학습 기법 중하나, 이를 통해 새로운 도메인에서 일반화된 성능을 갖음

- 새로운 도메인에서 학습할 데이터가 부족한 경우에 유용하게 사용함, 다양한 도메인 간의 지식을 전이해 일반화 성능 높임

원-샷(One-shot) 전이 학습 : 한번에 하나의 샘플만 사용해 모델을 학습, 매우 적은 양의 데이터를 이용하여 분류 문제를 해결

- 소포트 셋(Support Set) : 학습에 사용될 클래스의 대표 샘플을 의미, 각 클래스 당
- 쿼리 셋(Query Set) : 새로운 클래스를 분류하기 위한 입력데이터 의미, 서포트셋에서 수집한 샘플과는 다른 샘플이어야 함
    - 서포트셋의 대표 샘플과 쿼리 셋간의 거리를 측정해, 쿼리 셋과 가장 가까운 서포트셋의 대표 샘플의 클래스로 분류, 이때 거리는 유클리드 거리와 코사인 유사도 등이 사용

### 특징 추출(Feature Extraction) 및 미세 조정(Fine-tuning)

대규모 데이터세트로 사전 학습된 모델을 작은 데이터세트로 추가 학습해 가중치나 편향을 수정

- 대규모 데이터세트에서 배우 지식을 적용해 새로운 데이터세트에 맞는 지식을 제공

특징추출: 두 도메인이 유사하고 타깃 도메인의 데이터세트가 적을 때 사용, 매우 유사할 경우 타깃 도메인으로 학습해도 소스 도메인의 가중치나 편향도 유사

- 그러므로 특징 추출 계층은 동결해 학습하지 않고 기존에 학습된 모델의 가중치를 사용
    - 예측 모델마다 요구하는 출력 노드의 수가 다르므로 모델의 분류기만 재구성해 학습

미세 조정: 특징 추출 계층을 일부만 동결하거나 동결하지 않고 타깃 도메인에 대한 학습을 진행

- 두 도메인의 유사성이 낮아도 특징 추출을 위한 모델의 구조는 유효하기 때문에 모든 계층을 동결하지 않고 전체 데이터세트로 학습, 데이터가 많은 경우
- 데이터가 적은 경우, 일부 계층만 동결해 학습
- 두 도메인이 유사하지만, 충분한 데이터세트를 확보하지 못한 경우 유사성이 높은 경우 특징 추출 방법으로 모델을 학습할 수 있지만 데이터셋이 불충분하므로 일부 상위 계층을 학습하는 방법으로 모델 구축

특징 추출 데이터가 적고 유사도가 높음, 합성곱 계층은 학습하지 않음, 분류기에 학습

데이터가 많고 유사도가 낮은 경우 데이터가 가진 특징이 다르기 때문에 분류기를 포함한 모델 매개변수를 다시 학습

데이터도 적고 유사도도 낮다면

|  | 유사도 낮음 | 유사도 높음 |
| --- | --- | --- |
| 데이터 적음 | 초기 계층의 저수준 특징 추출기능을 동결 후 나머지 계층과 분류기를 학습 | 특징추출 합성곱 계층은 학습히자 않고 분류기만 학습 |
| 데이터 많음 | 분류기를 포함한 모델 매개변수 다시 학습 | 가장 큰 영향을 미치는 상위 계층과 분류기를 학습한다. 완전 동일은 아니기 때문에 |
