## **성능 저하 요인**

**과대적합**: 모델의 구조가 너무 복잡 → 훈련 데이터에 의존(**일반화 능력 저하**)

**과소적합**: 모델의 구조가 너무 단순 → **데이터 특징 학습 부족**

훈련데이터와 새로운 데이터에서 높은 성능을 띄려면?

→ 낮은 편향, 낮은 분산

분산이 높으면? 추정치 변동 폭이 커지고, 노이즈까지 학습해서 과대적합 발생

편향이 높으면? 추정치가 일정한 값을 가짐 → 과소 적합 발생

**편향 & 분산은 서로 반비례 관계**

모델이 복잡 = 분산 up, 편향 down

단순한 모델 = 분산 down, 편향 up

- 과대 적합을 막기 위해 “조기 중단”은 성능을 지속적으로 평가해서 저하되기 전에 학습을 중단시키는 방법인데, 학습할 데이터가 남아 있더라도 일부만 사용해서 학습을 시키는 건가요? 그럼 남은 데이터는 어떤 식으로 처리가 되는 지 궁금합니다.

## 배치 정규화

**내부 공변량 변화**를 줄여서 과대적합을 방지하는 기술

**<내부 공변량 변화>**

일반적으로 인공신경망은 입력값을 배치 단위로 나누고, 상위 계층의 매개변수가 갱신되면 현재 계층의 **데이터 분포도 변경**된다.

**계층 마다 계속 변화**하는 입력값을 학습해야하므로 성능과 안정성이 낮아져 속도도 느려짐.

→ 1. 은닉층에 전달되는 입력값이 균일하지 않음 - 학습 불안정, 속도 느

1. 초기 가중치 값에 민감해짐 → 더 많은 학습 데이터 요구

⇒ 각 계층에 배치 정규화 적용 (미니 배치 정규화하는 방식으로 동작)

### 정규화의 종류

1. 배치 정규화
미니 배치에서 계산된 평균 및 분산 기반, 계층 입력 정규화
주로 컴퓨터 비전에서 사용
순방향 신경망 : 합성곱 신경망(CNN), 다층 퍼셉트론(MLP)
클래스 : BarchNorm1d, 2d, 3d(2D/3D, 4D, 5D 입력데이터)
2. 계층 정규화
이미지 데이터 전체가 아니라, 각각의 이미지 데이터에 **채널별**로 정규화 수행
샘플이 서로 다른 길이를 가지더라도 정규화 수행 가능, 주로 자연어 처리에서 사용
순환 신경망(RNN), 트랜스포머 기반 모델
클래스 : LayerNorm(정규화하려는 차원 크기로 수행)
3. 인스턴스 정규화
**채널과 샘플 기준**으로 정규화 수행, 각 샘플에 대해 개별적으로 수행 되므로 입력이 다른 분포를 갖는 작업에 적합함.
생성적 적대 신경망(GAN), 스타일 변환(Style Transfer) 모델
클래스 : InstanceNorm1d, 2d, 3d(2D/3D, 4D, 5D 입력데이터)
4. 그룹 정규화
**채널을 N개의 그룹**으로 나누고 그룹 내 정규화 수행
N=1이면, 인스턴스 정규화와 동일
N(그룹의 수)=채널의 개수면, 계층 정규화와 동일
배치 크기가 작거나, 채널의 수가 매우 많은 경우 사용, 배치 정규화의 대안으로 사용됨
클래스 : GroupNorm(그룹과 채널을 나눠서 진행)

특징 맵(Feature Map)을 3D로 시각화 → 정규화가 수행되는 방식

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/20f943d6-e33f-41ff-b86c-ab4a2f031bcd/Untitled.png)

블록 : 이미지 데이터 or 텍스트 데이터

차원(Dimensions) : 이미지 데이터의 크기(너비, 높이) or 어휘사전의 크기

채널(Channels) : 이미지 데이터의 채널 or 시간 간격(Timestep)

### 배치 정규화 풀이

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/2ec095c2-f0e5-417f-ad01-d2ca379bacc7/Untitled.png)

ex) 텐서에서 배치 정규화 적용해보기

X에 대해서 하나씩 빼서 먼저 보고, x에 대해서 하나씩 적용, 새로 적용된 Y값 만들기, Y로 평균과 분산 계산해보면 0, 1로 정규화된걸 알 수 있음

→ 실습에서는 배치 정규화 클래스를 이용해 간단하게 적용가능(MarchNorm1d)

**1차원 배치 정규화 클래스**

특징 개수(입력데이터의 채널 수)를 입력 받아 수행

eps: 분모가 0이 되는 현상 방지(작은 상수)

**2차원 배치 정규화 클래스**

채널이 포함된 입력 데이터를 정규화하고 싶을 때

## 가중치 초기화

모델의 초기 가중치  값을 설정

모델 매개 변수에 적절한 초기값을 설정하면, 기울기 폭주나 소실 문제를 완화할 수 있음 & 수렴 속도 향상

### 상수 초기화

가장 쉬운 방법 = **숫자로 초기화**하기

초기 가중치 값을 모두 같은 값으로 초기화함

일반적으로는 사용되지 않음 

→ 왜? 배열 구조의 가중치에서 문제 발생

(**대칭 파괴 현상**- 모든 노드가 동일한 출력을 생성, 모델 학습 불가능)

→ 언제 쓰임? 스칼라값을 입력으로 받는 매우 작은 모델 or 퍼셉트론 등

### 무작위 초기화

초기 가중치 값을 **무작위 값 or 특정 분포 형태**(ex. 정규분포)로 초기화

노드의 가중치와 편향을 **무작위로 할당**해 학습 → 대칭 파괴 문제 방지

간단하고 많이 사용되는 초기화 방법임

→ 언제 쓰임? 계층이 적거나 하나만 있는 경우 보편적으로 적용

→ But! 계층 많아지고 깊어질수록 활성화 값이 양 끝단에 치우치게 되어, **기울기 소실 현상** 발생

### 제이비어 & 글로럿 초기화

**균등 분포나 정규 분포**를 사용해 가중치 초기화

2010년 제안. 각 노드의 “출력 분산=입력 분산”이 되도록 초기화

- 은닉층의 노드 수에 따라 **다른 표준 편차**를 할당
- 현재 계층의 **입력 및 출력 노드 수**를 기반으로 계산
- 활성화 함수(시그모이드나 하이퍼볼릭 탄젠트)의 네트워크에서 효과적임

### 카이밍&허 초기화

제이비어 초기화 방법과 마찬가지로 균등 분포나 정규 분포를 사용해 가중치를 초기화함

2015년 제안. 순방향 신경망 네트워크에서 효과적임

→ 차이점은? 현재 계층의 **입력 뉴런수**를 기반으로만 가중치를 초기화함

→ 보완은? 각 노드의 출력 분산과 입력분산을 동일하게 만들어 ReLU함수의 **죽은 뉴런 문제**를 최소화할 수 있음

- 죽은 뉴런 문제가 뭘까?

ReLU를 활성화 함수로 사용하는 네트워크에서 효과적임

### 직교 초기화

특이값 분해(SVD)를 활용, 자신을 제외한 나머지(모든 열, 행 벡터)와 직교+단위 벡터인 행렬을 만드는 방법

LSTM(장단기 메모리) 및 GRU(게이트 순환 유닛)같은 RNN에서 주로 사용됨

직교 행렬 : 고윳값의 절댓값은 1이므로 행렬곱을 여러번 수행해도 기울기 폭주나 소실이 발생하지 않음 → 최대한 1에 가까워지도록 해서 RNN에서 문제를 방지하는 데 사용됨

모델이 특정 초기화 값에 지나치게 민감해지므로, 순방향 신경망에서는 사용하지 않음

(192p - 가중치 초기화 실습)

## 정칙화

과대 적합 문제 방지, 모델이 일반화할 수 있도록 손실함수를 규제하는 방식

**암기(Memorization)**

- 훈련데이터의 노이즈를 학습했을때 발생
- 일반적인 패턴을 학습한 게 아닌, 학습데이터의 노이즈나 특정 패턴만을 학습한거임

**일반화(Generalization)**

- 새로운 데이터에서도 정확한 예측을 할 수 있음
- 일반적인 패턴을 학습해서, 학습 데이터와 약간 달라도 정확한 예측 가능

⇒ 정칙화는 모델이 특정 피처나 특정 패턴에 너무 많은 비중을 할당하지 않도록 손실함수에 규제를 가함 → 일반화 성능 향상

학습 데이터들의 차이점에 대해 덜 민감해짐, 모델의 분산값 낮아짐, 데이터를 학습하는 데에 있어서 **의존하는 특징의 수**를 줄임 → 모델의 추론 능력 개선

→ 언제 사용해? 모델이 비교적 복잡하고 사용되는 데이터의 수가 적을 때

→ 필요없는 경우는? 모델이 단순한 경우 매개변수의 수가 적어서 정칙화 필요없음. 데이터 수가 많거나, 잘 정제되어 있어서 노이즈가 거의 없는 경우도 필요없음

노이즈에 강건하게 만들어 일반화를 하는 데 사용 = 이미 정규화되어 있는 경우에는 사용 안해도 됨

### L1 정칙화

라쏘 정칙화(Lasso Regularization)라고도 부름

- L1 노름(L1 Norm)방식(벡터or행렬값의 **절댓값 합계** 계산)을 사용해 규제

해당 방식 차용, **손실 함수 + 가중치 절댓값의 합** ⇒ 과대 적합 방지

모델 학습 시, 값이 크지 않은 가중치는 0으로 수렴 → 예측에 필요한 특징의 수 감소(특징 선택 효과)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/bdb5fb87-7ec9-422d-81ce-65f35ad9a08d/Untitled.png)

규제 강도 : 0에 가까울수록 모델이 더 많은 특징을 사용하는 하이퍼파라미터(과대 적합 주의)

규제 강도가 너무 높아지면, 가중치가 0에 수렴되므로 과소적합 문제에 노출

모델의 가중치가 정확히 0인 경우도 있음 = 불필요한 특징을 처리하지 않음(모델의 성능은 증가), 예측에 사용되는 특징의 수가 줄어듦(정보의 손실)

계산복잡도 높음

- 모델의 가중치를 모두 계산(미분 못해서 역전파를 계산하는데 더 많은 리소스를 소모)해 갱신해야함
- 하이퍼파라미터가 적절하지 않으면, 가중치 값이 너무 작아져서 모델을 해석하기 더 어려움(여러번 반복해서 최적의 람다를 구해야 함)

→ 언제 사용? 주로 선형 모델(라쏘 회귀라 부름)

### L2 정칙화

릿지 정칙화(Ridge Regularization)라고도 불림

- L2 Norm 방식(벡터or 행렬 **값의 크기**를 계산)을 사용해 규제

해당 방식 차용, **손실 함수 + 가중치 제곱의 합** ⇒ 과대 적합 방지

