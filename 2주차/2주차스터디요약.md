## ❓질문1
과대 적합을 막기 위해 “조기 중단”은 성능을 지속적으로 평가해서 저하되기 전에 학습을 중단시키는 방법인데, 학습할 데이터가 남아 있더라도 일부만 사용해서 학습을 시키는 건가요? 그럼 남은 데이터는 어떤 식으로 처리가 되는 지 궁금합니다!
### 답변 정리
학습 데이터를 일부만 사용하는게 아니라, 학습을 하는 메커니즘 면에서 다음 메커니즘으로 넘어가는 과정을 중단하는 개념이다. 따라서, 일부만 사용하는게 아닌, 학습 데이터로 들어가 있는 모든 데이터를 활용해 여러번 학습을 진행하던 중에 더이상 성능에 영향을 끼치지 않는다고 판단되면 이런 학습을 중단시키는 것이다!

## ❓질문2
제로-샷 전이 학습은 사전 학습된 모델로 새로운 도메인에 대해서도 일반화된 성능을 가질 수 있는 전이 학습 기법인데, 어떻게 이전에 학습된 내용과 학습되지 않은 것을 비교하고 결론을 내리는지 원리가 궁금합니다!
### 답변 정리
- 텍스트 분류 문제에서는 "개는 귀 두 개,눈 두 개, 꼬리가 있다"라는 문장을 학습한 모델이 있을 때 "호랑이는 귀 두 개, 눈 두 개, 꼬리가 있다"라는 새로운 도메인인 호랑이에 대한 문장도 문장의 문맥 정보의 유사함을 통해 호랑이도 개처럼 동물이구나! 판단할 수 있다.
- 이미지 분류 문제에서는 독수리, 참새, 오리 등의 이미지 데이터를 학습한 모델이 있을 때 부엉이 이미지에 대해서도 부엉이의 날개나 눈 등의 이미지 속 정보를 통해 부엉이도 새구나! 판단할 수 있다.

## ❓질문3
🙄 학습 데이터를 늘리는 것이 상황에 상관없이 항상 학습의 성능을 향상시키는가?

### 답변 정리
- 가장 best는 데이터가 많은 것.
- but, data 양을 늘리는 것보다는 학습량을 늘리는 게 더 간단함.
- 따라서, 과대적합일 경우에는 데이터를 늘리거나 drop out 기법 사용
- 과소적합일 경우에는 먼저 학습량을 늘려보고 데이터를 추가
