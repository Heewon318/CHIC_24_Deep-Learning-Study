## ❓질문1

과대 적합을 막기 위해 “조기 중단”은 성능을 지속적으로 평가해서 저하되기 전에 학습을 중단시키는 방법인데, 학습할 데이터가 남아 있더라도 일부만 사용해서 학습을 시키는 건가요? 그럼 남은 데이터는 어떤 식으로 처리가 되는 지 궁금합니다!

### 답변 정리

학습 데이터를 일부만 사용하는게 아니라, 학습을 하는 메커니즘 면에서 다음 메커니즘으로 넘어가는 과정을 중단하는 개념이다. 따라서, 일부만 사용하는게 아닌, 학습 데이터로 들어가 있는 모든 데이터를 활용해 여러번 학습을 진행하던 중에 더이상 성능에 영향을 끼치지 않는다고 판단되면 이런 학습을 중단시키는 것이다!

## ❓질문2

제로-샷 전이 학습은 사전 학습된 모델로 새로운 도메인에 대해서도 일반화된 성능을 가질 수 있는 전이 학습 기법인데, 어떻게 이전에 학습된 내용과 학습되지 않은 것을 비교하고 결론을 내리는지 원리가 궁금합니다!

### 답변 정리

- 텍스트 분류 문제에서는 "개는 귀 두 개,눈 두 개, 꼬리가 있다"라는 문장을 학습한 모델이 있을 때 "호랑이는 귀 두 개, 눈 두 개, 꼬리가 있다"라는 새로운 도메인인 호랑이에 대한 문장도 문장의 문맥 정보의 유사함을 통해 호랑이도 개처럼 동물이구나! 판단할 수 있다.
- 이미지 분류 문제에서는 독수리, 참새, 오리 등의 이미지 데이터를 학습한 모델이 있을 때 부엉이 이미지에 대해서도 부엉이의 날개나 눈 등의 이미지 속 정보를 통해 부엉이도 새구나! 판단할 수 있다.

## ❓질문3

렐루함수의 죽은 뉴런 문제란 무엇이고, 렐루함수의 어떤 특성때문에 일어나는 문제일까요?

### 답변 정리

ReLU function는 입력값이 0보다 작으면 0이고, 0보다 크면 입력값 그대로 내보냅니다.

그렇다면 죽은 뉴런 문제란 무엇일까요?
렐루함수의 입력값이 음수인 경우 기울기가 0이 되어 가중치 업데이트가 안될 수 있습니다. 이로 인해 가중치가 업데이트 되는 과정에서 가중치 합이 음수가 되면 0만 반환되어 아무것도 변하지 않는 현상 발생하는데, 죽은 뉴런(Dead Neuron) 또는 죽어가는 렐루(Dying ReLU)라고 합니다.

## ❓질문4

컷믹스는 어떻게 사용할까요? 개와 고양이 분류 문제라고 하면 개, 고양이 둘 중 하나의 값을 내는건가요? 어떻게 사용되는지 궁금합니다!

### 답변 정리

비교적 최근에 제안된 컷아웃과 컷믹스에 대해서 보면, 컷아웃은 이미지의 일부를 도려내는 기법이고, 컷믹스는 그 도려낸 자리를 다른 범주에 속하는 이미지로 대체하여 면적 비율만큼 라벨값을 주는 기법입니다. 이 두 기법 모두 일반적인 이미지 분류 문제에서 다른 Data 증강 기법들보다 성능이 뛰어나다는 것이 논문을 통해 밝혀졌습니다.

기존의 데이터는 레이블이 0/1 로 나타냄으로써 Deterministic 했지만, Mix-up 을 통해 0과 1 사이의 label probability 에 대해서도 학습이 가능하기 때문에, deicision boundary 가 smooth 해지고, confidence 가 고르게 분포하게 됩니다.

Mixup은 두 데이터 샘플로부터 선형 보간법을 통해 새로운 샘플을 생성해내는 데이터 증강 기법입니다. 이는 매번 새로운 샘플을 학습하는 것과 같은 효과를 불러오며, 결과적으로 ERM 학습 모델과 비교해 보다 부드러운 결정 경계선을 생성할 수 있게 합니다.

## ❓질문5

🙄 학습 데이터를 늘리는 것이 상황에 상관없이 항상 학습의 성능을 향상시키는가?

### 답변 정리

- 가장 best는 데이터가 많은 것.
- but, data 양을 늘리는 것보다는 학습량을 늘리는 게 더 간단함.
- 따라서, 과대적합일 경우에는 데이터를 늘리거나 drop out 기법 사용
- 과소적합일 경우에는 먼저 학습량을 늘려보고 데이터를 추가
