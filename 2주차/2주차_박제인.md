# 파이토치 심화
### 과대적합과 과소적합
- 과대적합 : 모델이 훈련 데이터는 우수하게 예측하지만 새로운 데이터는 제대로 예측하지 못함. -> 오차가 큼(일반화 능력 저하). 모델 구조가 너무 복잡하기 때문.
- 과소적합 : 훈련 데이터도 예측 못하고 새로운 데이터도 제대로 예측 못함.  -> 모델 구조가 너무 단순하기 때문.
- 과대적합과 과소적합은 모델의 성능을 저하시킴. => 훈련데이터와 새로운 데이터 둘 다에 대해 우수한 성능 보이려면 낮은 편향과 낮은 분산 가져야 함!!
- 문제 해결 방법 : 모델 변경 또는 모델 구조 개선, 학습 데이터 수 늘리기, 피처 엔지니어링, 배치 정규화, 가중치 초기화, 정칙화  등
  
### 편향-분산 트레이드오프
![title](https://blog.kakaocdn.net/dn/bT2iuZ/btqyg76Nl3T/kPpkJ8zKTDzefTF7dTX2dK/img.png)  
  - 훈련데이터와 새로운 데이터 둘 다에 대해 우수한 성능 보이려면 낮은 편향과 낮은 분산 가져야 하는데 편향과 분산은 반비례한다.
  - 편향이 높으면 과소적합, 분산이 높으면 과대적합 문제가 발생함.
  - 모델의 성능을 높이려면 편향과 분산의 균형을 맞추어야 한다.

### 과대적합과 과소적합 문제 해결 방법 
### 1. 배치 정규화(Batch Normalization)
![title](https://i.ytimg.com/vi/zwGcVrdyCBg/hq720.jpg?sqp=-oaymwE7CK4FEIIDSFryq4qpAy0IARUAAAAAGAElAADIQj0AgKJD8AEB-AG-B4AC0AWKAgwIABABGGUgYyhYMA8=&rs=AOn4CLBFj7HnhVu_uHEh03zZtmEKndZl-Q)   
- 배치 정규화는 신경망이 학습하는 과정을 더 빠르고 안정적으로 만들기 위해 각 미니배치의 데이터에 대해 활성화 값을 정규화하여 데이터의 분포를 조정하는 방법이다.
 *  미니 배치(mini-batch)는 큰 데이터를 작은 덩어리로 나눈 것을 말한다. 예를 들어, 1000장의 이미지가 있으면, 이 이미지를 한 번에 처리하지 않고 100장씩 10번에 나눠서 처리하는 것이다. 이 작은 덩어리 하나를 미니 배치라고 한다.
- 쉽게 말해, 신경망이 더 잘 배우도록 데이터의 값을 "평균 0, 분산 1"로 맞춰주는 과정이다.
- 배치 정규화 과정 : 1. 미니 배치 선택: 데이터 전체가 아닌 작은 덩어리(미니 배치)를 선택한다. 2. 평균과 분산 계산: 미니 배치의 평균과 분산을 계산한다. 평균은 데이터의 중심값을, 분산은 데이터가 얼마나 퍼져 있는지를 나타냄. 3. 정규화: 각 데이터에서 평균을 빼고 분산으로 나눠서 데이터를 조정한다. 이렇게 하면 데이터의 중심이 0이 되고, 퍼짐 정도가 1이 된다. 4. 학습 가능한 매개변수 적용: 이렇게 조정된 데이터에 특별한 값(γ, β)을 더해준다. 이 값들은 신경망이 학습하면서 최적의 값을 찾아간다.
- 배치 정규화는 모델의 계층마다 평균과 분산을 조정해 내부 공변량 변화를 줄여 과대적합을 방지한다.
- 내부 공변량 변화(Internal Coveriate Shift)는? ✍️Internal Covariate Shift는 네트워크의 각 Layer나 Activation마다 출력값의 데이터 분포가 Layer마다 다르게 나타나는 현상을 말한다.


### 2. 정규화 종류 비교
![title](https://velog.velcdn.com/images/euisuk-chung/post/43cb32ac-32b0-4c8d-8196-552850ab11d1/image.png)   

### 3. 가중치 초기화
- 모델의 초기 가중치 값을 설정하는 것을 의미
- 최적화할 때 초기 가중치에 따라 모델 최적화에 어려움을 겪게 되는데 모델 매개변수에 적절한 초깃값을 설정하면 기울기 폭주나 기울기 소실 문제를 완화할 수 있음.
#### 가중치 초기화 방법
- 3-1. 가충치 초기화의 매우 간단한 방법은 상숫값으로 초기화하는 것인데 배열 구조의 가중치에서 대칭 파괴 문제가 발생한다. => 초기 가중치 값을 모두 0이나 0.1과 같은 작은 양의 상숫값으로 동일하게 할당한다. ex: 단위행렬 등<br>   
- 3-2. 무작위 값으로 초기화 : 초기 가중치 값을 무작위 값이나 특정 분포 형태(ex:균등 분포, 정규 분포 등)로 초기화하는 것. => 계층이 많아지고 깊어질수록 기울기 소실 현상 발생.<br>   
- 3-3. 제이비어 & 글로럿 초기화 : 제이비어 초기화는 글로럿 초기화라고도 하며 균등 분포나 정규 분포를 사용해 가중치를 초기화하는 방법임. 각 노드의 출력 분산이 입력 분산과 동일하도록 가중치를 초기화함. => 제이비어 초기화는 입력 데이터의 분산이 출력 데이터에서 유지되도록 가중치를 초기화하므로 시그모이드나 하이퍼볼릭 탄젠트를 활성화 함수로 사용하는 네트워크에서 효과적임.<br>   
- 3-4. 카이밍 & 허 초기화 : 카이밍 초기화는 허 초기화라고도 하며 제이비어 초기화의 문제점을 보완한 방법이다. 제이비어 초기화 방법과 마찬가지로 균등 분포나 정규 분포를 사용해 가중치를 초기화한다. 하지만 현재 계층의 입력 뉴런 수를 기반으로만 가중치를 초기화한다는 차이점이다. => 순방향 신경망 네트워크에서 효과적. ReLU함수를 활성화함수로 사용하는 네트워크에서 효과적임.
- 3-5. 직교 초기화 : 특잇값 분해(SVD)를 활용해 자기자신을 제외한 나머지 모든 열,행 벡터들과 직교이면서 동시에 단위 벡터인 행렬을 만드는 방법이며 장단기 메모리(LSTM), 게이트 순환 유닛(GRU)과 같은 순환 신경망(RNN)에서 주로 사용된다. => 직교 행렬의 고윳값 절댓값이 1이기 때문에 기울기 폭주나 기울기 소실 문제 발생 X
- 3-6. 정칙화 : 과적합 문제 방지, 암기가 아니라 일반화할 수 있도록 손실합수에 '규제'를 가하는 방식이다. 암기는 훈련 데이터의 노이즈를 학습하는 것이다. 정칙화는 모델의 복잡성을 줄여서 과적합을 방지하는 것. 이를 위해 모델의 학습 과정에 제약을 추가. 가장 일반적인 정칙화 방법 두 가지는 L1 정칙화와 L2 정칙화이다.
- - L1 정칙화 (라쏘, Lasso) : L1 정칙화는 모델의 모든 가중치(파라미터) 합에 페널티를 주는 방법. 쉽게 말해, 가중치들의 절댓값 합을 최소화. 이렇게 하면, 일부 가중치가 0이 되어서 모델이 더 단순해짐. 예시: 어떤 모델이 여러 가지 특성을 가지고 데이터를 학습하고 있다고 가정. L1 정칙화를 사용하면 덜 중요한 특성들의 가중치가 0이 되어서, 그 특성들은 모델에서 제외됨. 결과적으로, 모델이 더 간단해지고 과적합이 줄어든다.
- - L2 정칙화 (릿지, Ridge) : L2 정칙화는 모델의 모든 가중치의 제곱합에 페널티를 주는 방법. 쉽게 말해, 가중치들의 제곱합을 최소화. 이 방법은 가중치들을 모두 작게 만들지만, L1 정칙화처럼 0으로 만들지는 않는다. 예시: 모델이 여러 특성을 학습하고 있을 때, L2 정칙화를 사용하면 모든 가중치가 작아짐. 특성들이 모델에 여전히 포함되지만, 덜 중요한 특성의 영향이 줄어듦. 이로 인해 모델이 더 단순해지고 과적합이 줄어든다.
정칙화를 비유로 이해하면 일종의 "벌금 제도"로 생각해볼 수 있다. 모델이 너무 복잡해지려고 하면(즉, 가중치가 너무 크거나 많아지면), 벌금을 매겨서 모델이 단순하게 유지되도록 하는 것이다.
- 3-7
