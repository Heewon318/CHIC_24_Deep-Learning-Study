# 파이토치 심화(160p ~ 229p)

4장 전체 내용을 읽고 정리한 내용입니다.

## 과대적합과 과소적합

머신러닝 모델에서 자주 발생하는 일반적인 문제.

### 과대적합

> 새로운 데이터에 대해서는 성능이 저하되는 경우

훈련 데이터에서 우수하게 예측하지만, 새로운 데이터에서는 제대로 예측하지 못해 오차가 크게 발생.

### 과소적합

훈련 데이터에서도 성능이 좋지 않고, 새로운 데이터에 대해서도 성능이 좋지 않다.

### 과대적합과 과소적합의 공통점

- 성능 저하
- 모델 선택 실패 : **과대적합**(모델 구조가 복잡 -> 훈련데이터에만 의존), **과소적합**(모델 구조가 단순 -> 데이터 특징을 제대로 학습 못함)
- 편향-분산 트레이드 오프 : 좋은 모델 -> 낮은 편향과 낮은 분산

### 문제 해결

> 과대적합은 모델의 일반화(Generalization) 능력을 저하해 문제가 발생.
> 과소적합은 모델이 데이터의 특징을 제대로 학습할 수 없을 때 발생.

- 데이터 수집 : 모델이 훈련 데이터에서 노이즈를 학습하지 않으면서 일반적인 규칙을 찾을 수 있게 학습 데이터의 수를 늘린다.
- 피처 엔지니어링 : 신규 데이터 수집이 어려운 경우, 기존 훈련 데이터에서 변수나 특징을 추출하거나 피처를 더 작은 차원으로 축소.
- 모델 변경
- 조기 중단(Early Stopping) : 과대적합이 발생하기 전에 모델 학습을 중단.
- 배치 정규화(Batch Normalizaiton) : 적용 시, 모델 성능 및 모델 안정성 향상
- 가중치 초기화(Weight Initialization) : 학습 시 기울기가 매우 작아지거나 커지는 문제가 발생할 수 있는데, 이는 학습을 어렵게 만들거나 불가능하게 만듦. 가중치 초기화를 하면 과대적합 방지 가능.
- 정칙화(Regularization) : 적용하여 목적 함수에 페널티를 부여. 학습 조기 중단, L1 정칙화, L2 정칙화, 드롭아웃, 가중치 감쇠 등

## 배치 정규화(Batch Normalization)

> 내부 공변량 변화(Internal Covariate Shift)를 줄여 과대적합을 방지하는 기술.

배치 단위로 나눠 학습하는 경우 상위계층의 매개변수가 갱신될 때마다 현제 계층에 전달되는 데이터의 분포도 변경된다.

각 계층은 배치 단위의 데이터로 인해 계속 변화되는 입력 분포를 학습해야 하기 때문에 인공 신경망의 성능과 안정성이 낮아서 학습 속도가 느려짐.

내부 공변량 변화가 발생하는 경우 -> 은닉층에서 다음 은닉층으로 전달될 때 입력값이 균일해지지 않아 가중치가 제대로 갱신되지 않을 수 있다.

- 학습 불안정, 속도 느림 -> 일정한 값으로 수렴하기 어려워짐.
- 초기 가중치 값에 민감 -> 일반화 어려움

이러한 문제 해결을 위해 각 계층에 배치 정규화를 적용.

배치 정규화를 적용하면 각 계층에 대한 **입력이 일반화**, **독립적으로 정규화가 수행**되므로 더 빠르게 값 수렴, 입력이 정규화되므로 **초기 가중치에 대한 영향 감소** 가능.

### 정규화 종류(164p)

**1. 배치 정규화**

- 미니 배치에서 계산된 평균 및 분산을 기반으로 계층의 입력을 정규화
- 컴퓨터비전 관련 모델 중 합성곱 신경망(CNN)이나 다층 퍼셉트론(MLP)과 같은 **순방향 신경망(Feedforward Neural Network)** 에서 주로 사용됨.

**2. 계층 정규화**

- 이미지 데이터 전체 대상이 아닌 **각각의 이미지 데이터에 채널별로 정규화** 수행.
- 채널 축으로 계산되기 때문에 미니 배치 샘플 간의 의존관계가 없음. 그러므로 샘플이 서로 다른 길이를 가지더라도 정규화를 수행할 수 있음.
- 신경망 모델 중 자연어 처리에서 주로 사용. 순환 신경망(RNN)이나 트랜스포머 기반 모델에서 주로 사용.

**3. 인스턴스 정규화**

- 채널과 샘플을 기준으로 정규화 수행
- 입력이 다른 분포를 갖는 작업에 적합
- 생성적 적대 신경망(GAN)이나 이미지 스타일을 변환하는 스타일 변환 모델에서 주로 사용됨.

**4. 그룹 정규화**

- 채널을 N개의 그룹으로 나누고 각 그룹 내에서 정규화 수행.
- 그룹을 하나로 설정하면 인스턴스 정규화와 동일. 그룹의 개수를 채널의 개수와 동일하게 설정하면 계층 정규화와 동일한 기능을 함.
- 배치 크기가 작거나 채널 수가 매우 많은 경우에 주로 사용됨.

| 정규화 종류     | 클래스                                       | 의미                                      |
| --------------- | -------------------------------------------- | ----------------------------------------- |
| 배치 정규화     | torch.nn.BatchNorm1d(num_features)           | 2D, 3D 입력 데이터에 배치 정규화 수행     |
| 배치 정규화     | torch.nn.BatchNorm2d(num_features)           | 4D 입력 데이터에 배치 정규화 수행         |
| 배치 정규화     | torch.nn.BatchNorm3d(num_features)           | 5D 입력 데이터에 배치 정규화 수행         |
| 계층 정규화     | torch.nn.LayerNorm(normalized_shape)         | 정규화하려는 차원크기로 계층 정규화 수행  |
| 인스턴스 정규화 | torch.nn.InstanceNorm1d(num_features)        | 2D, 3D 입력 데이터에 인스턴스 정규화 수행 |
| 인스턴스 정규화 | torch.nn.InstanceNorm2d(num_features)        | 4D 입력 데이터에 인스턴스 정규화 수행     |
| 인스턴스 정규화 | torch.nn.InstanceNorm3d(num_features)        | 5D 입력 데이터에 인스턴스 정규화 수행     |
| 그룹정규화      | torch.nn.GroupNorm(num_groups, num_channels) | 그룹과 채널을 나눠 정규화 수행            |

## 가중치 초기화(170p)

**1. 상수 초기화**

매우 작은 양의 상숫값으로 모든 가중치를 동일하게 할당.

> ex) 0, 1, 특정값(Constant), 단위행렬(Unit Matrix), **디랙 델타 함수(Dirac Delta Functhin)**

- 구현이 간단하고 계산 비용이 거의 들지 않지만, 일반적으로 사용되지 않는 초기화 방법. **대칭 파괴(Breaking Symmetry)** 현상 발생.

**2. 무작위 초기화**

노드의 가중치와 편향을 무작위로 할당해 네트워크가 학습할 수 있게 하여, 대칭 파괴 문제를 방지할 수 있음.

간단하고 많이 사용되는 방법이지만, 계층이 많아지고 깊어질수록 활성화 값이 양 끝단에 치우치게 되어 기울기 소실 현상 발생.

**3. 제이비어& 글로럿 초기화**

> 제이비어 초기화(Xavier Initializaiton)는 글로럿 초기화(Glorot Initialization)라고도 함. 균등 분포나 정규 분포를 사용해 가중치를 초기화하는 방법.
> 제이비어 글로럿과 요슈아 벤지오가 2010년에 제안한 방법으로 각 노드의 출력 분산이 입력 분산과 동일하도록 가중치를 초기화 하는 방법.

### 제이비어 초기화와 확률 분포 초기화 방법의 주요한 차이점

동일한 표준편차를 사용하지 않고 은닉층의 노드 수에 따라 다른 표준편차를 할당함.

**입력 데이터의 분산의 출력 데이터에서 유지되도록 가중치를 초기화**하므로 시그모이드나 하이퍼볼릭 탄젠트를 활성화 함수로 사용하는 네트워크에서 효과적임.

**4. 카이밍 & 허 초기화**

- 순방향 신경망 네트워크에서 가중치를 초기화할 때 효과적임.
- 현재 계층의 입력 뉴런 수를 기반으로만 가중치를 초기화함.
  제이비어 초기화에서 발생한 문제점 보완 -> 각 노드의 출력 분산이 입력 분산과 동일하게 만들어 **ReLU 함수의 죽은 뉴런 문제**를 최소화 할 수 있음.

**5. 직교 초기화**

> 특잇값 분해(SVD)를 활용해 자기 자신을 제외한 나머지 모든 열, 행 벡터들과 직교이면서 동시에 단위벡터인 행렬을 만드는 방법.

- 장단기 메모리(LSTM) 및 게이트 순환 유닛(Gated Recurrent Units, GRU)과 같은 순환 신경망(RNN)에서 주로 사용됨.
- 직교 행렬의 고윳값의 절댓값은 1이기 때문에 행렬곱을 여러 번 수행하더라도 기울기 폭주나 기울기 소실이 발생하지 않음.
- 가중치 행렬의 고윳값이 1에 가까워지도록 해 **RNN에서 기울기가 사라지는 문제를 방지**하는데 사용됨.

## 정칙화(179p)

> 모델 학습 시 발생하는 과대적합 문제를 방지하기 위해 사용되는 기술로 모델이 암기가 아니라 일반화할 수 있도록 **손실함수에 규제(Penalty)를 가하는 방식**임.

**암기**란 모델이 데이터의 특성이나 패턴을 학습하는 것이 아니라 **훈련 데이터의 노이즈를 학습했을 때 발생**함.

즉, 모델이 데이터의 일반적인 패턴을 학습한 것이 아니라 학**습 데이터의 노이즈나 특정 패턴을 학습**한 것.

정칙화는 노이즈에 강건하고 일반화된 모델을 구축하기 위해 사용하는 방법. 손실함수에 규제를 가해 모델의 일반화 성능(Generalization Performance)을 향상시킴.

정칙화를 적용하면 학습 데이터들이 갖고 있는 **작은 차이점에 대해 덜 민감**해져 **모델의 분산 값이 낮아짐**.

**모델이 데이터를 학습할 때 의존하는 특징의 수를 줄임으로써 모델의 추론 능력을 개선함.**

- 모델이 비교적 복잡하고 학습에 사용되는 데이터의 수가 적을 때 활용.
- 모델이 단순하면 모델 매개변수의 수가 적어 정칙화가 불필요.
- 데이터 수가 많거나 데이터가 잘 정제되어 있어 노이즈가 거의 없는 경우에도 사용하지 않음.

### L1 정칙화

> 라쏘 정칙화(Lasso Regularization)라고도 함. L1 노름 방식을 사용해 규제하는 방식.

- 손실 함수에 가중치 절댓값의 합을 추가해 과대적합을 방지함.
- 손실 함수에 가중치 절댓값의 합으로 규제를 가하므로 모델은 가중치 절댓값의 합도 최소가 되는 방향으로 학습이 진행.
- 모델 학습 시 값이 크지 않은 가중치들은 0으로 수렴하게 되어 예측에 필요한 특징의 수가 줄어듦.
- 모델의 가중치를 정확히 0으로 만드는 경우가 있으므로 희소한 모델이 될 수 있음. 불필요한 특징을 처리하지 않으므로 모델의 성능이 올라갈 수 있찌만, 예측에 사용되는 특징의 수가 줄어들게 되므로 정보의 손실로 이어질 수 있음.
- **L1 정칙화를 적용하는 경우 입력 데이터에 더 민감해지며, 항상 최적의 규제를 가하지 않으므로 사용에 주의해야 함.**
- 주로 선형 모델에 적용.

### L2 정칙화

> 릿지 정칙화(Ridge Regularization)라고도 함. L2 노름 방식을 사용해 규제하는 방식.

- L2 노름은 벡터 또는 행렬의 값의 크기를 계산. 이러한 방식을 차용해 **손실함수에 가중치 제곱의 합을 추가해 과대적합을 방지하도록 규제.**
- **하나의 특징이 너무 중요한 요소가 되지 않도록** 규제를 가하는 것에 의미를 둠.
- L2 정규화는 비선형적 특성을 가지므로 가중치가 0에 가까워질수록 규젯값이 줄어듦.

과대적합을 효과적으로 방지하기 위해서는 조기 중지 또는 드롭아웃과 같은 기술을 함께 사용.

### 가중치 감쇠

> 모델이 더 작은 가중치를 갖도록 손실 함수에 규제를 가하는 방식. 일반적으로 **가중치 감쇠가 L2 정칙화와 동의어로 사용되지만, 가중치 감쇠는 손실 함수에 규제 항을 추가하는 기술 자체를 의미**함.

L2 정칙화를 간단하게 적용하는 방식으므로 L2 정칙화가 갖고 있는 장점과 단점을 그대로 포함함.

### 모멘텀

> 경사 하강법 알고리즘의 변형 중 하나. 이전에 이동했던 방향과 기울기의 크기를 고려하여 가중치 갱신.

- 이를 위해 지수 가중 이동평균을 사용하며, 이전 기울기 값의 일부를 현재 기울기 값에 추가해 가중치를 갱신함.
- 이전 기울기 값에 의해 설정된 방향으로 더 빠르게 이동하므로 일종의 관성(Momentum) 효과를 얻을 수 있음.

### 엘라스틱 넷

> L1 정칙화와 L2 정칙화를 결합해 사용하는 방식. L1 정칙화는 모델이 희박한 가중치를 갖게 규제하는 반면, L2 정칙화는 큰 가중치를 갖지 않게 규제함.

- 두 정칙화 방식을 결합함으로써 희소성과 작은 가중치의 균형을 맞춤.
- 두 정칙화 방식의 선형 조합으로 사용하며, 혼합 비율을 설정해 가중치를 규제.
- 엘라스틱 넷은 L1, L2 정칙화보다 트레이드오프 문제를 더 유연하게 대처할 수 있음.
- **특징의 수가** 샘플의 수보다 **더 많을 때 유의미한 결과를 가져옴.**

각 정칙화가 가진 장점을 최대한 활용할 수 있지만, 균형적인 규제를 가하기 위해 새로운 하이퍼파라미터인 **혼합비율**도 조정해야 하므로 더많은 튜닝이 필요.

또한 두 정칙화 모두 계산 복잡도 문제를 갖고있으므로 **더 많은 리소스를 소모**함.

### 드롭아웃

> 모델의 훈련 과정에서 일부 노드를 일정 비율로 제거하거나 0으로 설정해 과대적합을 방지하는 간단하고 효율적인 방법.

**과대적합**을 발생시키는 이유 중 하나 -> 모델 학습 시 발생하는 **노드 간 동조화(Co-adaptation) 현상.**

\*동조화 현상: 모델 학습 중 특정 노드의 가중치나 편향이 큰 값을 갖게 되면 다른 노드가 큰 값을 갖는 노드에 의존하는 것을 말함.

이러한 현상을 특정 노드에 의존성이 생겨 학습 속도가 느려지고 새로운 데이터를 예측하지 못해 성능을 저하시킬 수 있음. 그러므로 학습 과정에서 일부 노드를 제거해 노드 간 의존성을 억제해야 함.

드롭아웃을 적용할 때는 **충분한 데이터세트**와 비교적 **깊은 모델**에 적용.

- 일반적으로 배치 정규화와 동시에 사용하지 않음.
- 드롭아웃과 배치 정규화는 서로의 정칙화 효과를 방해할 수 있음.
- 배치 정규화의 경우 내부 공변량 변화를 줄여 과대적합을 방지하는데, 드롭아웃은 일부 노드를 제거.
- 드롭아웃과 배치 정규화를 사용하는 경우에는 드롭아웃, 배치 정규화 순으로 적용.

### 그래디언트 클리핑

> 모델을 학습할 때 기울기가 너무 커지는 현상을 방지하는 데 사용되는 기술.

- 높은 가중치는 높은 분산 값을 갖게 하여 모델의 성능이 저하될 수 있음.
- 이러한 현상을 방지하기 위해 가중치 최댓값을 규제해 최대 임곗값을 초과하지 않도록 기울기를 잘라(Clipping) 설정한 임곗값으로 변경함.
- 그레이디언트 클리핑은 순환 신경망(RNN)이나 LSTM 모델을 학습하는 데 주로 사용 됨.
- 두 모델은 기울기 폭주에 취약한데, 그래디언트 클리핑은 최댓값을 억제하므로 많이 활용됨.

## 데이터 증강 및 변환

### 데이터 증강(Data Augmentation)

데이터가 가진 고유한 특징을 유지한 채 변형하거나 노이즈를 추가해 데이터세트의 크기를 인위적으로 늘리는 방법.

- 모델의 과대적합을 줄이고 일반화 능력을 향상시킬 수 있음.
- 데이터세트를 인위적으로 확장한다면 기존 데이터 품질을 유지한 채 특징을 살려 모델 학습에 사용할 수 있음.
- 데이터세트를 인위적으로 늘린다면 기존 데이터의 형질이 유지되므로 모델의 분산과 편향을 줄일 수 있음.
- 데이터 수집 시 잘못된 정보가 들어오는 문제가 발생되지 않음.
- 특정 클래스의 데이터 수가 적은 경우 데이터 증강을 통해 데이터 불균형을 완화할 수 있음.
- 모델의 일반화 능력과 클래스 간 불균형을 완화할 수 있지만, 기존 데이터를 변형하거나 노이즈를 추가하므로 너무 많은 변형이나 노이즈를 추가한다면 기존 데이터가 가진 특징이 파괴될 수 있음.

### 텍스트 데이터

> 문서 분류 및 요약, 문장 번역과 같은 자연어 처리 모델을 구성할 때 데이터세트의 크기를 쉽게 늘리기 위해서 사용됨.

- 텍스트 데이터 증강 방법은 크게 삽입, 삭제, 교체, 대체, 생성, 반의어, 맞춤법 교정, 역번역 등이 있음.
- 책에서는 **자연어 처리 데이터 증강(NLPAUG) 라이브러리**를 활용해 텍스트 데이터를 증강함.

### 이미지 데이터

> 객체 검출 및 인식, 이미지 분류와 같은 이미지 처리 모델을 구성할 때 데이터세트의 크기를 쉽게 늘리기 위해 사용됨.

- 회전, 대칭, 이동, 크기 조정 등
- 책에서는 토치비전 라이브러리(torchvision)와 이미지 증강 라이브러리(imgaug) 활용.
- 이미지 증강은 이미지를 변형하기 때문에 과도하게 증강하면 본래의 특징이 소실될 수 있으며, 실제 데이터에 존재하지 않는 데이터가 생성될 수 있음.
- 컷아웃과 무작위 지우기는 거의 동일한 시점에 제안된 증강 및 정칙화 방법임. **컷아웃**은 이미지에서 **임의의 사각형 영역을 삭제하고 0의 픽셀값으로 채우는 방법**이며, **무작위 지우기**는 **임의의 사각형 영역을 삭제하고 무작위 픽셀값으로 채우는 방법**임.
- **컷아웃**은 동영상에서 **폐색 영역(Occlusion)**에 대해 모델이 더 강건하게 해주며, **무작위 지우기**는 일부 영역이 누락되거나 잘렸을 때 더 강건한 모델을 만들 수 있게 함.
- **컷믹스**는 네이버 클로바에서 발표한 이미지 증강 방법으로 **이미지 패치 영역에 다른 이미지를 덮어씌우는 방법**. 이미지 영역을 잘라내고 붙여넣기 하는 방법으로 볼 수 있음.
- 컷믹스는 패치 위에 새로운 패치를 덮어씌워 비교적 **자연스러운 이미지를 구성**함.

## 사전 학습된 모델

> 대규모 데이터세트로 학습된 딥러닝 모델로 이미 학습이 완료된 모델을 의미.

- 사전 학습된 모델 자체를 현재 시스템에 적용하거나 사전 학습된 임베딩 벡터를 활용해 모델을 구성할 수 있음.
- 이미 학습된 모델의 일부를 활용하거나 추가 학습을 통해 모델의 성능을 끌어낼 수 있음.
- 이미 다양한 작업에서 성능을 검증한 모델이므로 사전 학습된 모델을 사용하면 안정되고 우수한 성능을 기대할 수 있음. 또한 대규모 데이터세트에서 데이터의 특징을 학습했으므로 유사한 작업에 대해서도 우수한 성능을 기대할 수 있음.
- 처음부터 모델을 훈련하지 않으므로 학습에 필요한 시간이 대폭 감소해 모델 개발 프로세스를 가속화하고 모델의 성능을 향상시킬 수 있음.

사전 학습된 모델은 전이학습과 같은 작업뿐만 아니라 백본 네트워크로 사용되며, 대규모 데이터에서 학습한 지식을 활용하여 소량의 데이터로도 우수한 성능을 달성할 수 있음.

### 백본

> 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 딥러닝 모델의 일부를 의미.

백본에 대한 개념은 합성곱 신경망 모델인 VGG, ResNet, Mask R-CNN 논문 등에서 직간접적으로 언급.

논문에서 합성곱 계층이 입력 이미지를 고차원 특징 벡터로 변환해 이미지 분류 작업을 돕는 특징 추출기의 역할로 사용할 수 있다는 점에서 백본이라는 용어가 등장.

백본 네트워크는 입력 데이터에서 특징을 추출하므로 노이즈와 불필요한 특성을 제거하고 가장 중요한 특징을 추출할 수 있음. 이렇게 추출된 특징을 활용해 새로운 모델이나 기능의 입력으로 사용함.
