# 트랜스포머

- 딥러닝 모델 중 하나.
- 2017년 ‘Attention is All You Need’ 논문을 통해 소개된 신경망 아키텍처.
- 트랜스포머 주요 기능 중 하나 → 기존의 순환 신경망과 같은 순차적 방식이 아닌 **병렬로 입력 시퀀스를 처리하는 기능.**
- 긴 시퀀스의 경우 트랜스포머 모델을 순환 신경망 모델보다 훨씬 더 빠르고 효율적으로 처리.
- 트랜스포머 모델 구조 : 기존의 순차 처리나 반복 연결에 의존하지 않고 입력 토큰 간의 관계를 직접 처리하고 이해할 수 있도록 하는 **셀프 어텐션**을 기반으로 함. ⇒ 모델이 재귀나 합성곱 연산 없이 입력 토큰간의 관계를 직접 모델링 가능. (순환 신경망이나 합성곱 신경망과 달리 **어텐션 메커니즘** 만을 사용해 시퀀스 임베딩 표현)
    - 어텐션 메커니즘 : 인코더와 디코더 간의 상호작용으로 입력 시퀀스의 중요한 부분에 초점을 맞추어 문맥을 이해하고 적절한 출력을 생성. 이 때 인코더와 디코더 단어 사이의 상관관계를 계싼하여 중요한 정보에 집중함. ⇒ 긴 문장에서도 중요한 정보에 집중할 수 있음.
    - **Attention Mechanism (주의 메커니즘)과 Self-Attention의 차이가 뭘까?**
        - 모두 트랜스포머 모델의 핵심 개념이지만, 그 역할과 사용 방식에서 차이가 있음.
        - Self-Att드
```python
import math
import torch
from torch import nn

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len, dropout = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p = dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0 / d_model)))

        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[: x.size(0)]
        return self.dropout(x)

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, emb_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

class Seq2SeqTransformer(nn.Module):
    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, max_len, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward, dropout = 0.1,):
        super().__init__()
        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(d_model = emb_size, max_len = max_len, dropout = dropout)
        self.transformer = nn.Transformer(d_model = emb_size, nhead = nhead, num_encoder_layers = num_encoder_layers, num_decoder_layers = num_decoder_layers, dim_feedforward = dim_feedforward, dropout = dropout,)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)

    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):

        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src = src_emb, tgt = tgt_emb, src_mask = src_mask, tgt_mask = tgt_mask, memory_mask = None, src_key_padding_mask = src_padding_mask, tgt_key_padding_mask = tgt_padding_mask, memory_key_padding_mask = memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src, src_mask):
        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)

    def decode(self, tgt, memory, tgt_mask):
        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)
```
## 트랜스포머 모델 구조
```python
from torch import optim

BATCH_SIZE = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

model = Seq2SeqTransformer(num_encoder_layers = 3, num_decoder_layers = 3, emb_size = 512, max_len = 512, nhead = 8, src_vocab_size = len(vocab_transform[SRC_LANGUAGE]), tgt_vocab_size = len(vocab_transform[TGT_LANGUAGE]), dim_feedforward = 512,).to(DEVICE)

criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX).to(DEVICE)
optimizer = optim.Adam(model.parameters())

for main_name, main_module in model.named_children():
    print(main_name)

    for sub_name, sub_modeule in main_module.named_children():
        print("└", sub_name)

        for ssub_name, ssub_module in sub_modeule.named_children():
            print("|    └", ssub_name)

            for sssub_name, sssub_module in ssub_module.named_children():
                print("|    |   └", sssub_name)
```
## 모델 구조 실행 결과
```python
src_tok_emb
└ embedding
tgt_tok_emb
└ embedding
positional_encoding
└ dropout
transformer
└ encoder
|    └ layers
|    |   └ 0
|    |   └ 1
|    |   └ 2
|    └ norm
└ decoder
|    └ layers
|    |   └ 0
|    |   └ 1
|    |   └ 2
|    └ norm
generator
```
## 모델 학습 및 평가 코드
```python
def run(model, optimizer, criterion, split):
    model.train() if split == "train" else model.eval()
    data_iter = Multi30k(split = split, language_pair = (SRC_LANGUAGE, TGT_LANGUAGE))
    dataloader = DataLoader(data_iter, batch_size = BATCH_SIZE, collate_fn = collator)

    losses = 0

    for source_batch, target_batch in dataloader:
        source_batch = source_batch.to(DEVICE)
        target_batch = target_batch.to(DEVICE)

        target_input = target_batch[: -1, :]
        target_output = target_batch[1 :, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(source_batch, target_input)

        logits = model(src = source_batch, trg = target_input, src_mask = src_mask, tgt_mask = tgt_mask, src_padding_mask = src_padding_mask, tgt_padding_mask = tgt_padding_mask, memory_key_padding_mask = src_padding_mask)

        optimizer.zero_grad()
        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))
        if split == "train":
            loss.backward()
            optimizer.step()
        losses += loss.item()

    return losses / len(list(dataloader))

for epoch in range(5):
    train_loss = run(model, optimizer, criterion, "train")
    val_loss = run(model, optimizer, criterion, "valid")

    print(f"Epoch: {epoch + 1}, Train loss: {train_loss: .3f}, Val loss: {val_loss: .3f}")
```
## 트랜스포머 모델의 번역 결과
```python
def greedy_decode(model, source_tensor, source_mask, max_len, start_symbol):
    source_tensor = source_tensor.to(DEVICE)
    source_mask = source_mask.to(DEVICE)

    memory = model.encode(source_tensor, source_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len - 1):
        memory = memory.to(DEVICE)
        target_mask = generate_square_subsequent_mask(ys.size(0))
        target_mask = target_mask.type(torch.bool).to(DEVICE)

        out = model.decode(ys, memory, target_mask)
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim = 1)
        next_word = next_word.item()

        ys = torch.cat([ys, torch.ones(1, 1).type_as(source_tensor.data).fill_(next_word)], dim = 0)
        if next_word == EOS_IDX:
            break

    return ys

def translate(model, source_sentence):
    model.eval()
    source_tensor = text_transform[SRC_LANGUAGE](source_sentence).view(-1, 1)
    num_tokens = source_tensor.shape[0]
    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
    tgt_tokens = greedy_decode(model, source_tensor, src_mask, max_len = num_tokens + 5, start_symbol = BOS_IDX).flatten()
    output = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))[1 : -1]
    return " ".join(output)

output_oov = translate(model, "Eine Gruppe von Menschen steht vor einem Iglu.")
output = translate(model, "Eine Gruppe von Menschen steht vor einem Gebäude .")
print(output_oov)
print(output)
```
