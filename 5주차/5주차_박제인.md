# 트랜스포머

- 딥러닝 모델 중 하나.
- 2017년 ‘Attention is All You Need’ 논문을 통해 소개된 신경망 아키텍처.
- 트랜스포머 주요 기능 중 하나 → 기존의 순환 신경망과 같은 순차적 방식이 아닌 **병렬로 이볅 시퀀스를 처리하는 기능.**
- 긴 시퀀스의 경우 트랜스포머 모델을 순환 신경망 모델보다 훨씬 더 빠르고 효율적으로 처리.
- 트랜스포머 모델 구조 : 기존의 순차 처리나 반복 연결에 의존하지 않고 입력 토큰 간의 관계를 직접 처리하고 이해할 수 있도록 하는 **셀프 어텐션**을 기반으로 함. ⇒ 모델이 재귀나 합성곱 연산 없이 입력 토큰간의 관계를 직접 모델링 가능. (순환 신경망이나 합성곱 신경망과 달리 **어텐션 메커니즘** 만을 사용해 시퀀스 임베딩 표현)
    - 어텐션 메커니즘 : 인코더와 디코더 간의 상호작용으로 입력 시퀀스의 중요한 부분에 초점을 맞추어 문맥을 이해하고 적절한 출력을 생성. 이 때 인코더와 디코더 단어 사이의 상관관계를 계싼하여 중요한 정보에 집중함. ⇒ 긴 문장에서도 중요한 정보에 집중할 수 있음.
    - **Attention Mechanism (주의 메커니즘)과 Self-Attention의 차이가 뭘까?**
        - 모두 트랜스포머 모델의 핵심 개념이지만, 그 역할과 사용 방식에서 차이가 있음.
        - Self-Attention는  사용 맥락이 시퀀스 내에서의 관계에 집중되고 **Attention Mechanism은**  입력의 특정 부분에 더 집중한다는 차이가 있음.
        - **Self-Attention**은 Attention Mechanism의 한 종류임.
        - **Attention Mechanism**: 인코더-디코더 구조에서 입력과 출력 간의 중요한 부분을 강조하는 역할을 함. 한 시퀀스의 입력과 다른 시퀀스의 출력 간의 관계를 계산.
        - **Self-Attention**: 시퀀스 내에서 각 요소들이 서로에게 얼마나 중요한지를 계산하는 메커니즘임. 같은 시퀀스 내에서 각 요소 간의 관계를 고려하여 각 요소의 표현을 조정함.
- 트랜스포머 모델 학습은 **대용량** 데이터 세트에서 매우 효율적, 데이터의 양이 많은 기계 번역, 언어 모델링, 텍스트 요약, 챗봇, 음성 인식  등의 작업에 적합.
    - 왜 대용량 데이터 세트에 적합할까?
        - 이전의 RNN(Recurrent Neural Network)이나 LSTM(Long Short-Term Memory) 같은 모델들은 순차적으로 데이터를 처리했지만, 트랜스포머는 병렬 처리가 가능하다. 이는 트랜스포머가 대규모 데이터셋을 더 빠르게 처리할 수 있도록 해준다. (병렬처리가 핵심.)
- **Positional Encoding**
    
    트랜스포머 모델은 입력 데이터에 순서 정보가 포함되어 있지 않음, 따라서 모델이 입력 시퀀스의 위치 정보를 알 수 있도록 Positional Encoding을 사용함. ⇒ 모델이 단어의 순서를 인식할 수 있음.
    
- **Encoder-Decoder 구조**
    - 트랜스포머는 일반적으로 인코더-디코더 구조로 구성.
    - **Encoder**: 입력 시퀀스를 처리하여 그 시퀀스의 표현을 생성.
    - **Decoder**: 위치 인코딩이 적용된 타깃 데이터의 입력 임베딩을 입력받음. 표현을 바탕으로 출력 시퀀스를 생성. 번역이나 텍스트 생성 작업에서 주로 사용.

## 트랜스포머 기반 모델

1. 오토 인코딩 (Auto-Encoding)
    - 랜덤하게 문장의 일부를 빈칸 토큰으로 만들고 해당 빈칸에 어떤 단어가 적절할지 예측하는 작업(Task)을 수행.
    - 인코더 : 예측되는 토큰의 양옆에 있는 토큰들을 참조하기 때문에 **‘양방향 구조’**
2. 자기 회귀 (Auto-Regressive)
    - 현재 시점의 데이터가 이전 시점의 데이터에 의존하여 생성되는 확률 모델
    - 다음 단어를 생성할 때 이전에 생성된 단어들에 기반하여 다음 단어를 예측하는 방식이 자기 회귀 모델의 기본 원리
    - 과거의 데이터를 이용해 현재 또는 미래의 데이터를 예측하는 모델.
    - 활용
        - 시계열 분석
            - ex: 주식 가격 예측
        - 자연어 처리
            - ex:  "The cat sat on the"라는 문장이 있을 때, 자기 회귀 모델은 "the" 다음에 올 단어를 예측하기 위해 앞선 단어들("The cat sat on")을 고려.
3. GPT (Generative Pre-trained Transformer)
    - 2018년 OpenAI에서 발표한 트랜스포머 기반 언어 모델
    - ELMo와 같이 대규모 말뭉치로 사전 학습된 모델로 다양한 다운스트림 작업에서 우수한 성능 보임.
    - 트랜스포머의 디코더를 여러 층 쌓아 만든 언어 모델
      

## 트랜스포머 모델 구조
```python
from torch import optim

BATCH_SIZE = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

model = Seq2SeqTransformer(num_encoder_layers = 3, num_decoder_layers = 3, emb_size = 512, max_len = 512, nhead = 8, src_vocab_size = len(vocab_transform[SRC_LANGUAGE]), tgt_vocab_size = len(vocab_transform[TGT_LANGUAGE]), dim_feedforward = 512,).to(DEVICE)

criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX).to(DEVICE)
optimizer = optim.Adam(model.parameters())

for main_name, main_module in model.named_children():
    print(main_name)

    for sub_name, sub_modeule in main_module.named_children():
        print("└", sub_name)

        for ssub_name, ssub_module in sub_modeule.named_children():
            print("|    └", ssub_name)

            for sssub_name, sssub_module in ssub_module.named_children():
                print("|    |   └", sssub_name)
```
## 모델 구조 실행 결과
```python
src_tok_emb
└ embedding
tgt_tok_emb
└ embedding
positional_encoding
└ dropout
transformer
└ encoder
|    └ layers
|    |   └ 0
|    |   └ 1
|    |   └ 2
|    └ norm
└ decoder
|    └ layers
|    |   └ 0
|    |   └ 1
|    |   └ 2
|    └ norm
generator
```
## 모델 학습 및 평가 코드
```python
def run(model, optimizer, criterion, split):
    model.train() if split == "train" else model.eval()
    data_iter = Multi30k(split = split, language_pair = (SRC_LANGUAGE, TGT_LANGUAGE))
    dataloader = DataLoader(data_iter, batch_size = BATCH_SIZE, collate_fn = collator)

    losses = 0

    for source_batch, target_batch in dataloader:
        source_batch = source_batch.to(DEVICE)
        target_batch = target_batch.to(DEVICE)

        target_input = target_batch[: -1, :]
        target_output = target_batch[1 :, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(source_batch, target_input)

        logits = model(src = source_batch, trg = target_input, src_mask = src_mask, tgt_mask = tgt_mask, src_padding_mask = src_padding_mask, tgt_padding_mask = tgt_padding_mask, memory_key_padding_mask = src_padding_mask)

        optimizer.zero_grad()
        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))
        if split == "train":
            loss.backward()
            optimizer.step()
        losses += loss.item()

    return losses / len(list(dataloader))

for epoch in range(5):
    train_loss = run(model, optimizer, criterion, "train")
    val_loss = run(model, optimizer, criterion, "valid")

    print(f"Epoch: {epoch + 1}, Train loss: {train_loss: .3f}, Val loss: {val_loss: .3f}")
```
## 트랜스포머 모델의 번역 결과
```python
def greedy_decode(model, source_tensor, source_mask, max_len, start_symbol):
    source_tensor = source_tensor.to(DEVICE)
    source_mask = source_mask.to(DEVICE)

    memory = model.encode(source_tensor, source_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len - 1):
        memory = memory.to(DEVICE)
        target_mask = generate_square_subsequent_mask(ys.size(0))
        target_mask = target_mask.type(torch.bool).to(DEVICE)

        out = model.decode(ys, memory, target_mask)
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim = 1)
        next_word = next_word.item()

        ys = torch.cat([ys, torch.ones(1, 1).type_as(source_tensor.data).fill_(next_word)], dim = 0)
        if next_word == EOS_IDX:
            break

    return ys

def translate(model, source_sentence):
    model.eval()
    source_tensor = text_transform[SRC_LANGUAGE](source_sentence).view(-1, 1)
    num_tokens = source_tensor.shape[0]
    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
    tgt_tokens = greedy_decode(model, source_tensor, src_mask, max_len = num_tokens + 5, start_symbol = BOS_IDX).flatten()
    output = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))[1 : -1]
    return " ".join(output)

output_oov = translate(model, "Eine Gruppe von Menschen steht vor einem Iglu.")
output = translate(model, "Eine Gruppe von Menschen steht vor einem Gebäude .")
print(output_oov)
print(output)
```
## 모델 실습 코드 -  네이버 영화 리뷰 데이터세트를 불러오는 방법
### 허깅 페이스 트랜스포머 라이브러리의 BERT 모델과 네이버 영화 리뷰 감정 분석 데이터세트를 활용해 분류 모델을 학습.

```python
import numpy as np
import pandas as pd
from Korpora import Korpora

corpus = Korpora.load("nsmc")
df = pd.DataFrame(corpus.test).sample(20000, random_state = 42)
train, valid, test = np.split(df.sample(frac = 1, random_state = 42), [int(0.6 * len(df)), int(0.8 * len(df))])

print(train.head(5).to_markdown())
print(f"Training Data Size: {len(train)}")
print(f"Validation Data Size: {len(valid)}")
print(f"Testing Data Size: {len(test)}")
```

## BERT 토크나이저 클래스로 데이터 전처리 및 데이터 로더 적용 방법
```python
import torch
from transformers import BertTokenizer
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.data import RandomSampler, SequentialSampler

def make_dataset(data, tokenizer, device):
    tokenized = tokenizer(text = data.text.tolist(), padding = "longest", truncation = True, return_tensors = "pt")

    input_ids = tokenized["input_ids"].to(device)
    attention_mask = tokenized["attention_mask"].to(device)
    labels = torch.tensor(data.label.values, dtype = torch.long).to(device)

    return TensorDataset(input_ids, attention_mask, labels)

def get_dataloader(dataset, sampler, batch_size):
    data_sampler = sampler(dataset)
    dataloader = DataLoader(dataset, sampler = data_sampler, batch_size = batch_size)

    return dataloader

epochs = 5
batch_size = 32
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = "bert-base-multilingual-cased", do_lower_case = False)

train_dataset = make_dataset(train, tokenizer, device)
train_dataloader = get_dataloader(train_dataset, RandomSampler, batch_size)

valid_dataset = make_dataset(valid, tokenizer, device)
valid_dataloader = get_dataloader(valid_dataset, SequentialSampler, batch_size)

test_dataset = make_dataset(test, tokenizer, device)
test_dataloader = get_dataloader(test_dataset, SequentialSampler, batch_size)

print(train_dataset[0])
```

## BERT 모델 설정 코드
```python
from torch import optim
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path = "bert-base-multilingual-cased", num_labels = 2).to(device)
optimizer = optim.AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)
```
