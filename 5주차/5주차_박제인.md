# 트랜스포머

- 딥러닝 모델 중 하나.
- 2017년 ‘Attention is All You Need’ 논문을 통해 소개된 신경망 아키텍처.
- 트랜스포머 주요 기능 중 하나 → 기존의 순환 신경망과 같은 순차적 방식이 아닌 **병렬로 입력 시퀀스를 처리하는 기능.**
- 긴 시퀀스의 경우 트랜스포머 모델을 순환 신경망 모델보다 훨씬 더 빠르고 효율적으로 처리.
- 트랜스포머 모델 구조 : 기존의 순차 처리나 반복 연결에 의존하지 않고 입력 토큰 간의 관계를 직접 처리하고 이해할 수 있도록 하는 **셀프 어텐션**을 기반으로 함. ⇒ 모델이 재귀나 합성곱 연산 없이 입력 토큰간의 관계를 직접 모델링 가능. (순환 신경망이나 합성곱 신경망과 달리 **어텐션 메커니즘** 만을 사용해 시퀀스 임베딩 표현)
    - 어텐션 메커니즘 : 인코더와 디코더 간의 상호작용으로 입력 시퀀스의 중요한 부분에 초점을 맞추어 문맥을 이해하고 적절한 출력을 생성. 이 때 인코더와 디코더 단어 사이의 상관관계를 계싼하여 중요한 정보에 집중함. ⇒ 긴 문장에서도 중요한 정보에 집중할 수 있음.
    - **Attention Mechanism (주의 메커니즘)과 Self-Attention의 차이가 뭘까?**
        - 모두 트랜스포머 모델의 핵심 개념이지만, 그 역할과 사용 방식에서 차이가 있음.
        - Self-Att드
```python
import math
import torch
from torch import nn

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len, dropout = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p = dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0 / d_model)))

        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[: x.size(0)]
        return self.dropout(x)

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, emb_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

class Seq2SeqTransformer(nn.Module):
    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, max_len, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward, dropout = 0.1,):
        super().__init__()
        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(d_model = emb_size, max_len = max_len, dropout = dropout)
        self.transformer = nn.Transformer(d_model = emb_size, nhead = nhead, num_encoder_layers = num_encoder_layers, num_decoder_layers = num_decoder_layers, dim_feedforward = dim_feedforward, dropout = dropout,)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)

    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):

        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src = src_emb, tgt = tgt_emb, src_mask = src_mask, tgt_mask = tgt_mask, memory_mask = None, src_key_padding_mask = src_padding_mask, tgt_key_padding_mask = tgt_padding_mask, memory_key_padding_mask = memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src, src_mask):
        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)

    def decode(self, tgt, memory, tgt_mask):
        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)
```
