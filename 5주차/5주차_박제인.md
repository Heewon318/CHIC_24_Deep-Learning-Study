# 트랜스포머

- 딥러닝 모델 중 하나.
- 2017년 ‘Attention is All You Need’ 논문을 통해 소개된 신경망 아키텍처.
- 트랜스포머 주요 기능 중 하나 → 기존의 순환 신경망과 같은 순차적 방식이 아닌 **병렬로 이볅 시퀀스를 처리하는 기능.**
- 긴 시퀀스의 경우 트랜스포머 모델을 순환 신경망 모델보다 훨씬 더 빠르고 효율적으로 처리.
- 트랜스포머 모델 구조 : 기존의 순차 처리나 반복 연결에 의존하지 않고 입력 토큰 간의 관계를 직접 처리하고 이해할 수 있도록 하는 **셀프 어텐션**을 기반으로 함. ⇒ 모델이 재귀나 합성곱 연산 없이 입력 토큰간의 관계를 직접 모델링 가능. (순환 신경망이나 합성곱 신경망과 달리 **어텐션 메커니즘** 만을 사용해 시퀀스 임베딩 표현)
    - 어텐션 메커니즘 : 인코더와 디코더 간의 상호작용으로 입력 시퀀스의 중요한 부분에 초점을 맞추어 문맥을 이해하고 적절한 출력을 생성. 이 때 인코더와 디코더 단어 사이의 상관관계를 계싼하여 중요한 정보에 집중함. ⇒ 긴 문장에서도 중요한 정보에 집중할 수 있음.
    - **Attention Mechanism (주의 메커니즘)과 Self-Attention의 차이가 뭘까?**
        - 모두 트랜스포머 모델의 핵심 개념이지만, 그 역할과 사용 방식에서 차이가 있음.
        - Self-Attention는  사용 맥락이 시퀀스 내에서의 관계에 집중되고 **Attention Mechanism은**  입력의 특정 부분에 더 집중한다는 차이가 있음.
        - **Self-Attention**은 Attention Mechanism의 한 종류임.
        - **Attention Mechanism**: 인코더-디코더 구조에서 입력과 출력 간의 중요한 부분을 강조하는 역할을 함. 한 시퀀스의 입력과 다른 시퀀스의 출력 간의 관계를 계산.
        - **Self-Attention**: 시퀀스 내에서 각 요소들이 서로에게 얼마나 중요한지를 계산하는 메커니즘임. 같은 시퀀스 내에서 각 요소 간의 관계를 고려하여 각 요소의 표현을 조정함.
- 트랜스포머 모델 학습은 **대용량** 데이터 세트에서 매우 효율적, 데이터의 양이 많은 기계 번역, 언어 모델링, 텍스트 요약, 챗봇, 음성 인식  등의 작업에 적합.
    - 왜 대용량 데이터 세트에 적합할까?
        - 이전의 RNN(Recurrent Neural Network)이나 LSTM(Long Short-Term Memory) 같은 모델들은 순차적으로 데이터를 처리했지만, 트랜스포머는 병렬 처리가 가능하다. 이는 트랜스포머가 대규모 데이터셋을 더 빠르게 처리할 수 있도록 해준다. (병렬처리가 핵심.)
- **Positional Encoding**
    
    트랜스포머 모델은 입력 데이터에 순서 정보가 포함되어 있지 않음, 따라서 모델이 입력 시퀀스의 위치 정보를 알 수 있도록 Positional Encoding을 사용함. ⇒ 모델이 단어의 순서를 인식할 수 있음.
    
- **Encoder-Decoder 구조**
    - 트랜스포머는 일반적으로 인코더-디코더 구조로 구성.
    - **Encoder**: 입력 시퀀스를 처리하여 그 시퀀스의 표현을 생성.
    - **Decoder**: 위치 인코딩이 적용된 타깃 데이터의 입력 임베딩을 입력받음. 표현을 바탕으로 출력 시퀀스를 생성. 번역이나 텍스트 생성 작업에서 주로 사용.

## 트랜스포머 기반 모델

1. 오토 인코딩 (Auto-Encoding)
    - 랜덤하게 문장의 일부를 빈칸 토큰으로 만들고 해당 빈칸에 어떤 단어가 적절할지 예측하는 작업(Task)을 수행.
    - 인코더 : 예측되는 토큰의 양옆에 있는 토큰들을 참조하기 때문에 **‘양방향 구조’**
2. 자기 회귀 (Auto-Regressive)
    - 현재 시점의 데이터가 이전 시점의 데이터에 의존하여 생성되는 확률 모델
    - 다음 단어를 생성할 때 이전에 생성된 단어들에 기반하여 다음 단어를 예측하는 방식이 자기 회귀 모델의 기본 원리
    - 과거의 데이터를 이용해 현재 또는 미래의 데이터를 예측하는 모델.
    - 활용
        - 시계열 분석
            - ex: 주식 가격 예측
        - 자연어 처리
            - ex:  "The cat sat on the"라는 문장이 있을 때, 자기 회귀 모델은 "the" 다음에 올 단어를 예측하기 위해 앞선 단어들("The cat sat on")을 고려.
3. GPT (Generative Pre-trained Transformer)
    - 2018년 OpenAI에서 발표한 트랜스포머 기반 언어 모델
    - ELMo와 같이 대규모 말뭉치로 사전 학습된 모델로 다양한 다운스트림 작업에서 우수한 성능 보임.
    - 트랜스포머의 디코더를 여러 층 쌓아 만든 언어 모델
