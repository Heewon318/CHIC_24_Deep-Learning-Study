# 7장 트랜스포머 (p.355~p.453)

7장 내용을 읽고 정리한 내용입니다.

## Transformer

- "Attention is All You Need" 논문을 통해 소개된 **신경망 아키텍처**
- 기존 순환 신경망과 같은 순차적 방식이 아닌 병렬로 입력 시퀀스를 처리하는 기능
- 긴 시퀀스의 경우, 순환 신경망 모델보다 훨씬 더 빠르고 효율적으로 처리함.

트랜스포머 모델의 구조가 기존의 `순차 처리(Sequential Processing)`나 `반복연결(Recurrent Connections)`에 의존하지 않고, 입력 토큰 간의 관계를 직접 처리하고 이해할 수 있도록 하는 셀프 어텐션을 기반으로 하기 때문.

- 트랜스포머 모델의 학습은 대용량 데이터세트에서 매우 효율적이며, 데이터 양이 많은 기계 번역과 같은 작업에 적합함.
- 언어 모델링 및 텍스트 분류와 같은 작업에서 매우 효과적.
- 광범위한 자연어 처리 작업에서 높은 효율을 보임.
- 기계 번역, 언어 모델링, 텍스트 요약과 같은 장기적인 종속성을 포함하는 작업에 주로 사용되며, 자연어 처리 분야에서 널리 사용되고 영향력이 큰 모델이 되었음.

## 입력 임베딩과 위치 인코딩

트랜스포머 모델의 입력은 임베딩(Embedding)과 위치 인코딩(Positional Encoding)을 거쳐 인코더에 전달됨.

- **입력 임베딩**: 텍스트 데이터(단어 또는 토큰)를 벡터 형태로 변환. 임베딩은 단어 간의 의미적 유사성을 반영하도록 학습된 벡터임.
- **위치 인코딩**: 트랜스포머는 순서를 고려하지 않는 모델이므로, 순서 정보를 추가하기 위해 위치 인코딩을 사용. 이는 입력 토큰의 위치 정보를 포함하며, 주로 `sin함수`와 `cos함수`를 기반으로 계산됨.

## 특수 토큰

특수 토큰은 모델이 문장의 시작, 끝, 또는 패딩 등을 인식할 수 있도록 도와줌.

- [BOS]: Beginning of Sentence, 문장의 시작
- [EOS]: End of Sentence, 문장의 끝
- [UNK]: Unknown, 어휘 사전에 없는 단어로 모르는 단어를 의미. 모델이 이전에 본 적이 없는 단어를 처리할 때 사용됨.
- [PAD]: Padding, 모든 문장을 일정한 길이로 맞추기 위해 사용됨. 짧은 문장의 빈 공간을 채울 때 사용됨.

## 트랜스포머 인코더

트랜스포머의 인코더는 입력 문장의 정보를 인코딩하여 고차원 벡터 표현으로 변환함.

- **멀티헤드 셀프 어텐션(Multi-Head Self-Attention)**: 입력의 각 위치에 대해 모든 다른 위치와의 상관 관계(어텐션)을 계산함. 멀티헤드는 여러 개의 어텐션 메커니즘을 병렬로 수행하여 정보를 풍푸하게 인코딩함.

멀티헤드 어텐션 단계에서 입력 텐서 차원이 [N, S, d]라고 한다면, 입력 임베딩은 선형 변환을 통해 3개의 임베딩 벡터를 생성함.

- **쿼리 벡터(Q)**: 현재 시점에서 참고하고자 하는 정보의 위치를 나타내는 벡터. 인코더의 각 시점마다 생성됨.
- **키 벡터(K)**: 쿼리 벡터와 비교되는 대상으로 쿼리 벡터를 제외한 입력 시퀀스에서 탐색되는 벡터가 됨. 인코더의 각 시점에서 생성됨.
- **값 벡터(V)**: 쿼리 벡터와 키 벡터로 생성된 어텐션 스코어를 얼마나 반영할지 설정하는 가중치 역할을 함.

초기에는 비슷한 값을 가지지만, 모델 학습을 통해 각 벡터는 의도한 의미의 값을 갖게 됨.

## 트랜스포머 디코더

트랜스포머의 디코더는 인코더의 출력과 디코더의 이전 출력(자기회귀적 예측)을 결합하여 최종 출력 시퀀스를 생성함.

---

# GPT (Generative Pre-trained Transformer)

## GPT-1

- 트랜스포머의 디코더 부분을 기반으로 한 단방향 언어 모델
- 약 4.5GB의 BookCorpus(출판되지 않은 여러 장르의 책 7,000여 권을 모은 데이터세트) 데이터세트를 사용해 언어 모델을 사전학습함.

## GPT-2

- GPT-1의 확장판으로, 훨씬 더 많은 매개변수(1.2억 -> 15억)를 사용
- 제로샷 학습: 특정 작업에 대해 미세조정 없이도 강력한 성능을 발휘
- 미국의 웹사이트에서 스크래핑한 약 8백만 개의 문서를 사용해 약 40GB 데이터로 사전학습을 수행.

## GPT-3

- 1,750억 개의 매개변수를 가지는 초대형 언어 모델.
- 멀티 헤드 어텐션 과정에서 연산량을 줄이기 위해 희소 어텐션(Sparse Attention)과 일반 어텐션을 섞어 사용함.
- 웹 크롤링, 위키백과, 서적 등에서 수집한 약 45TB에 달하는 방대한 양의 데이터 세트를 이용하여 모델 학습.
- 질의응답, 번역, 요약, 문서 생성 등 다양한 다운스트림 작업에서도 높은 성능을 보임.

## GPT 3.5

- GPT-3의 개선판으로, 모델 효율성과성능을 더욱 향상시킴.
- RLHF(Reinforcement Learning from Human, 휴먼 피드백 강화 학습): 인간 피드백을 통해 모델의 출력을 더욱 자연스럽고 정확하게 조정함.

## GPT-4

- GPT 3.5보다 더 큰 규모와 성능을 자랑하며, 다양한 언어적 과제에서 인간 수준의 성과를 보여줌.
- 멀티 모달 학습: 텍스트 뿐만 아니라 이미지와 같은 다양한 입력 형식을 처리할 수 있는 능력을 포함하고 있음.

---

## BERT

Bidirection Encoder Representations from Transformer

- 2018년 구글에서 발표한 언어모델
- 트랜스포머 기반 양방향 인코더를 사용하는 자연어 처리 모델
- **`양방향 인코더`** 는 입력 시퀀스를 양쪽 방향에서 처리하여 이전과 이후의 단어를 모두 참조하면서 단어의 의미와 문맥을 파악함.

### 사전학습방법

- **Masked Language Model(MLM, 마스킹된 언어 모델링)**: 입력 문장의 일부 단어를 마스킹하여 모델이 해당 단어를 예측하도록 학습함. 이는 모델이 양방향 정보를 학습할 수 있게 함.
- **Next Sentence Prediction(NSP)**: 두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장에 이어지는 문장인지 여부를 예측하는 과제를 학습함.

---

## BART

Bidirectional Auto-Regressive Transformer

- 2019년 메타의 FAIR 연구소에서 발표한 트랜스포머 기반의 모델.
- BERT의 인코더와 GPT의 디코더를 결합한 시퀀스-시퀀스 구조로 노이즈 제거 오토인코더로 학습됨.

### 사전학습방법

- **디노이징 오토인코더(Denoising Autoencoder)**: 입력 텍스트에 인위적인 노이즈를 추가하고, 이를 복원하는 방식으로 학습함. BART는 텍스트의 순서를 바꾸거나, 문장의 일부를 마스킹하는 등의 방식으로 노이즈를 추가함.

### 미세조정방법

- BART는 다양한 다운스트림 작업(예: 요약, 번역, 생성 등)에서 미세조정(Fine-tuning)할 수 있으며, 인코더-디코더 구조 덕분에 텍스트 변환 작업에 강력한 성능을 보임.

---

## ELECTRA

Efficiently Learning an Encoder taht Classifies Token Replacements Accurately

- 2020년 구글에서 발표한 트랜스포머 기반의 모델.
- GAN을 사용해 학습하므로 이전 언어 모델과 비교하여 더 효율적인 학습이 가능함.
- BERT와 같은 모델과 비교한다면 모델의 매개변수 수가 더 적음. 그로 인해 모델의 크기가 줄어들어 모델을 더 빠르게 실행할 수 있으며, 더 적은 메모리를 사용함.

### 사전학습 방법

- Replaced Token Detection(RTD): ELECTRA는 일부 토큰을 생성자(Generator)가 다른 단어로 대체한 후, 판별자(Discriminator)가 해당 단어가 원래의 단어인지 아닌지를 예측하는 방식으로 학습함. 이 방법은 전통적인 MLM보다 학습 효율성이 높음.

---

## T5

Text-to-Text Transfer Transformer

- 2019년 구글에서 발표한 자연어 처리 분야의 딥러닝 모델로 트랜스포머 구조를 기반으로 함.
- 인코더-디코더 모델 구조를 바탕으로 GLUE, SuperGLUE, CNN/DM(Cable News Network/Daily Mail) 등의 데이터세트에서 SOTA(State-of-the-art, 특정 분야에서 현재 최고의 기술이나 성능을 나타내는 것)를 달성했으며 다양한 자연어 처리 작업에서 높은 성능을 보이는 모델임.
- 모든 NLP 작업을 텍스트 입력과 텍스트 출력의 형식으로 통일하여 학습함.
