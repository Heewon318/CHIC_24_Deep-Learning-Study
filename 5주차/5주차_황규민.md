# 트랜스포머
- 기존 순환 신경망의 문제점인 직렬 처리를 해결해 훨씬 빠르고 효율적인 처리가 가능
    - self-Attention 입력 토큰 간의 관계를 직접 처리하고 이해

대용량 데이터세트에서 매우 효율적, 언어 모델링 및 텍스트 분류와 같은 작업에서 매우 효과적

- 오토 인코딩(Auto-Encoding), 자기 회귀(Auto-Regressive) 방식 또는 두 개의 조합으로 학습
    - 오토 인코딩: 랜덤하게 문장의 일부를 빈칸 토큰으로 만들고 해당 빈칸에 어떤 단어가 적절할지 예측하는 작업을 수행, 양옆에 있는 토큰들을 참조하기 때문에 양방향 구조를 가지고 인코더라 함
    - 자기 회귀: 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 맞히는 작업 수행
    
    

# Transformer

Attention Mechanism을 사용해 시퀀스 임베딩을 표현, 입력 시퀀스의 중요한 부분에 초점을 맞추어 문맥을 이해하고 적절한 출력을 생성한다.

인코더는 입력 시퀀스를 임베딩해 고차원 벡터로 변환, 디코더는 인코더의 출력을 입력으로 받고 출력 시퀀스를 생성한다. 이때 어탠션 매커니즘은 인코더와 디코더는 단어 사이의 상관관계를 계산해 중요한 정보에 집중하고 이를 통해 입력 시퀀스의 각 단어가 출력 시퀀스의 어떤 단어와 관련 있는지를 파악해 번역이나 요약문과 관련된 작업을 수행

기존 순환 신경망 기반 모델보다 학습 속도가 빠르고, 병렬 처리가 가능해 대규모 데이터 세트에서도 높은 성능을 보임, 임베딩 과정에서 문장의 전체 정보를 고려하기 때문에 문장이 길어져도 성능이 유지된다.

- 구조

- 인코더와 디코더는 두 부분으로 구성 됨.
    - 각각 N개의 트랜스포머 블록(Transformer Block)으로 구성됨
        - 블록은 멀티 헤드 어탠션(Multi-Head Attention)과 순방향 신경망으로 이뤄짐
    - 멀티 헤드 어텐션은 입력 시퀀스에서 쿼리(Query), 키(Key), 값(Value) 벡터를 정의해 입력 시퀀스들의 관계를 셀프 어텐션(Self-Attetion)하는 벡터 표현 방법
        - 이 과정에서 각 키의 유사도를 계산하고, 해당 유사도를 가중치로 사용해 값 벡터를 합산한다.
    - 계산된 어텐션 행렬은 입력시퀀스 각 단어의 임베딩 벡터를 대체하고 입력 시퀀스 사이의 상호작용을 고려해 임베딩 벡터를 갱신
    - 순방향 신경망은 이 과정에서 산출된 임베딩을 더욱 고도화하기 위해 사용, 학습된 가중치들은 입력 시퀀스의 각 단어의 의미를 잘 파악할 수 있는 방식으로 갱신 됨
- 트렌스 포머에서는 입력 시퀀스 데이터를 소스(Source)와 타깃(Target) 데이터로 나눠 처리
    - 인코더는 소스 시퀀스 데이터를 위치 인코딩(Positional Enconding)된 임베딩으로 표현해 트렌스포머 블록의 출력 벡터를 생성한다.  입력 시퀀스 데이터의 관계를 잘 표현할 수 있게 구성
    - 디코더는 마스크 멀티 헤드 어텐션(Masked Multi-Head Attention)을 사용해 타깃 시퀀스 데이터를 순차적으로 생성시키고  이때 디코더 입력 시퀀스들의 관계를 고도화하기 위해 인코더의 출력 벡터 정보를 참조

최종적으로 생성된 디코더 출력 벡터는 선형 임베딩으로 재표현되어 이미지나 자연어 모델에 활용됨.

## 입력 인베딩과 위치 인코딩

위치 인코딩: 병렬 구조로 처리 돼 단어의 순서 정보가 없는 임베딩 벡터에 위치 정보를 추가함

- sin 함수와 cos 함수를 사용해 생성, 이를 통해 임베딩 벡터와 위치 정보가 결합된 최종 입력 벡터
    - 각 토큰의 위치를 각도로 표현해 위치 인코딩 벡터를 계산

Positional Encoding

단어의 위치를 각도 정보로 변환 후 sin or cos 함수를 적용

## 특수 토큰

입력 시퀀스의 시작과 끝을 나타내거나 마스킹(Masking) 영역으로 사용

자연어 처리에서 일반적으로 사용되는 토큰, BOS, EOS, UNK, PAD

생성된 문장 토큰 배열을 어휘 사전에 등장하는 위치에 원-핫 인코딩으로 표현

어휘사전의 크기: V, 입력 임베딩 차원: d, N개의 문장이 최대 S개의 토큰 길이를 가질 때

- 단어의 원-핫 벡터는[1,V] 크기를 갖고 임베딩 행렬[V, d]에 의해 [1, d]크기의 벡터로 변환 됨.
- 일반화 하면 [N, S, V]크기의 원-핫 벡터 텐서는 [N, S, d] 크기의 임베딩 텐서로 변환

임베딩  텐서는 입력으로 사용되고, 모든 계층에서 공유 되는 텐서로 사용

## 트랜스포머 인코더

위치 인코딩이 적용된 입력 인베딩을 입력받는데 멀티 헤드 어텐션 단계로 갈 때 선형 변환을 통해 3개의 임베딩 벡터를 생성 각 Q, K, V 벡터로 정의

Q: 현재 시점에서 참조하고자 하는 정보의 위치를 나타내는 벡터, 인코더의 각 시점마다 생성된다.

K: Q와 비교되는 대상으로 탐색되는 벡터가 된다, 각 시점에 생성

V: Q와 K로 생성된 어텐션 스코어를 얼마나 반영할지 설정하는 가중치 역활을 함

Q와 K 의 연관성은 내적 연산으로 [N, S, S] 어텐션 스코어 맵을 사용

어텐션 스코어 값을 구하고 그 값에 $\sqrt{d}$(벡터 차원의 제곱근) 을 나눠 보정

- 벡터 차원이 커질 때 스코어값이 같이 커지는 문제를 완화

보정된 어텐션 스코어를 소프트멕스 함수를 사용하고 이를 V와 내적해 셀프 어텐션 된 벡터 생성

- 이 과정을 반복해 셀프 어텐션된 스코어 맵을 생성

멀티 헤드는 이런 셀프 어텐션을 여러번 수행해 여러 개의 헤드를 만들고 각 헤드가 독립적으로 어텐션을 수행한 후 그 결과를 합친다.

[N, K, d] 텐서에 k개의 셀프 어텐션 벡터를 생성하고자 하면 [N, k, S, d/k] 형태를 구성

k개의 셀프 어텐션 벡터는 임베딩 차원 축으로 다시 병합(concatenation) [N, S, d] 형태로 출력

덧&정는 멀티헤드 어텐션을 통과하기 이전의 입력값 텐서와 통과한 이후 출력값 텐서를 더함으로 써 기울기 소실을 완화, 여기에 차원 축으로 정규화하는 계층 정규화를 적용

FFN은 2가지 방법을 적용할 수 있는데, 선형 임베딩과 ReLU로 이뤄진 인공 신경망이나 1차원 합성곱이 사용됨. 이어서 덧&정 과정이 수행

인코더는 어러 개의 트랜스 포머 인코더 블록으로 구성

디코더는 멀테 헤드 어텐션 모듈에서 참조되는 K, V 활용

## 트렌스포머 디코더

인코더의 소스 데이터가 K와 V로 사용, Q벡터는 타깃 데이터의 위치정보를 포함한입력 임베딩과 위치 인코딩을 더한 벡터가 됨

디코더에서는 멀티 헤드 어텐션을 마스크 멀티 헤드 어텐션 모듈로 대체해 인과성을 반영 시킨다.

- 어텐션 스코어 배을 계산할 때 첫 번째 쿼리 벡터가 첫 번째 K만 바라보도록 마스크를 씌움, 두 번째 쿼리 벡터는 첫 번째와 두 번째 K를 바라보게 마스크를 씌움.

마스크를 적용해 현재 위치 이전 단어들만 참조할 수 있도록 만들어 인과성이 보장됨

인코더의 멀티헤드 어텐션과 거의 동일하지만 셀프 어텐션을 방지하기 위한 마스킹이 차이점

# GPT(Generative Pre-trained Transformer)

모델 ELMo(Embeddings from Language Model) 대규모 말뭉치로 사전 학습된 모델로 다양한 다운 스트림 작업에서 우사한 성능을 보임

트랜스포머의 디코더를 여러층 쌓아 만든 언어 모델로  트랜스포머의 인코더는 입력 문장의 특징을 추출하는데 초점을 두고, 디코더는 입력 문장과 출력 문장간의 관계를 이해하고 출력하는 문장을 생성하는데 초점을 둠

- 이런 특성으로 디코더를 쌓아 모델을 구성하는 것이 자연어 생성과 같은 언어 모델링 작업에서 더 높은 성능을 발휘

일반적으로 자연어 생성, 기계 번역, 질의응답, 요약 등 다양한 자연어 처리 작업에 활용되며, 예측 성능이 뛰어나고 적은 데이터로도 높은 성능을 보임

많은 매걔변수를 줄위기 위해 RLHF(Reinforcement Learning from Human Feedback) 방법 도입

RLHF은 인간의 피드백을 이용하는 강화학습, 모델의 결과물을 인간이 평가하는 방식

- 기존보다 더욱 정확하고 효율적인 모델 학습이 가능해 60억 개의 가중치로도 뛰어난 성능을 보임.

## GPT-1

트랜스포머 구조를 바탕으로 한 당방향(Unidirecional) 언어 모델. 

- 텍스트를 생성할 때 현재 위치에서 이전 단어들에 대한 정보만 참고해 다음 단어를 예측하는 모델

디코더 부분만 사용해, 인코드-디코더 간 언텐션 연산을 제외, 디코더의 다중헤드 어텐션은 사용하지 않음.

문장의 일부만 보고 다음 토큰을 예측하도록 하는 언어 모델을 통해 사전 학습한다.

입력 임베딩은 토큰 이베딩과 위치 임베딩을 이용해 계산

미세 조정을 위한 다운스트림 작업에도 입출력 구조를 바꾸지 않고 언어 모델을 보조 학습해 사용

- 보조 학습은 모델이 주어진 주요 학습 작업 외에도 다른 작은 부가적인 학습 작업을 동시에 수행하는 것을 의미

다운스트림 작업에서는 원하는 목표에 맞게 모델을 세밀하게 조정해야 하기 때문에, 일반적인 언어 모델 학습 시 사용되는 손실 함수 외에 다운스트림 작업에서 필요한 추가 손실 함수가 필요함.

두 개의 손실 함수를 사용해 최적화 하면 보조 학습을 통해 언어 모델을 개선하는 동시에, 다운스트림 작업의 목표를 달성하기 위해 필요한 정보를 학습할 수 있다.

특수 토큰으로  시작의 <start>, 문장의 경계를 의미하는<delim>, 마지막을 의미하는<extract>

## GPT-2

버전 1과 비교해 더 많은 문장을 입력 받을 수 있게 되고 디코더 층이 더 깊어져 복잡한 패턴을 학습할 수 있게 됨.

제로-샷 학습이 가능해지고 별도의 미세 조정 없이 다운스트림 작업에서도 사용할 수 있게 사전 학습 과정에서 특정한 형식의 데이터를 입력함.

## GPT-3

모델은 거의 동일한 모델 구조를 갖지만 매개변수의 변화를 통해 더운 거대한 모델이 됨

멀티 헤드 어텐션 과정에서 연산량을 줄이기 위해 Sparse Attention과 일반 어텐션을 섞어 사용

토큰 임베딩의 크기도 대폭 증가해 긴 문장에 대한 처리 능력이 향상

큰 모델이여서 매우 느리고 비용이 많이 들어가는 단점 또 사전 학습된 데이터에 기반해 작동하기 때문에 데이터가 편향돼 있거나 정확하지 않을 경우 오류 발생

## GPT-3.5

버전3의 모델 구조와 동일하지만 새로운 학습 방법인 RLHF를 도입해 매개변수를 줄이고 모델의 자연수로움을 높임

이런 강과학습 과정을 지도 미세 조정(Supervised Fine-Tuning, SET)이라고 함

- 사람이 답변을 보고 평가한 것을 보상으로 모델을 학습
    - 보상 모델로 사람이 평가하고 그에 따른 랭킹을 부여함, 랭킹은 생산된 답변의 우수성을 평가한 결과이고, 이를 보상 모델의 입력으로 사용함. 보상 모델은 이렇게 생성된 답변과 랭킹 정보를 활용해 학습하고 이를 통해 더 자운스러운 문장을 생성

GPT모델은 seed에 따라 다른 답변을 생성하므로 내용이 서로 다른 여러 개의 답변이 생성될 수 있음

## GPT-4

텍스트 데이터뿐만 아니라 이미지 데이터도 인식 가능한 멀티모달(Multimodal)모델

처리가능한 데이터가 더 많아짐.

언어 모델이 일부 입력에 대해 현실적이지 않은 결과를 생성하는 환각(Hallucination)현상이 아직존재

- 모델이 인간과 같은추론과 판단 능력을 갖추지 못하고 부적절하거나 오류가 많은 답변을 제시하는 현상

# BERT(Bidirectional Encoder Representaitons from Transformers)

구글에서 발표한 언어 모델로 트랜스포머 기반 양방향 인코더를 사용하는 자연어 처리 모델

- 양방향(Bidrectional) 인코더는 입력 시퀀스를 양쪽 방향에서 처리하여 이전과 이후 단어 모두 참초해 단어의 의미와 문맥을 파악

기존 모델보다 더 정확하게 문맥을 파악하고, 다양한 자연어 처리 작업에서 높은 성능을 보임

대규모 데이터를 사용해 사전 학습돼 있으므로 전이 학습에 주로 활용, 일부 또는 전체를 다른 작업에서 재사용해 적은 양의 데이터로도 높은 정확도를 달성할 수 있고, 모델 학습 시간을 크게 단축

트랜스포머의 인코더를 기반으로 하고, 이를 효과적으로 재활용함으로써 학습데이터의 약과 다양성을 고려한 높은 성능을 보임.

입력 문장의 의미와 구조를 학습하고 다양한 자연어 처리 작업에 적용할 수 있는 사전 학습된 언어 모델. 트랜스포머의 인코더 모듈만 사용해 입력 문장을 처리

- 입력 문장의 단어를 양방향으로 처리해 문맥 정보를 모델링하므로 인코더 계층만 사용

사전 학습을 위해 마스킹된 언어 모델링(MLM)과 다음 문장을 예측하는 방법(NSP)를 사용

## 사전 학습 방법

(Masked Language Modeling) MLM은 입력 문장에서 임의로 일부 단어를 마스킹 하고 해당 단어를 예측하는 방식

NSP(Next Sentence Prediction)은 두 개의 문장이 주어졌을 때, 두번째 문장이 첫 번째 문장의 다음에 오는 문장인지 여부를 판단하는 작업, 두 문장이 연속적인 관계인지 아닌지 예측. 두 문장 간의 관계를 학습하고, 문장 간의 의미적인 유사성을 파악함

- 입력 문장에 특수토큰을 추가 문장의 시작[CLS], 두 개 이상의 문장을 구분하기 위한[SEP], 문장 내에서 임의로 선택된 단어를 가리키는[MASK] (텍스트 토큰 중 15%, 이 중 80%는 토큰으로 대체 10% 어휘 사전에 존재하는 무작위 단어로 변경, 나머지 10% 실제 토큰 그대로 새용)
- 토큰 임베딩(Token Embedding) 각 토큰에 대해 토큰 임베딩 벡터를 생성
- 위치 임베딩(Position Embedding) 입력 토큰의 위치 정보를 나타내는 임베딩, 각 입력 토큰의 위치 인덱스 정보를 입력받아 해당 위치의 임베딩 벡터를 출력, 토큰들의 상대적 위치 정보를 고려해 모델이 문장의 전체적인 의미를 파악할 수 있게 도움
- 문장 구분 임베딩(Token Type Embedding) 각 문장을 구분하는 임베딩을 생성

입력 임베딩 이후 트랜스포머 인코더 블록을 통과해 MLM 또는 NSP 작업 수행.

# BART(Bidirectional Auto-Regressive Transformer)

BERT의 인코더와 GPT의 디코더를 결합한 Seq-Seq구조로 노이즈 제거 오토인코더(Denoising Autoencoder)로 사전 학습됨

오토인코더 사전 학습 방법은 입력 데이터에 잡음을 추가하고, 잡음이 없는 원본 데이터를 복원하도록 학습하는 방식으로 수행

BETR의 인코더가 MLM로 문장 전체의 맥락을 이해하는 특징과 GPT의 디코더의 문장 내 단어들의 순서와 문맥을 파악해 다음에 올 단어를 예측 특징을 가져옴

- 노이즈가 추가된 텍스트 인코더에 입력하고 원본 텍스트를 디코더에 입력해 디코더가 원본 텍스트를 생성할 수 있게 학습하는 방식
    - 문장의 구조와 의미를 보존하면서 다양한 변형을 학습할 수 있음. 이 구조는 입력문장에 큰 제약 없이 노이즈 기법을 적용할 수 있어 더 풍부한 언어적 지식을 습득할 수 있음

순방향 정보로 인식할 수 있는 GPT의 단점을 개선해 양방향 문맥 정보를 반영하고 디코더를 사용해 문장 생성의 BERT의 단점을 개선

- 트랜스포머와 차이점

트랜스포머는 인코더의 모든 계층과 디코더의 모든 계층사이의 어텐션 연산을 수행

BART는 인코더의 마지막 계층과 디코더의 각 계층 사이에만 어텐션 연산을 수행

- 정보의 전달을 최적화하고 메모리 사용량을 줄임

## 사전 학습 방법

MLM 이외에도 다양한 노이즈 기법을 사용, 토큰 삭제, 문장 교환, 텍스트 채우기 기법

토큰 삭제: 입력 문장에서 불필요한 정보나 중요하지 않은 정보를 자동으로 필터링 처리할 수 있게되고 모델의 학습과 예측시간을 줄이고 일반화 성능을 향상

문장 순열: 마침표를 기준으로 문장을 나누고 문장의 순서를 섞는 방법, 단어들이 어떻게 연결돼 있는지 더 잘 파악하게 되고 입력 문장이 다른순서로 주어졌을 때도 잘 처리

문서 회전: 임의의 토큰으로 문서가 시작하도록, 문장 순열과 다르게 문장의 순서는 유지, 문서의 시작점을 인식할 수 있도록 함.

택스트 채우기: 몇 개의 토큰을 하나의 구간으로 묶고 일부 구간을 마스크 토큰으로 대체

## 미세 조정 방법

문장 분류 작업에서는 입력 문장을 인코더와 디코더에 동이하게 입력하고 디코더의 마지막 토큰 은닉 상태를 선형 불류기의 입력값으로 사용한다. 이때 BERT의 CLS 토큰과 비슷하지만, BART는 전체 입력과 어텐션 연산이 적용된 은닉 상태를 사용

토큰 분류 작업도 입력 문장을 동일하게 입력, 디코더의 각 시점별 마지막 은닉 상태를 토큰 분류기의 입력으로 사용한다. 전체 입력과 디코더의 각 시점별 은닉 상태와의 어텐션 연산을 수행한다

사전 학습된 인코더에 기계 번역을 위한 인코더를 추가해 작업할 때 추가된 인코더는 기존 단어 사전을 사용하지 않아도 되고 디코더는 사전 학습된 가중치와 단어 사전을 사용

첫 번째 단계에서는 인코더의 가중치와 위치 임베딩, 첫 번째 인코더 층의 셀프 어텐션 입력 행렬 가중치만 학습, 두 번째 단계에서는 모든 신경망의 가중치를 작은 반복으로 학습

문장 내부의 토큰 사이의 상관관계를 파악해 문장의 의미를 더욱 정확하게 이해할 수 있다.

루지(Recall-Oriented Understudy for Gisting Evaluation, ROUGE): 생성 요약문과 정답 요약문이 얼마나 유사한지 평가하기 위해 N-gram 정밀도와 재현율을 이용해 평가하는 지표

ROUGE-L: 생성된 요약문과 정답 요약문 사이에 최장 공통부분 수열(Longest Common Subsequence,LCS)기반의 통계 방식, 문장의 구조적 유사성을 고려하고 가장 길게 연속되는 N-gram을 식별 요약 문장이 입력 문장의 의미를 잘 전달하는지를 평가하는데 유용

ROUGE-LSUM: ROUGE-L의 변형 개행 문자를 문장 경계로 인식하고, 각 문장 쌍에 대해 LCS를 계산한 수, union-LCS라는 값을 계산. 각 문장 쌍의 LCS 합집합 한것으로 중복된 부분을 제거한 후 길이를 계산, 텍스트 생성 작업에서 요약의 정확성과 완전성을 모두 반영할 수 있는 지표

ROUGE-W: 가중치가 적용된 LCS방법으로 연속된 LCS에 가중치를 부여해 계산, 공통부분 문자열의 길이뿐만 아니라 해당 부분 문자열 내의 단어에 가중치를 부여해 평가하는 방식. 단어 간 유사도를 고려해 요약 문장이 입력 문장의 의미를 더욱 잘 전달하는지 평가하는데 유용함.

# ELECTRA(Efficiently Learning an Encoder that Classifies Token Replacements Accurately)

구글이 발표한 트랜스포머 기반의 모델

입력을 마스킹하는 대신 생성자(Generator)와 판별자(DIscriminator)를 사용해 사전 학습을 수행

생성적 적대 신경망(GAN)과 유사한 방법으로 학습

## 사전 학습 방법

생성자와 판별자 모두 트랜스포머 인코더 구조를 다르고, 

생성자 모델은 입력 문장의 문장의 일부 토큰을 마스크 처리하고 마스크 처리된 토큰이 원래 어던 토큰이엇는지 예측하며 학습

- 입력 문장의 15%를 마스크 처리

판별자는 각 입력 토큰이 원본 문장의 토큰인지 생성된 토큰인지 맞히며 학습 이를 RTD(Replaced Token Detection)이라 함

GAN과 차이점

1. 생성자 모델이 원래 토큰을 정확히 예측한 경우 생성된 토큰이 아니고 원본 토큰으로 인시
2. GAN은 생성 모델과 판별 모델이 적대적으로 학습함
- 생성모델은 판별 모델이 구분하기 어렵게, 판별모델은 실제 데이터와 생성데이터를 더 잘구분하게
- ELECTRA의 생성 모델은 마스킨된 언어 모델을 통해 학습되고, 판별 모델은 각 토큰이 바귄 토큰인지 원본인지 구분하도록 학습
1. GAN은 완전한 노이즈 벡터를 입력 받아 생성, ELECTRA의 생성 모델은 일부 토큰이 마스크 처리된 텍스트를 입력 받음

사전 학습이 완료되면 오직 판별 모델만 사용해 다운스트림 작업을 수행, 판별 모델은 트랜스포머 인코더로 구성되고, BERT와 동일한 구조

- 미세 조정도 동일한 방식

# T5(Text-to_Text Transfer Transformer)

입 출력을 모드 토큰 시퀀스로 처리하는 텍스트-텍스트(Text-to-Text)구조

- 입출력의 형태를 자유롭게 다룸

마스킹 토큰을 예측하는 것을 목적으로 사전 학습을 하고 미세 조정을 통해 다양한 자연어 처리 작업을 수행

- 미세 조정은 지도학습 방식으로 학습
