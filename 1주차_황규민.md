## 텐서(Tensor)

 모델의 입출력, 모델의 매개변수를 부호화 해 GPU 연산에 활용

텐서 속성 : 형태(shape), 자료형(dtype), 장치(device) CPU or GPU

CPU 텐서 와 GPU 텐서 연산 불가능, CPU텐서와 넘파이 배열 연산 가능, GPU와 넘파이는 불가능

GPU텐서 > CPU 텐서 > 넘파이 순서로 변환 : detach 메서드 사용

## 가설(Hypothesis)

의미 : 어떤 사실을 설명하거나 증명하기 위한 가정, 두 개 이상의 변수의 관계를 검증 가능한 형태로 기술해 변수간의 관계를 예측, (가설 h, 가설집합 H)

단일 가설 : 입력과 출력을 매핑하고 평가하고 예측하는데 사용하는 단일 시스템

가설 집합 : 출력에 입력을 매핑하기 위한 가설 공간으로, 모든 가설을 의미

### 통계적 가설 검증

: 독립 변수와 종속 변수 간의 관계를 가장 잘 근사(Approximation)시키기 위해 사용

- t-test
    - paired t-test : 동일한 항목 또는 그룹이 두 번 테스트 할 때 사용
    - unpaired t-test : 등분산성을 만족하는 두 개의 독립적인 그룹 간의 평균을 비교하는 데 사용
- 통계량이 크고 유의 확률이 작다면 귀무가설이 참일 확률이 낮다고 할 수 있음

## 손실 함수(Loss Function)

실젯값과 예측값을 통해 계산된 오찻값을 최소화해 정확도를 높이는 방법으로 학습

$손실함수\subset 비용함수 \subset 목적함수$ (Objective Function), (Cost Function)

목적함수 : 함숫값의 결과를 최댓값 또는 최솟값으로 최적화하는 함수

비용함수 : 전체 데이터에 대한 오차를 계산하는 함수

$오차 = (실제값 - 예측값)$

### 제곱오차(Squared Error, SE)

연속형 변수에 사용되는 손실함수

$Y: 실젯값, \hat Y:예측값$

실젯갑에서 예측값을 뺀 값의 제곱, 방향 보다 오차의 크기가 중요해서 절대값은 사용안함

$SE = (Y_i - \hat {Y_i})^2$

### 오차 제곱합(Sum of Squared for Error, SSE)

제곱 오차를 모두 더한 값 정확도는 모르지만, 하나의 값으로 모델을 평가 가능

$SSE = \displaystyle\sum _{i=1}^n(Y_i - \hat Y_i)^2$

### 평균제곱오차(Mean Squared Error, MES)

오차 제곱합에서 평균을 취하는 방법, 오차가 큰 것인지 데이터가 많은 것인지 구분하기 위해서

품질 측정 가능, 오차가 0에 가까워질수록 높은 품질, 회귀 분석에 많이 사용

- 파이토치에서 결괏값을 조절하는 제어변수가 존재하지 않기 때문에 매개변수를 입력받지 않

$MSE = \dfrac{1}{n}\displaystyle\sum _{i=1}^n(Y_i - \hat Y_i)^2$

- 루트평균제곱오차(Root Mean Squared for Error, RMES)
    
    제곱을 적용해 왜곡이 발생했지만 왜곡을 감소시키면 정밀도를 표현하기에 적합한 형태
    

### 교차 엔트로피(Cross-Entropy)

이산형 변수의 손실함수, 실젯값의 확률분포와 예측값의 확률분포 차이를 계산

$y:실제\space 확률분포, \hat y: 예측된 \space 확률분포$

$CE(y,\hat y) = -\displaystyle\sum_Jy_j\log \hat y_j$

결과적으로 예측확률과 실제확률 부분만 연산

## 최적화(Optimization)

목적 함수의 결괏값을 최적화 하는 변수를 찾는 알고리즘을 의미

오차를 최소로 줄이는 가중치와 편향을 구하는 것

최적의 가중치와 편향을 갖는 가설은 오찻값이 0에 가까운 함수

이는 오차에 대한 도함수의 변화량이 0에 가깝다는 의미

가중치와 오차에 대한 그래프의 극값이 가설을 가장 잘 표현하는 가중치와 오차가 됨

### 경사 하강법(Gradient Descent)

함수의 기울기를 낮은 곳을 계속 이동시켜 극값에 도달할 때까지 반복하는 알고리즘

$W_0 = Initial\space Value$

$W_{i+1} = W_i - \alpha\nabla f(W_i)$

$\alpha:가중치,\nabla f(W_i):기울기$

### 학습률(Learning Rate)

경사 하강법에서 $\alpha$는 학습률

적절한 값을 찾아줘야 함

$\because$시간이 오래걸리 거나, 그래프가 발산하여 아예 값을 찾지 못할 수 있음.

Local minimum에 속아 Global minimum을 못 찾을 수 있다.

이 외 모멘텀(Momentum), Adagrad(Adaptive, Gradinet), Adam(Adapitve Moment Estimation)

### 단순선형회귀(Simple linear regeression)

하이퍼파라메터를 초기화 하고 실행

optimizer.zero_grad: 텐서의 기울기는 grad 속성에 누적해서 더해지기 때문에 0으로 초기화

- 기울기가 weight = x가 아닌 weight += x 구조로 저장되기 때문에 중복 연산을 방지

cast.backward() : 역전파, 매개변수들의 기울기가 새로 계산

optimizer.step(): 계산된 기울기를 최적화에 반영시킴

- 신경망 패키지 torch.nn
    
    네트워크를 정의하거나 자동 미분, 계층 등을 정의할 수 있는 모듈
    
    - 선형 변환 클래스(Linear Transformation)
        
        torch.nn.Linear()
        

## 데이터세트와 데이터로더

[torch.utils.data](http://torch.utils.data) 모듈에 포함

### 데이터세트(data set)

데이터세트의 일반적인 구조는 데이터베이스의 테이블과 같은 형태로 구성

직접 반영하면 모듈화(Modularization), 재사용성(Reusable), 가독성(Readability)등을 떨어지는 주요 원인이 돼서, 방지하고 코드를 구조적으로 설계할 수 있게 사용

데이터세트와 데이터로더를 통해 학습에 필요한 데이터 구조를 생성, 일반적으로 데이터세트를 재정의해 가장 많이 사용, 데이터로드에서는 주로 배치 크기를 조절해가며 학습 환경에 맞는 구조로 할당

- 초기화 메서드
    - 파일을 불러와 활용 가능한 형태로 변형하는 과정을 진행
- 호출 메서드
    - 학습을 진행할 대 사용되는 하나의 행을 불러오는 과정, 입력된 색인에 해당하는 데이터 샘플을 불러오고 반환, 초기화 메서드에서 변형되거나 개선된 데이터를 가져오며, 데이터 샘플과 정답을 반환한다.
- 길이 반환 메서드
    - 학습에 사용된 전체 데이터세트의 개수를 반환, 이 메서드로 몇개의 데이터로 학습이 진행되는지 확인 가능

### 데이터로더(DataLoader)

- 데이터세트에 저장된 데이터를 어던 방식으로 불러와 활용할지 정의
    - 배치크기(batch_size) : 1 에폭을 배치 크기로 나눠서 학습
    - 데이터 순서 변경 (shuffle) : 입력과 정답의 관계는 변경되지 않고, 행의 순서를 변경하는 개념
    - 데이터 로드 프로세스 수(num_workers) : 프로세스의 개수를 의미, 데이터 로드에 필요한 프로세스의 수를 늘릴 수 있다.

### 다중 선형 회귀

텐서 데이터세트의 초기값은 *args 형태로 입력받음.

데이터로더 > 모델, 오차 함수, 최적화 선언

지속적으로 검증하고 최적의 매개변수를 찾는 방법으로 모델을 구성해야 함.

## 모델/데이터세트 분리

Moudle 클래스 활용

파이터치의 모델은 인공 신경망 모듈을 활용해 구현

연산을 수행하는 계층을 정의하고, 순방향 연산을 수행한다.

다른 모듈 클래스를 포함할 수 있고 트리구조(Tree Structure)로 중첩가능.

### 모듈 클래스

super 함수로 모듈 클래서의 속성을 초기화.

순방향 메서드(forward) 매개 변수를 활용해 신경망 구조 설계

역방향 메서드(backward) super 함수로 부모 클래스를 초기화해 역방향 연산은 정의하지 않아도 됨.

자동 미분 기능(Autograd) 모델의 매개변수를 역으로 전파해 자동으로 기울기 or 변화도를 계산 별도의 역전파 기능을 구성하지 않아도 된다.

### 비선형회귀

사용자 정의 데이터세트(CustomDataset) : 데이터 세트를 사용자가 원하는 구조로 만들 수 있음

선형 변호나 함수( nn.Linear)의 입력데이터 차원 크기(in_features), 출력 데이터 차원 크기(out_features)

배치크기(batch_size), 데이터 순서 변경(shuffle), 마지막 배치 제거(drop_last)

다중 선형 회귀 학습 방법과 동일한 구조, 차이점은 GPU 연산을 적용해 학습에 사용되는 x, y변수에 to 메서드를 적용하는 점.

### 모델 평가

학습에 사용하지 않은 임의의 데이터로 평가, torch.no_grad 클래스 사용(기울기 계산을 비활성화)

- 메모리 사용량을 줄여 추론에 적합한 상태로 변경

모델을 평가모드로 변경 (eval 메서드) 변경하지 않으면 일관성 없는 추론 결과를 반환

학습모드(train 메서드)

### 데이터세트 분리

모델 평가에 사용할 데이터 세트 미리 분리, 6:2:2 또는 8:1:1 비율 사용

Training Data : 모델 학습에 사용되는 데이터셋

Testing Data : 검증용 데이터를 통해 결정된 성능이 가장 우수한 모델을 최종 테스트하기 위한 목적으로 사용, 학습에는 사용되지 않은 데이터

Validation Date : 검증 데이터, 주로 구조가 다른 모델과 성능 비교를  위해 사용, 계층 및 하이퍼파라미터 차이로 인한 성능비교

데이터 분리시 무작위 분리를 위해 torch.utils.data의 random_split함수를 사용

분리 시 분리 길이와 데이터 총합이 같아야 함

생성자(Generator) : 서브셋에 포함될 무작위 데이터들의 난수 생성 시드를 의미 torch.manual_seed 함수로 랜덤 시드 생성

## 모델 저장 및 불러오기

모델 학습은 오랜 시간이 소요되는 작업으로 결과를 저장 및 불러와 활용할 수 있어야 함

직렬화(Serialize)와 역직렬화(Deserialize)를 통해 객체를 저장및 불러올 수 있음

피클(Pickle)을 활용해 파이썬 객체 구조를 바이너리 프로토콜(Binary Protocols)로 직렬화해 모델에서 사용된 텐서나 매개변수를 저장

모델을 불러 올 때는 저장된 객체 파일을 역질렬화해 현재 프로세스 메모리에 업로드한다.

### 모델 전체 저장/불러오기

학습에 사용된 모델 클래스의 구조와 학습 상태 등을 모두 저장

- 모델의 계층구조, 모델 매개변수 등이 기록, 동일 구조로 구현 가능

학습 결과를 저장하는 모델 인스턴스(model), 경로(path)

모델 학습 상태가 장치(device를 map_location 매개변수로 설정 가능

모델을 불러오는 경우에도 동일한 형태의 클래스가 선언돼 있어야 함.

모델 전체 파일은 갖고 있지만 구조를 알 수 없는 경우에 모델 구조를 출력해 확인 가능

- 모델 구현 시 주의 사항 변수의 명칭(layer)까지 동일한 형태로 구현

### 모델 상태 저장/불러오기

모델의 상태 값만 저장해 불러오는 방법, 모델의 모든 정보를 저장하면 더 많은 저장 곤간이 필요

모델의 매개변수만 저장하여 활용하는 방법

순서가 있는 딕셔너리(orderedDict)형식으로 반환

torch.load 함수를 통해 불러오고 model 인스턴수의 load_state_dict 메서드로 모델 상태를 반영

### 체크포인터 저장/불러오기

학습 과정의 특정 지점마다 저장하는 것을 의미, torch.save활용해 여러 상태를 저장할 수 있음

- 학습을 이어서 진행하기 위한 목적이므로 에폭, 모델 상태(model.state_dict), 최적화 상태(optimizer.state.dict)등은 필수로 포함

## 활성화함수(Activation Function)

은닉층을 활성화 하기 위한 함수, 뉴런의 출력값을 선형에서 비선형으로 변환하는 것

- 네트워크 데이터의 복잡한 패턴을 기반으로 학습하고 결정을 내릴수 있게 제어함

노드마다 전달돼야 하는 정보량이 다름

활성화 함수는 비선형 구조를 가져 역전파 과정에 서 미분값을 통해 학습이 진행될 수 있게 함

입력을 정규화(Normalization)하는 과정

입력 데이터의 값을 정해진 수식에 따라 변환하는 식(Equation)

### 이진분류(Binary classification)

규칙에 따라 입력된 값을 두 그룹으로 분류하는 작업을 의미, 논리 회귀(Logistic Regression) 또는
논리 분류(Logistic Classification)

관측치는 0~1 범위로 예측된 점수를 반환하며, 데이터를 0 또는 1로 분류하기 위해 임곗값을 0.5로 설정

### 시그모이드 함수(Sigmoid Function)

S자형 곡선 모양, 반환값은 0~1 또는 -1~1 범위를 갖는다.

$Sigmoid(x)=\dfrac{1}{1+e^{-x}}$ x의 계수에 따라 곡선의 완만함을 정할 수 있음

계수가 0에 가까워 질 수록 완만해짐

주로 로지스틱 회귀에 사용

- 독립변수 X의 선형 결합을 활용하여 결과를 예측한다.
- 종속변수 Y는 범주형 데이터를 대상으로 계산하기 때문에 해당 데이터의 결과가 특정 분류로 나뉨
- 분류 문제에서도 사용가능

장점

- 유연한 미분값을 가지므로, 입력에 따라 값이 급격하게 변하지 않음
- 출력값의 범위가 0~1 사이로 기울기 폭주(Exploding Gradient) 문제가 발생하지 않음
- 미분식이 단순한 형태를 지님

단점

- 기울기 소실(Vanishing Gradient) 계층이 많아지면서 점점 값이 0에 수렴 됨
- Y값의 0이 아니므로 입력데이터가 항상 양수인 경우라면, 기울기가 양수 또는 음수가 돼 지그재그 형태로 변동하는 문제로 학습 효율성을 감소

### 이진 교차 엔트로피(Binary Cross Entropy)

평균 제곱 오차 함수를 이진 분류에 사용하면 좋은 결과를 얻기 어려움

$\because$ 예측값과 실제값의 값의 차이가 작으면 계산되는 오차의 크가기 작아져 학습을 원할하게 진행하기 어려움

$BCE1=-Y_i\cdot\log(\hat Y_l)$

$BCE2=-(1-Y_i)\cdot\log(1-\hat Y_l)$

$BCE=BCE1+BCE2=-(Y_i\cdot \log( \hat Y_l)+(1-Y_i)\cdot\log({1-\hat Y_i}))$

두 가지 로그 함수를 교차해 오차를 계산

BCE#1은 실제값$(Y_i=1)$이 1일 때 적용, BCE#2은 실젯값$(Y_i=0)$이 0 일때 적용

기존의 평균 제곱 오차 함수는 명확하게 불일치하는 경우에도 높은 손실 값을 반환하지 않음

로그 함수는 로그의 진수가 0에 가까워질수록 무한대로 발산하는 특성이 있음

**불일치하는 비중이 높을수록 높은 손실(Loss) 값을 반환**

한쪽으로는 무한대로 이동하며 다른 한쪽 0에 가까워지기 때문에 기울기가 0이 되는 지점을 찾기 위해 두 가지 로그 함수를 하나로 합쳐 사용

오차를 계산하기 위해 각 손실값의 평균을 반환

기본형 $BCE=-\dfrac{1}{n}\displaystyle\sum^n_{i=1}(Y_i\cdot \log( \hat Y_l)+(1-Y_i)\cdot\log({1-\hat Y_i}))$

### 이진 분류: 파이토치

시퀀셜(Sequential)을 활용해 여러 계층을 하나로 묶음, 순차적으로 실행되고 가독성을 높임

BCELoss로 순전파를 통해 나온 출력값과 실젯값을 비교해 오차를 계산

임곗값인 0.5에 가까워 질수록 결과가 정확하지 않다

### 비선형 활성화 함수(Non-linear Activations Function)

입력이 선형 조합이 아닌 형태로 출력을 생성하는 함수

실제 세계에서 입출력의 관계는 대부분 비선형적인 구조를 갖고 있기 때문에

입출력 간의 관계를 학습하고 더 정확한 예측을 할 수 있고 이를 통해 네트워크가 학습 데이터의 복잡한 패턴과 관계를 학습할 수 있게 지원

### 활성화 함수 종류

- 계단함수(Step Function)
    - 이진 활성화 함수(Binary Activation Function), 퍼셉트론(Perceptrom)에서 최초로 사용한 활성화 함수, 입력값의 합이 임곗값을 넘기면 0, 못 넘기면 1
- 임곗값 함수(Threshold Function)
    - 임계값보다 크면 입력값(x) 그대로 전달, 작으면 특정 값(value)로 변경
    - 선형 함수와 계단함수의 조합
    - 이진 분류 작업을 위해 신경망에서 사용
    - 함수의 기울기를 계산할 수 없으므로 네트워크를 최적화하기 어려워 사용하지 않음
- 시그모이드 함수(Sigmoid Function)
    - 출력 값을 0~1 값으로 매핑, 이준 분류 신경망의 출력 계층에서 활성화 함수로 사용
    - 단순한 형태의 미분 식, 입력값에 따라 출력값이 급력하게 변하지 않음
    - 기울기 소실 문제
    - 주로 출력층에서만 사용
    
    $Sigmoid(x)=\sigma(x)\dfrac{1}{1+e^{-x}}$
    
- 하이퍼볼릭 탄젠트 함수(Hyperbolic Tangent Function)
    - 시그모이드와 유사한 형태, 출력값의 중심은 0, -1~1 범위의 출력값
    - 기울기 소실 문제가 덜함
    
    $Tanh(x)= \dfrac{e^x-e^{-x}}{e^x+e^{-x}}$
    
- ReLU 함수(Rectified Linear Unit Function)
    - 0보다 작거나 같으면 0 반환, 0보다 크면 선형 함수에 값을 대입하는 구조
    - 입력이 양수면 출력값이 제한되지 않아 기울기 소실 문제 해결
    - 수식 또한 간단해 순전파 역전파 과전의 연산이 매우 빠름
    - 음수 경우 항상 0을 반환해 죽은 뉴런(Dead Neuron)이 됨
- LeakyReLU(Leaky Rectified Linear Unit Function)
    - 음수 기울기를 제어해, 죽은 뉴런 현상을 방지하기 위해 사용,
    - 양수의 경우 ReLU와 동일, 음수인 경우 작은 값이라도 출력시켜 기울기를 갱신
- PReLU(Parametic Rectified Linear Unit Function)
    - 형태는 LeakyReLU 함수와 동일, 음수 기울기는 지속해서 값이 변경됨
- ELU(Exponential Linear Unit Function)
    - 지수 함수를 사용해 부드러운 곡선형태, 음수 기울기에서 비선형 구조를 갖음
    - 입력값이 0인 경우에도 출력값이 급변하지 않아, 경사 하강법의 수렴 속도가 비교적 빠름
    - 데이터의 복잡한 패턴 관계를 학습하는 네트워크의 능력
- 소프트맥스 함수(Softmax Function)
    - k번째 클래스에 속할 확률을 계산, 출력층에서 사용, 네트워크의 출력 가능한 클래스에 대한 확률 분포로 매핑, 확률의 합 1
    
    $p_k=\dfrac{e^{zk}}{\sum^n_{i =1} e^{zi}}$
    

## 순전파와 역전파

순전파(Forward Propagation) 입력이 주어지면 신경망의 출력을 계산하는 프로세스

- 입력층부터 출력층까지 차례대로 변수를 계산해 추론, 계층마다 가중치와 편향 계산한 값이 활성화 함수로 전달, 활성화 함수에서 출력값 계산, 손실 함수에 실젯값과 함께 연산해 오차 계산

$\hat y=activation(weight\times x+bias)$

역전파(Back Propagation) 순전파와 반대로 연산, 예측된 출력값과 실제 출력값 사이의 오류를 최소화, 순전파 과정을 토해 나온 오차를 활용해 각 계층의 가중치와 편향을 최적화

**순전파와 역전파는 네트워크가 입력값을 기반으로 예측을 수행하도록 함**

### 순전파 계산

첫 번째 계층의 가중합(Weighted Sum)을 계산, 입력값과 가중치의 곱을 모두 더한 값에 편한을 더한 값을 의미, 가중합을 활성화 함수에 적용

### 오차 계산

실젯값과 예측값으로 오차를 계산, 역전파 과정에서 사용

### 역전파 계산

계층의 역순으로 가중치와 편향을 갱신, 오차가 0에 가깝도록 가중치를 갱신하는 방법

chain rule 연쇄법칙 

학습 : 순전파를 통해 오차를 계산 역전파를 통해 오찻값이 0이 될 수 있게 가중치 갱신

### 갱신 결과 비교

시그모이드 함수는 출력값의 범위가 0~1로 제한하기 때문에 역전파 과정에서 0에 가까운 기울기가 곱해져 변화량이 미미하므로 성능이 떨어진다. 이런 이유로 깊은 모델의 은닉층에서는 시그모이드를 활성화 함수로 사용하지 않는다.

## 퍼셉트론(Perceptron)

인공신경망의 한 종류, 신경 셍포가 신호를 전달하는 구조와 유사한 방식으로 구현, 이진 분류 작업에 사용되는 간단한 모델

TLU(Threshold Logic Unit) 형태를 기반으로, 계단 함수를 적용해 결과를 반환

### 단층 퍼셉트론(Single Layer Perceptron)

하나의 계층을 갖는 모델

한계 : XOR 게이트처럼 하나의 기울기로 표현하기 어려운 구조에 적용이 힘듬

### 다층 퍼셉트론(Multi-Layer Perceptroen, MLP)

단층 퍼셉트론을 여러개 쌓아 은닉층을 생성, 은닉층이 한 개 이상인 퍼셉트론 구조 의미, 은닉층이 
2개 이상 연결한다면 심층 신경망(Deep Neural Network, DNN), 은닉층이 늘수록 복잡한 구조의 문제 해결 가능

하지만 계층이 늘수록 갱신해야 하는 가중치와 편향이 늘어남

### 퍼셉트론 모델 실습

이진 분류 작업에서 사용되는 간단하고 효율적인 모델

데이터의 복잡한 패턴을 학습할 수 없다