##  딥러닝 스터디 1주차
1~155p : 인공지능과 방법론 ~ 파이토치 기초

### 1. 인공지능과 방법론
#### 1-1. 머신러닝 시스템
<details><summary>
1. 지도 학습
</summary>
</details>

<details><summary>
2. 비지도 학습
</summary>

**이상치(Outlier)** : 횡단면 데이터에서 비정상, 밀도가 낮은 지역의 데이터(고립)

**이상(Anomaly)** : 시계열 데이터에서 비정상

**차원 축소**
- 특징 선택 : 유용한 변수만 남기고 불필요한 변수 제거 
- 특징 추출 : 여러 특징을 하나로 압축 → 새로운 특징 생성
</details>

<details><summary>
3. 준지도 학습
</summary>

- 레이블 포함 & 포함 안된 데이터도 같이 학습
- 소량의 레이블로 성능 높은 모델 (데이터는 많지만, 레이블 할당된 데이터가 적을 경우 사용)

1. **평활도** : 레이블이 존재하는 데이터와 유사한 값을 가지면, 동일한 레이블일 가능성이 높다.
2. **저밀도** : 현재 구축에 사용되는 데이터가 이산형 군집을 형성할 가능성이 높다. 👉 밀도가 높은 곳 - 군집 분리 불가능(높지 않은 곳에서 결정 경계 생성)
3. **다양체** : 고차원의 데이터의 차수를 낮게 표현할때 더 쉽게 이해가능한 부분 공간이 존재한다. (초평면에 투영, 정사)


</details>

<details><summary>
4. 강화 학습
</summary>

**구성 요소**
- 환경 : 학습을 진행하는 공간
- 에이전트 : 환경과 상호작용하는 프로그램(Player or Observer)
- 상태 : 에이전트의 상황
- 행동 : 에이전트가 취하는 행동
- 보상 : 에이전트의 행동에 따라 제공되는 것 (2가지 종류 - 양/음)
- 정책 : 에이전트가 보상을 최대화하기 위해 행동하는 알고리즘(반복 학습으로 보상 최대화 하려는 행동)

**마르코프 결정과정**

이산 시간 확률 제어 과정 - 시간에 따른 시스템의 상태 변화

중요한 가정 : 환경이 마르코프 속성(Markov property: 과거 상태들과 현재 상태가 주어진 경우, 미래는 오직 현재 상태에 의해 결정됨)을 가짐

**가치 함수**

정책에 따라 행동할때 얻게 되는 기대 보상

최적의 가치 함수 구현 = 효율적인 정책 구성 가능

👉 강화학습은 에이전트의 시행착오(Trial and error)를 통해 보상을 최대로 할 수 있는 정책을 찾음

여기서 모델은 결과값에 대한 규칙이 아닌, 환경에 대한 가정을 말함 - 상태 전이와 보상 예측

**1. 모델 기반 강화학습(Model -based)**  
에이전트의 행동에 따라 환경의 변화 알 수 있음 → 행동 전 환경 변화를 예상하여 최적의 행동 실행 가능

    장: 적은 양의 데이터로 효율적인 학습

    단 : 모델의 정확한 환경이 중요함

**2. 모델이 없는 강화학습(Model-free)**       
1과 반대의 장단을 가짐

    장: 모델을 구현하기 어려운 상황에도 사용 가능

</details>

#### 1-2. 머신러닝 아키텍쳐
- 머신러닝 시스템의 목적을 달성하기 위한 설계(목적: 시스템을 활용해 우수한 제품 제공)

 **멱등성(Idempotent)** : 연산을 여러 번 적용하더라도 결과가 달라지지 않는 성질

각 단계가 최상의 결과와 유의미한 가치 제공 필요 : 프로세스및 절차를 수립해야 함

머신러닝 시스템이 개선 및 변경되더라도 안정적인 기능을 제공해야 함 → 자동화된 파이프라인 구축 가능

**지속적인 통합(CI; Continuous Integration)** : 빌드 및 테스트 자동화, 동시에 여러면  개발 및 관리 가능

**지속적인 서비스 제공(CD; Continuous Delivery)** : 지속적인 배포를 의미
