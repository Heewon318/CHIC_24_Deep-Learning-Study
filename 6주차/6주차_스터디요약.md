## ❓질문1

🙄 최댓값 풀링이 왜 평균값 풀링보다 성능이 좋지?

### 답변 정리

1 특징 강조: 최댓값 풀링은 중요한 특징들을 강조하는 효과가 있는 반면, 평균값 풀링은 이런 중요한 요소들을 희석해버린다.

2 노이즈 제거: 최댓값 풀링은 작은 값이나 노이즈를 무시하고 중요한 특징만을 강조하기 때문에 잡음에 덜 민감!

## ❓질문2

ResNet을 할때 원래는 이전 시점의 출력값을 현재 출력값이랑 잔차 연결하는데 이전 시점말고 더 과거의 출력값이랑 잔차연결하게 되면 어떻게 돼?

### 답변 정리

1. 정보 결합의 다양성 증가: 더 과거의 출력값을 잔차로 연결하면, 모델이 더 넓은 범위의 정보(더 과거의 특성)를 결합할 수 있게 됩니다. 이는 모델이 더 넓은 문맥에서 정보를 고려하도록 도와줄 수 있습니다.

2. 복잡한 학습 패턴: 레이어 간 연결이 멀어지면 잔차 연결의 효과가 복잡해질 수 있습니다. 이는 모델이 학습하는 패턴이 더 복잡해지는 것을 의미할 수 있으며, 학습이 잘 이루어진다면 더 정교한 특성 추출이 가능할 수 있습니다. 그러나 동시에 모델이 더 학습하기 어려워질 수도 있습니다.

3. 기울기 흐름 변화: 잔차 연결이 더 과거로 이어지면, 기울기가 흐르는 경로가 길어져서 기울기 소실 문제가 다시 발생할 가능성이 높아질 수 있습니다. 이는 ResNet의 주요 장점을 일부 상실하게 만드는 결과로 이어질 수 있습니다.

4. 모델의 해석 어려움 증가: 모델의 구조가 더 복잡해지면서, 어떤 출력이 어떤 입력에 의한 것인지 해석하기가 더 어려워질 수 있습니다. 이는 모델의 해석 가능성을 저하시킬 수 있습니다.
