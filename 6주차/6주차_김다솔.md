# 이미지 분류
1. 단일 클래스 분류
- 출력: true/false
2. 다중 클래스 분류
- 출력: 개/고양이
3. 다중 레이블 분류
- 출력: 개/소파/블라인드 등 요소들을 뽑아낼 수 있음.
### 알고리즘
1. 서포트 벡터 머신
2. KNN
3. decision tree
4. ANN

**5. CNN**
## AlexNet
- 인식 오류율 26% -> 16% 줄임
- 이미지 특징 추출: 합성곱, 최댓값 풀링
- 클래스 분류: 완전 연결 계층
- 순전파 과정: 차원 수 증가, 크기 감소
- 차원 수가 증가할 수록 모델의 표현력 증가, 특징 맵 크기 줄여 연산량 줄임

| 차이점 | LeNet-5 | AlexNet |
| --- | --- | --- |
| 활성화함수 | 로지스틱 시그모이드 (기울기 소실 문제 o) | ReLu (기울기 소실 문제 x) |
| 풀링 방식 | 평균값 풀링 | 최댓값 풀링 |
| 드롭아웃 | O | X |

- 평균값 풀링
![title](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMAgeP%2Fbtq1JAU6zSX%2F6bpqiUk2dbyzDpXNtq4k11%2Fimg.png)   
- 최댓값 풀링
![title](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FJaok0%2Fbtq1JWcvag0%2FOcVGTPPWoUlIhcRZVP3k1k%2Fimg.png)

### 🙄 최댓값 풀링이 왜 평균값 풀링보다 성능이 좋지?

<details><summary>
1 특징 강조: 최댓값 풀링은 중요한 특징들을 강조하는 효과가 있는 반면, 평균값 풀링은 이런 중요한 요소들을 희석해버린다.

2 노이즈 제거: 최댓값 풀링은 작은 값이나 노이즈를 무시하고 중요한 특징만을 강조하기 때문에 잡음에 덜 민감!
</summary></details>

## VGG
- 13개 합성곱 계층 + 3개 완전 연결 계층 = 16개의 계층

더 작은 크기의 필터를 사용해 계산 효율성을 향상시켰지만, 깊은 신경망 문제로 인해 기울기 소실 문제가 발생함.

```
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision import transforms

hyperparams = {
    "batch_size": 4,
    "learning_rate": 0.0001,
    "epochs": 5,
    "transform": transforms.Compose([
        transforms.Resize(256), #이미지 데이터 크기 조절
        transforms.CenterCrop(224), #중앙 자르기 수행
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.48235, 0.45882, 0.40784], 
            std=[1.0/255.0, 1.0/255.0, 1.0/255.0])
    ])
}

train_dataset = ImageFolder("../datasets/pet/train", transform=hyperparams["transform"])
test_dataset = ImageFolder("../datasets/pet/test", transform=hyperparams["transform"])

train_dataloader = DataLoader(
    train_dataset, batch_size=hyperparams["batch_size"], shuffle=True, drop_last=True)
test_dataloader = DataLoader(
    test_dataset, batch_size=hyperparams["batch_size"], shuffle=True, drop_last=True)
```
## ResNet
VGG의 기울기 소실 문제를 해결하기 위해서
잔차 연결, 항등 사항, 잔차 블록 등을 사용
 
1. 잔차 연결 (= 스킵 연결, 단축 연결)
: 입력값이 신경망 계층을 통과한 후 출력값에 더해지는 연결
- 일반적인 딥러닝 신경망에서는 입력과 출력을 직접 연결함. 이렇게 되면 네트워크가 깊어질수록 입출력 간의 거리가 멀어져 정보의 손실 가능성이 높아짐!

### 기울기 저하 문제
정확도: 56개 계층 < 20개 계층
