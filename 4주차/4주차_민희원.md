# Seq2Seq

- input으로 단어의 sequence를 받고, Output으로 단어의 sequence를 내뱉는 모델
- 입력된 sequence를 끝까지 읽고 정보를 압축한 뒤, 압축한 정보를 바탕으로 출력 sequence를 구성
- `Encoder`(입력된 문장을 끝까지 읽고 정보를 압축하는 역할을 하는 모듈)와 `Decoder`(압축된 정보를 바탕으로 출력 sequence를 만드는 역할을 하는 모듈)로 구성
- `context vector`: (압축된 정보)
- Encoder와 Decoder 간에 서로 파라미터 공유는 하지 않음. (서로 다른 모델이기 때문에, Encoder는 입력시퀀스와 관련된 도메인 학습, Decoder는 출력시퀀스와 관련된 도메인 학습.)
- RNN task 중 Machine translation(기계번역)은 many to many에 해당됨.

### 활용 분야

- 기계 번역 (Machine Translation)
- 챗봇
- 내용 요약 (Text Translation)
- Speech to Text
- Image Captioning
- 말의 어투 변경

## Decoder 2가지 학습 방법

![Decoder 학습 방법 차이](https://velog.velcdn.com/images/nkw011/post/b2cf3989-542a-4d8c-96fe-e23ef0969716/image.png)

1. 첫 번째 시점의 출력값을 두 번째 시점의 입력값으로 사용.

학습 속도가 상대적으로 느리지만, 실제 환경과 가까움.

2. 정답 Sequence의 첫 번째 토큰을 두 번째 시점의 입력값으로 사용.

정답을 알려주면서 하는 방법으로 teacher forcing이라고 함.

학습 속도가 빠르지만, 실제 환경과 떨어져 있음.

## RNN Seq2Seq Model 한계점

- RNN 구조의 경우, Encoder에서 Decoder로 넘어갈 때 하나의 hidden vector만 넘길 수 있음.
- 따라서 Encoder의 마지막 hidden state에 input 문장에 대한 모든 정보를 담아서 Decoder에 전달. (문장이 길어지면 모든 정보를 Encoder의 마지막 hidden state에 제대로 담지 못하는 문제 발생.)-> 정보 변질, 손실의 위험 / Encoder의 마지막 hidden state에만 정보 전달을 의존하므로 앞쪽 정보를 거의 저장하지 못함.
- Encoder에서 앞쪽 정보를 저장하지 못하면 Decoder에서 첫 단어부터 제대로 번역 X -> 이후 단어들도 엉망이 될 가능성이 높음.
- Encoder 문장의 많은 정보를 하나의 벡터에만 담아야 해서 나타나는 문제를 Bottleneck problem이라고 함. -> Attention 방식을 통해 RNN Seq2Seq 모델의 한계 해결.
  +) LSTM도 long-term depedency 문제를 완벽히 극복한 것은 아님.

# Seq2Seq Model with Attention

> Decoder의 매 시점마다 관련이 깊은 입력(source) 부분에 초점을 맞추어 예측을 진행. 입력 sequence 중에서 Decoder의 현재 시점과 가장 관련이 깊은 부분을 찾음.

> Decoder의 매 시점마다 관련이 깊은 입력(source) 부분을 찾아내기 위해 Encoder 모든 시점의 hidden state vector 정보를 활용해 유사도를 계산하는 작업(Attention)이 추가됨

Seq2Seq 구조는 한정된 길이의 벡터를 이용해 입력 시퀀스의 모든 정보를 담기 때문에 입력 정보의 유실이 일어남. 이 한계점을 극복한 모델이 `Seq2Seq with Attention`.

## Encoder

1. Sequence 단어 임베딩 벡터가 차례로 입력으로 들어가고, 각 time step 마다 다음으로 hidden state를 전달.
2. 마지막 단어가 들어가는 time step에서 나온 hidden state는 decoder의 시작 hidden state로 전달.

## Attention을 이용해 Output 구하기

1. Decoder에서 현재 시점에 생성된 hidden state vector와 Encoder의 모든 hidden state vector들과 내적하여 유사도를 구함. 이렇게 구하게 된 유사도를 `Attention score`라고 부름.
   ![Attention Decoder 1단계](https://velog.velcdn.com/images/nkw011/post/3737b7a9-cce3-4842-99c9-6ea9c513e861/image.png)

2. `attention score`를 `softmax 함수`를 사용해 합이 1인 확률로 바꿈. 이렇게 만들어진 확률 값들을 `Attention distribution`이라고 한다.
   ![Attention Decoder 2단계](https://velog.velcdn.com/images/nkw011/post/618560cd-859e-4d5c-a2ec-1f66c9f6de93/image.png)

3. 만들어진 확률값(attention distribution)을 가중치로 사용하여 encoder의 hidden state vector들의 가중합을 구함.
   이렇게 구한 가중합 벡터를 `Attention output`이라고 가정. 만들어진 확률값의 합이 1이므로 `가중평균`으로 볼 수도 있다.
   ![Attention Decoder 3단계](https://velog.velcdn.com/images/nkw011/post/7b406a81-ecb0-440e-a634-804fc59adf2b/image.png)

4. Attention output과 Decoder의 현재 시점 `hidden state vector`를 `concatenation` 한다.
5. 이 벡터를 output layer에 통과시켜 현재 시점의 예측값을 구함.
   ![Attention Decoder 4,5단계](https://velog.velcdn.com/images/nkw011/post/fd046577-ec3b-470c-afd6-6961f4c9f6c1/image.png)

1~5단계를 다음 시점에도 계속 반복.

### 장점

1. NMT(Neural Machine Translation)의 성능 올림.

- Decoder가 입력의 특정 부분에 집중해 번역하는 효과가 있기 때문에 번역 성능을 높일 수 있음.

2. Bottleneck problem을 해결

- Attention의 경우, Decoder가 입력의 모든 부분을 직접적으로 볼 수 있기 때문에 정보의 손실이 거의 발생하지 않는다.

3. Vanishing Gradient 현상 해결

- Attention을 이용한 경우 output에서 직접 Encoder의 각 hidden-state vector를 바라보기 때문에, Encoder까지 가는 경로의 길이가 줄어 vanishing gradient 문제를 해소.

4. Attention은 해석 가능성을 제공

- Attention distribution을 이용하면 decoder가 매 시점마다 어떤 부분에 집중하는지 알 수 있꼬 이는 decoder에서 언제 어떤 단어를 학습하는지 보여줌.

# 질문

1. Attention Machanism을 여러 분야에서 활용 가능할 것 같은데 어떤 부분들이 있을까요?

Attention과 관련된 연구로, Attention이라는 단어가 쓰이지는 않았지만 Attention 개념을 제공한 연구(바다나우 어텐션)와 Attention 개념을 깔끔하게 정립한 연구(루옹 어텐션)로 나뉩니다.
