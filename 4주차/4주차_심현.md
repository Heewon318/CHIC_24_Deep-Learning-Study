# seq2seq

RNN의 종류에서 many to many의 구조를 가지고 있습니다.

![](https://blog.kakaocdn.net/dn/bpFLlu/btsHcrb2pck/7lmmBLDxm2OGVY8H1tVCZ0/img.png)

전체적인 seq2seq의 구조는 입력된 문장이 있으면 인코더에 들어가게됩니다.

인코더를 거치게 되면 고정길이의 context vector를 생성하게 됩니다.

이렇게 생성된 context vector는 입력된 문장의 정보를 가지고 있습니다.

context vector는 디코더에 입력이 됩니다.

인코더에서 context vector를 만듭니다.

이렇게 만들어진 context vector는 디코더에 들어가게 됩니다.

&#60;sos>라는 시작 토큰서부터 시작해서 output을 생성합니다.

### Encoder에서의 역할

Encoder 역할은 입력된 텍스트를 숫자 형태 혹은 벡터 형태로 변환하여 저장합니다.

1. 입력 시퀀스를 고정된 길이의 벡터 형태로 변환합니다.
2. 주어진 입력 시퀀스의 각 요소를 순차적으로 처리하고, 이를 임베딩 토큰과 함께 인코더에 들어갑니다.
3. RNN, LSTM, GRU등의 순환 신경망 구조를 사용하여 입력 시퀀스를 고정 길이의 벡터로 변환하는 역할을 수행합니다.
4. 인코더는 입력 시퀀스를 고정된 길이의 벡터로 압축하고, 입력 시퀀스의 의미를 압축합니다.

입력 시퀀스의 각 단어를 순차적으로 처리하면서 각 단계에 hidden state를 업데이터하고 최종적으로 전체 입력 시퀀스를 대표하는 고정 길이의 context vector를 생성합니다.

### Deocder에서의 역할

Decoder의 역할은 Encdoer에서 얻은 context vector를 다른 형태의 텍스트 데이터로 변환합니다.

1. 인코더에서 생성된 문맥 벡터를 기반으로 출력 시퀀스를 생성합니다.
2. 디코더는 초기 상태로 인코더의 문맥 벡터를 받아들이고, 출력 시퀀스의 첫번재 토큰을 생성합니다.
3. 생성된 토큰은 다음 시간 단계에서 디코더의 입력으로 사용합니다. 이러한 과정은 <eos>의 토큰이 생성될 떄 까지 반복합니다.

디코더는 인코더의 출력인 고정 길이의 벡터를 기반으로 원하는 출력 시퀀스를 생성하는 역항을 수행합니다. 디코더 역시 RNN, LSTM, GRU등의 순환 신경망 구조를 사용합니다.

기존 순환 신경망 구조와 다른 점은 context vector를 입력으로 받는다는 것입니다.

디코더의 예측은 일반적으로 소프트맥스 활성화 함수를 통해 확률 분포로 변환되며, 가장 확률이 높은 단어가 선택됩니다.

이렇게 한 문장을 입력하게 되면 context vector에 압축이 되어 번역된 문장이 나옵니다.

# attention

![](https://velog.velcdn.com/images%2Fsujeongim%2Fpost%2F2040c1f1-323a-4fee-9807-d93a74bfce13%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-03-20%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2012.23.20.png)

기본적인 attention의 구조로서 decoder에서의 한개의 출력값 마다 하나씩 계산을 각각합니다.

encoder부분에서 단어들이 입력될때 RNN을 쓴다고 되어있는데 seq2seq처럼 LSTM, GRU도 사용 가능합니다.

attention을 사용할 때 한 문장이 입력이 될 때 토큰화와 임베딩을 거친후 attentino의 encoder 부분에 들어갑니다.

encoder에 들어가게 되면 RNN, LSTM, GRU중 하나의 모델을 사용해 <sos> 토큰을 만듭니다. (그림에선 <START> 부분)

이렇게 <sos> 토큰(decoder의 은닉층)이 만들어 진다면 아까 계산했던 RNN, LSTM, GRU와 attention score를 냅니다.

decoder에서 온 단어가 encoder에서의 각 단어들과의 연관도 / 유사도를 따지는 점수입니다.

decdoer에서의 단어가 encoder에있던 단어와 연관도 / 유사도가 높으면 높은점수를, 낮다면 낮은 점수의 결과를 가져옵니다.

이렇게 연관도 / 유사도를 알게되었다면 softmax 함수를 이용하여 0 ~ 1사이의 값으로 바꿔줍니다.

유사도 / 연관도를 계산했을때 높은점수와 낮은점수간의 큰폭을 줄이기 위해서 사용됐습니다.

이 부분이 그림에서의 attentnion distribution입니다.

softmax함수를 거친다음 value와 곱합니다.

이렇게 곱한값을 다 더해 context vector의 값을 계산합니다.

context vector를 얻은 다음 decoder의 은닉 상태와 concatenate해 이어 붙입니다.

그리고  다시한번 softmax 함수를 취해 다음 단어를 예측합니다.

이렇게 해서 나온 예측단어가 다음 decoder의 은닉층이 되어 문장의 끝을 알리는 토큰인 <eos>가 나올 때 까지 반복합니다.

여기서 decoder의 은닉층 부분이 query가 되고

attention의 입력값이 RNN, LSTM, GRU중 하나의 모델을 사용한 후 value와 key가 생성이 됩니다.

## Teacher Forcing

Teacher Forcing은 Ground Truth를 디코더의 다음 입력으로 넣어주는 기법입니다.

Teacher Forcing을 쓰는 이유: 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다.

![](https://postfiles.pstatic.net/MjAyMDAxMzFfMjE5/MDAxNTgwMzk5NDY3ODYw.ciFO_I524M7SugDcXJZbD0ryZfPwqGEsMJpGix0iAUMg.m2OMy9QHY7NdRxzYX7YXFbPV5v6B97PZD68HZ3GHBgwg.PNG.sooftware/image.png?type=w3840)

위 이미지는 Teacher Forcing이 적용되지 않은 그림입니다.

$t-1$시점의 출력값이 $t$시점의 입력값으로 넘어가게 됩니다.

만약 $t-1$시점의 출력값이 제대로 나왔다면 다음 입력값에 대해서 문제가 없지만, $t-1$시점에서 잘 못 된 값을 출력하게 된다면 이 이후의 출력값에 문제가 생기게 됩니다.

이러한 문제로 인해 학습 초기에 학습 속도 저하의 요인이 됩니다.

이러한 단점을 해결하기 위해 Teacher Forcing 기법을 사용합니다.

![](https://postfiles.pstatic.net/MjAyMDAxMzFfMjMg/MDAxNTgwNDAwMjU0MTE4.I4J-OBg1SdAaSVbW-4le_mb1AKfJC8T80IchLNxEsBAg.5GwkfoPFYFBPB3hdoBVs9zHGIsjgs6cF49KoWfatju8g.PNG.sooftware/image.png?type=w773)

이렇게 Ground Truth값을 넣어줘서 출력이 잘 못 되어도 정답값을 넣어줘서 모델을 훈련 시킵니다.

또한 학습시 더 정환한 예측이 가능하고, 초기 학습 속도를 빠르게 올릴 수 있습니다.

Teacher Forcing은 현재 시점에서의 decoder 출력 부분과 다음 시점의 ground truth를 비교를 합니다.

일반적으로 손실함수를 사용해 손실값을 계산합니다.

이렇게 계산된 손실값을 사용해 파라미터를 업데이트합니다.

역전파 알고리즘을 통해 기울기와 가중치를 계산해 조정합니다.

조정된 기울기와 가중치를 다음 번 학습을 할 때 정답에 가까워지게 됩니다.

이렇게 Teacher Forcing을 하게되면

- 학습이 빠르다.
    
    학습 초기 단계에서는 모델의 예측 성능이 나쁩니다. 때문에 Teacher Forcing을 이용하지 않으면 잘못된 예측 값을 토대로 Hidden State 값이 업데이트되고, 이 때문에 모델의 학습 속도는 더뎌지게 됩니다.
    

이러한 장점이 있지만 단점도 있습니다.

- 노출 편향 문제
    
    추론 (Inference) 과정에서는 Ground Truth를 제공할 수 없어서 모델은 전 단계의 자기 자신의 출력값을 기반으로 예측을 이어가야합니다.
    
    이러한 학습과 추론 단계에서의 차이 (discrepancy) 가 존재하여 모델의 성능과 안정성을 떨어뜨릴 수 있다.
    

attention mechanisem의 Teacher Forcing 또한 seq2seq의 형태와 비슷합니다.

Teacher Forcing을 활용할땐 출력값과 ground truth를 비교를하고 활용하지 않을 땐 $t$시점의 출력값을 다음 시점의 입력값으로 쓰입니다.

Teacher Forcing을 통한 학습도 똑같습니다.

$t$시점의 출력값과 $t+1$시점의 ground truth를 비교해 손실값을 계산해 가중치와 기울기를 조정해 다음 학습때는 더 좋은 결과를 가지고 오도록합니다.