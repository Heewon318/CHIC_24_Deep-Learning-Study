Seq2Seq with Attention

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/31c05d27-d495-4a7c-9077-a76a00f1eac8/Untitled.png)

- 인코더와 디코더 간 파라미터를 공유하지는 않음
- RNN의 타입 : many to many
- 한계(**Bottleneck problem**)
    - RNN의 경우, 인코더에서 hidden vector만 넘길 수 있는데, 문장이 길어지면 문제 발생
    → 마지막 hidden state에만 정보 전달을 의존하게 됨(앞 쪽 정보 거의 저장 못함)
    - 인코더에서 정보를 잘 저장하지 못하면, 디코더에도 문제 발생
    - 어순을 거꾸로 배열해 집어 넣은 시도 → 인코더의 모든 정보를 잘 담아내진 못함(근본적 해결 x)

⇒ Attention 방식을 이용해 한계를 해결함

### 전체 과정 설명

<aside>
📎 디코더의 각 time step마다, Source Sentence(인코더에 들어온 문장)의 특정 부분에 집중해보자.

</aside>

ex) 프랑스어 → 영어 번역

- 인코더 : 기존과 크게 다르지 않음, 차례대로 다음 hidden state에 전달
- 디코더
    1. 인코더 마지막 hidden state가 디코더의 처음으로 전달됨
    2. 만들어진 hidden state 벡터를 가지고 인코더를 다시 봄.(내적 연산, 코사인 유사도와 비례)
    3. 내적값을 softmax 함수에 통과시켜 확률값으로 변경(전체 합 1)
    4. 확률값 = 인코더의 hidden state벡터의 가중치, Attention output vector(encoder hidden vector와 가중치 곱의 합= context vector)
    5. context vector와 디코더의 첫번째 hidden state의 output vector를 concatenate
    → 최종 output vetor 구하기
    6. 현재 time step의 다음 단어 내뱉기(Attention에서는 output vector와 **context vector**를 모두 고려해서 다음 단어 예측)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/b1dfca68-a02a-4813-bfea-0a0c86733a9b/Untitled.png)
    

<aside>
📎 디코더에서 단어를 하나 뱉을 때마다 인코더에 있는 모든 단어를 본다고 이해하기
(디코더의 매 time step마다 인코더의 모든 단어에 대해 내적과 softmax를 취해 context 벡터를 봄)

</aside>

- 유사도 계산 방법(내적 외의 방법→ 학습 가능한 파라미터 존재)
    1. Generalized dot product
    내적과 달리, 두 벡터 사이에 가중치 행렬이 하나 더 추가됨
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/b362c53a-46ec-404c-be4b-afe24b2d7fd2/Untitled.png)
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/c18fb0ef-9427-419e-bc38-1dc4ecbeaa24/Untitled.png)
        
    2. Concatenation
    두 벡터를 concat한 뒤에 행렬 변환하여 score를 구하는 방법
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/bb08d649-b186-4f6b-8bf0-36fbd531035f/Untitled.png)
        

## Attention 보충

### 1. Decoder hidden state 역할

RNN Seq2Seq

1. 해당 time step의 output을 도출
2. 다음 time step으로 현재 정보를 전달

Attention

1. 현재 time step에서 인코더의 단어중 중점으로 둘 가중치 구하기

### 2. Back propagation

Attention Forward propagation

1. 다음 단어 예측(Decoder hidden state → output)
2. 가중치 계산(Decoder hidden state&Encoder hidden state 계산 → output)

Back propagation

- Forward의 역방향 ⇒ 두가지 경우 확인 가능(1. Decoder hidden state vector로 2. Encoder의 각 hidden state vector)

### 3. Teacher Forcing

- Decoder에 들어가는 input을 실제 정답 단어로 넣는 것(이전 출력으로 내보낸 단어가 아니라)
- 원래 Decoder의 방식은 현재를 잘못 예측한 경우, 연쇄적인 문제가 생김
→  따라서 Attention의 학습 과정에서는, 실제 단어를 넣어서 학습 시키고 inference과정에서는 이전과 같은 방식을 사용하는 것 ⇒ Teacher Forcing
- 장점: 빠른 속도의 학습 과정, 안정화 가능
- 단점 : Training과 Inference 단계의 괴리감(Professor Forcing등이 해결 방식으로 제안되고 있음)

## Attention 성능

1. NMT performance (실제로 그 정보를 사용해서 다음 단어를 에측하는 것) - 어떤 부분에 정보를 집중해서 볼 것인지 정함
2. bottleneck problem 해결(디코더가 직접적으로 인코더의 Sequence를 들여다 볼 수 있음)
3. vanishing gradient problem 해결(Attention을 이용해 문장의 앞쪽 단어까지 gradient를 전달할 때, 지름길이 만들어짐)
4. interpretability 제공(설명가능한 부분 추가-디코더가 각 단어를 예측할 때 인코더의 어떤 단어에 더 집중했는지를 볼 수 있음)
ex) 왼쪽 언어를 영어로 번역하는 경우
