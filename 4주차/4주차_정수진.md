## Seq2Seq

1. **기계 번역**
2. 챗봇
3. 내용 요약, Speech to Text, 어투 변경
   
![Untitled (3)](https://github.com/user-attachments/assets/24a2a118-1b6a-41a9-ad1f-a87031843a4b)


인코더와 디코더 내부에 sequence를 처리하는 모델인 RNN이나 LSTM같은 것들이 담겨 있음

디코더의 두번째 시점 입력값 사용 방법

1. 첫번째 시점의 출력값 사용
첫번째에 정상적인 출력값을 만들었다면, 정답이 나올거임
속도가 상대적으로 느리지만, 실제 환경과 가까움
2. 정답 sequence의 첫번째 토큰 사용
teacher forcing - 정답을 알려주면서 하는 방식
속도가 빠르지만, 실제 환경과 떨어져 있음

단점 : 한정된 길이 - 시작지점에 가까운 정보 유실 위험

RNN모델의 구조적 한계(long-term dependency)
<details><summary>long-term dependency

</summary>
RNN 모델은 시간 순서에 따라 데이터를 처리하면서, 각 시점에서의 상태(hidden state)가 이전 시점의 상태에 의존합니다. 이론적으로는, RNN이 긴 시퀀스에서도 앞서 나온 정보를 기억하고 이를 활용할 수 있어야 하지만, 실제로는 그렇지 않습니다.

**기울기 소실(Vanishing Gradient)**: 역전파를 통해 학습이 이루어지는데, 이 과정에서 시퀀스가 길어질수록 초기 단계의 정보는 점차 사라지게 됩니다. 즉, 오래된 정보는 모델이 잘 기억하지 못하게 되어, 장기적인 의존성을 학습하기 어렵습니다.

**기울기 폭발(Exploding Gradient)**: 반대로, 기울기 값이 점점 커져서 숫자가 폭발적으로 증가하는 현상도 발생할 수 있습니다. 이 경우, 학습이 불안정해지며 모델이 제대로 작동하지 않을 수 있습니다.
    
이러한 문제로 인해, RNN은 긴 시퀀스에서 장기적인 패턴을 학습하는 데 어려움을 겪습니다.
    
이러한 한계를 극복하기 위해 LSTM(Long Short-Term Memory)와 GRU(Gated Recurrent Unit)와 같은 구조가 제안되었습니다. 이들은 RNN의 확장된 모델로, 중요한 정보를 더 오래 기억하고 필요하지 않은 정보를 잊는 메커니즘을 통해 장기 의존성 문제를 완화할 수 있습니다.

이와 같은 구조적 한계 때문에, RNN은 상대적으로 짧은 시퀀스 데이터를 처리하는 데 더 적합하며, 장기 의존성을 요구하는 작업에는 LSTM이나 GRU 같은 대안이 더 효과적입니다.
</details>    
<details><summary>LSTM의 장기의존성 문제 측면의 한계
</summary>       
    LSTM은 셀 상태(cell state)와 게이트 메커니즘(입력 게이트, 삭제 게이트, 출력 게이트)을 도입하여 중요한 정보를 선택적으로 기억하거나 잊을 수 있도록 설계되었습니다. 이러한 구조 덕분에 일반적인 RNN에 비해 장기 의존성을 더 잘 학습할 수 있습니다.
    
그러나 LSTM도 다음과 같은 한계를 가지고 있습니다:
    
1. **장기 시퀀스의 한계**: LSTM이 RNN보다 더 긴 시퀀스를 처리할 수 있지만, 시퀀스가 매우 길어질 경우 여전히 초기 정보의 손실이 발생할 수 있습니다. 이로 인해, 아주 긴 시퀀스에서는 여전히 장기 의존성 문제에 직면할 수 있습니다.
2. **계산 복잡도**: LSTM은 복잡한 게이트 구조를 가지므로, RNN보다 계산 비용이 높습니다. 이로 인해 매우 긴 시퀀스를 처리하거나 대규모 데이터셋을 학습할 때는 학습 속도가 느려질 수 있습니다.
3. **학습의 어려움**: LSTM은 RNN보다 더 복잡한 구조로 인해 학습이 더 어려울 수 있습니다. 특히, 하이퍼파라미터 튜닝이 어렵고, 최적화가 복잡할 수 있습니다.
</details>    

LSTM의 한계를 극복하기 위해 **Transformer** 모델과 같은 새로운 접근법이 등장했습니다. Transformer 모델은 RNN 계열 모델과 달리, 모든 입력을 병렬로 처리할 수 있으며, Attention 메커니즘을 통해 긴 시퀀스에서도 중요한 정보를 효과적으로 추적할 수 있습니다. 이로 인해, Transformer는 자연어 처리 분야에서 LSTM을 대체하는 주요 모델로 자리 잡고 있습니다.

따라서 LSTM은 RNN보다 장기 의존성 문제를 잘 해결하지만, 여전히 완벽한 해결책은 아니며, 더 복잡한 시퀀스 처리에는 Transformer와 같은 새로운 방법들이 더 적합합니다.

## Seq2Seq with Attention

![Untitled](https://github.com/user-attachments/assets/96d7426e-fcf8-461a-81d6-f6baee2d5774)

- 인코더와 디코더 간 파라미터를 공유하지는 않음
- RNN의 타입 : many to many
- 한계(**Bottleneck problem**)
    - RNN의 경우, 인코더에서 hidden vector만 넘길 수 있는데, 문장이 길어지면 문제 발생
    → 마지막 hidden state에만 정보 전달을 의존하게 됨(앞 쪽 정보 거의 저장 못함)
    - 인코더에서 정보를 잘 저장하지 못하면, 디코더에도 문제 발생
    - 어순을 거꾸로 배열해 집어 넣은 시도 → 인코더의 모든 정보를 잘 담아내진 못함(근본적 해결 x)

⇒ Attention 방식을 이용해 한계를 해결함

### 전체 과정 설명

📎 디코더의 각 time step마다, Source Sentence(인코더에 들어온 문장)의 특정 부분에 집중해보자.

ex) 프랑스어 → 영어 번역

- 인코더 : 기존과 크게 다르지 않음, 차례대로 다음 hidden state에 전달
- 디코더
    1. 인코더 마지막 hidden state가 디코더의 처음으로 전달됨
    2. 만들어진 hidden state 벡터를 가지고 인코더를 다시 봄.(내적 연산, 코사인 유사도와 비례)
    3. 내적값을 softmax 함수에 통과시켜 확률값으로 변경(전체 합 1)
    4. 확률값 = 인코더의 hidden state벡터의 가중치, Attention output vector(encoder hidden vector와 가중치 곱의 합= context vector)
    5. context vector와 디코더의 첫번째 hidden state의 output vector를 concatenate
    → 최종 output vetor 구하기
    6. 현재 time step의 다음 단어 내뱉기(Attention에서는 output vector와 **context vector**를 모두 고려해서 다음 단어 예측)
    ![Untitled (1)](https://github.com/user-attachments/assets/5c23c3d1-be2b-4b1e-812b-00db66099e65)
    

📎 디코더에서 단어를 하나 뱉을 때마다 인코더에 있는 모든 단어를 본다고 이해하기
(디코더의 매 time step마다 인코더의 모든 단어에 대해 내적과 softmax를 취해 context 벡터를 봄)

<details><summary>유사도 계산 방법(내적 외의 방법→ 학습 가능한 파라미터 존재)

</summary> 

1. Generalized dot product

   내적과 달리, 두 벡터 사이에 가중치 행렬이 하나 더 추가됨
![Untitled (4)](https://github.com/user-attachments/assets/82926d01-466f-483c-aee3-21b54edb789d)
![Untitled](https://github.com/user-attachments/assets/16db477b-4c9a-4b8a-aa67-c8a3c5c91750)

3. Concatenation

   두 벡터를 concat한 뒤에 행렬 변환하여 score를 구하는 방법
   ![Untitled (5)](https://github.com/user-attachments/assets/8f953d43-7d5c-459e-a13b-c8a400ca93d4)

</details>    
        

## Attention 보충

### 1. Decoder hidden state 역할

RNN Seq2Seq

1. 해당 time step의 output을 도출
2. 다음 time step으로 현재 정보를 전달

Attention

1. 현재 time step에서 인코더의 단어중 중점으로 둘 가중치 구하기

### 2. Back propagation

Attention Forward propagation

1. 다음 단어 예측(Decoder hidden state → output)
2. 가중치 계산(Decoder hidden state&Encoder hidden state 계산 → output)

Back propagation

- Forward의 역방향 ⇒ 두가지 경우 확인 가능(1. Decoder hidden state vector로 2. Encoder의 각 hidden state vector)

### 3. Teacher Forcing

- Decoder에 들어가는 input을 실제 정답 단어로 넣는 것(이전 출력으로 내보낸 단어가 아니라)
- 원래 Decoder의 방식은 현재를 잘못 예측한 경우, 연쇄적인 문제가 생김
→  따라서 Attention의 학습 과정에서는, 실제 단어를 넣어서 학습 시키고 inference과정에서는 이전과 같은 방식을 사용하는 것 ⇒ Teacher Forcing
- 장점: 빠른 속도의 학습 과정, 안정화 가능
- 단점 : Training과 Inference 단계의 괴리감(Professor Forcing등이 해결 방식으로 제안되고 있음)

## Attention 성능

1. NMT performance (실제로 그 정보를 사용해서 다음 단어를 에측하는 것) - 어떤 부분에 정보를 집중해서 볼 것인지 정함
2. bottleneck problem 해결(디코더가 직접적으로 인코더의 Sequence를 들여다 볼 수 있음)
3. vanishing gradient problem 해결(Attention을 이용해 문장의 앞쪽 단어까지 gradient를 전달할 때, 지름길이 만들어짐)
4. interpretability 제공(설명가능한 부분 추가-디코더가 각 단어를 예측할 때 인코더의 어떤 단어에 더 집중했는지를 볼 수 있음)
ex) 왼쪽 언어를 영어로 번역하는 경우
![Untitled (2)](https://github.com/user-attachments/assets/fe2d70bd-313a-4c2c-ba36-ca21f5040604)
