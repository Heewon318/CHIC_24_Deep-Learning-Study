## ❓질문1

Attention Machanism을 여러 분야에서 활용 가능할 것 같은데 어떤 부분들이 있을까요?

### 답변 정리

<img width="719" alt="image" src="https://github.com/user-attachments/assets/9df3ef03-4bbe-4755-9d39-35eebe01fe9a">

## ❓질문2

1. attention score와 weight의 차이
2. attention 알고리즘에서 encoder hidden vector와 decoder hidden vector 내적값이 음수 일 때 반의어라고 볼 수 있나?

### 답변 정리

1. score 에 exp()를 해 양수 값으로 모두 만든 후 전체 합으로 나눠줘서 총 합이 1이 되도록 한 게 weight이다.
2. 그렇다.

## ❓질문3

Attention Score와 Attention Weight를 통해 모델의 결정 과정을 해석할 수 있는데, Attention Score가 높은 요소가 실제로 중요한 요소임을 보장할 수 없는 사례가 있다면, 이를 어떻게 설명할 수 있을까? (Attention Score가 높은 요소가 실제로 중요한 요소가 아니라면 어떻게 해석?설명?!)

### 답변 정리

Attention Score가 높은 요소가 실제로 중요한 요소가 아닐 수 있는 이유로는 데이터 및 학습 과정의 편향, 모델의 불완전성, 복잡한 의존성이 있다고 합니다.
이러한 경우, Attention Score와 Attention Weight를 분석할 때, 이를 지나치게 단순하게 해석하는 것을 피해야 하며, 다른 해석 기법과 병행하는 것이 중요합니다. 예를 들어, Shapley 값이나 **Layer-wise Relevance Propagation (LRP)** 와 같은 다른 모델 해석 기법을 사용하여 모델의 결정 과정을 다각도로 분석할 수 있습니다.
도메인 전문가의 관점 반영해보는 것이 좋습니다!

\*`LRP`: 딥러닝 모델의 결과를 역추적해서 입력 이미지에 히트맵을 출력합니다. LRP는 히트맵방식 XAI 기법 중 가장 대표적인 방식이기도 합니다.

[+) LRP 추가 설명](https://velog.io/@sjinu/%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC-LRPLayer-wise-Relevance-Propagation)

## ❓질문4

> attention에서 encoder hidden vector와 decoder hidden vector를 내적해서 내적값을 구하는데, 문득 내적값이 음수인 걸 어떻게 해석해야 하는지에 대해 의문이 들었다고 함(블로그에서)

두 단어가 각각 -1, -2가 나왔다면 -1의 경우보다 -2의 경우가 연관성이 더 떨어진다고 보는 게 맞나?? 두 벡터가 완전히 반대 방향을 보고 있으면 이는 그냥 두 단어와 완전 관계가 없다고 봐야 하는건지, 아니면 반대 의미를 가진다고 해석해도 되는건지 궁금합니다!

### 답변 정리

Attention에서는 유사도의 개념으로 접하는 것이 좋다는 의견 - softmax를 지나면서 연관이 없는 값으로 변함
음수의 경우, 의미가 반대되는 것으로 해석이 될 수도 있다는 의견도 나옴
