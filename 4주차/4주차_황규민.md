# Seq2Seq

- 인코더와 디코더로 구성되고 고정된 Context Vector를 생성
    - Context Vector는 Input(Sentence)을 Incoder로 Embedding 처리를 한 것
    - Decoder로 Context Vector를 입력 받아서 각 단어별로 출력 생성

Context Vector의 뒷 부분을 제대로 학습하지 못하는 문제점이 있음

- 장기 의존성 문제
    - RNN에서 문장이 길어지면서 업데이트 과정에서 초기의 정보가 점점 소실 되는 문제
    - 기울기 소실 문제, 위와 같은 맥락
- 모델의 구조 문제
    - 긴 문장을 고정된 길이의 Vector로 압축하는 과정에서 모든 정보를 표현하기 어려움
    - 위의 문제로 Decoder에서 입력 시퀀스의 전체 맥락을 파악하지 못함

이런 문제를 해결하기 위해 사용되는 기법

- LSTM이나 GRU같은 변형 모델
    - 장기 의존성 문제를 덜 겪음
- Teacher Forcing
    - 모델 학습 시, 이전 출력을 다음 입력으로 사용하는 대신 실제 정답 단어를 입력으로 사용해 학습하는 기법
- Attention mechanism

# Attention

기본적인 구조는 Seq2Seq와 같은 구조

- 인코더와 디코더로 구성
    - 차이점은 디코더에서 Attention Mechanism이 사용됨

Seq2Seq와 같은 방법으로 인코더에서 Sentence를 Embedding 처리 후 hidden state를 디코더의 출력으로 사용

- 여기서부터 Attention Mechanism

디코더에서 현시점의 hidden state와 인코더의 모든 hidden state와의 각각의 유사도(score)를 매김

이 score로 softmax 함수를 사용해   Attention Value(Context Vector)를 구하고 현재 시점의 hidden state와 concate를 해 다음 상태 예측

- 한계점
    - 계산 복잡도 증가(Attention)
        - 모든 hidden state와 유사도를 구해야하고 점수와 가중치를 사용
    - 직렬 처리
        - 병렬처리가 안 추론 속도가 늦음