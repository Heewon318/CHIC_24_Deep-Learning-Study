# Seq2Seq Model
- sequence-to-sequence
- input: 단어의 sequence
- output: 단어의 sequence
- encoder, decoder로 구성되며 서로 파라미터 공유 x
- 주로 번역기에 사용됨.
![title](https://wikidocs.net/images/page/24996/%EC%8B%9C%ED%80%80%EC%8A%A4%ED%88%AC%EC%8B%9C%ED%80%80%EC%8A%A4.PNG)   
![title](https://wikidocs.net/images/page/24996/seq2seq%EB%AA%A8%EB%8D%B811.PNG)   
- 컨텍스트 벡터: 인코더가 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만든 것

입력 문장이 압축된 하나의 컨텍스트 벡터를 디코더로 전송하고 디코더는 이를 하나씩 순차적으로 출력함.

![title](https://wikidocs.net/images/page/24996/%EC%BB%A8%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B2%A1%ED%84%B0.PNG)   
컨텍스트 벡터(4 사이즈)

![title](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)   

seq2seq는 모든 단어를 워드 임베딩 처리를 한 후 계산한다. 

![title](https://wikidocs.net/images/page/24996/%EC%9E%84%EB%B2%A0%EB%94%A9%EB%B2%A1%ED%84%B0.PNG)   

단점
- 하나의 컨텍스트 벡터로 압축하려다보니 정보 손실이 발생함
- RNN의 고질적인 문제인 기울기 소실 문제 존재
- 문장이 길면 번역 품질이 떨어짐
- long-term dependency

---
임수정님의 궁금증
- 두 벡터가 반대 방향이면 반대 의미가 아닐까? 왜냐면 벡터는 방향을 가지고 있기 때문에 방향이 완전 다르다는 의미이기 때문, 뭔가  벡터 내적 값이 0에 가까울 수록 두 벡터의 의미 관계가 없다는 뜻일 것 같다.
---

# Attention
seq2seq의 단점을 극복하기 위해 나옴.
- 인코더에서 전체 입력 문장을 다시 한 번 참고함.
- but, 입력 문장을 같은 중요도로 보는 것이 아니라 예측 단어와 연관도가 높은 단어 위주로 attention해서 봄.

<텐서 기초>
1. W를 통과해도 정보는 보존
: 어떤 정보를 나타내는 텐서
- x -> Wx: x 정보 남음
2. weighted sum
: 정보 혼합
3. 내적
: 정보가 얼마나 비슷?
- 같은 방향일 수록 값이 커짐. : query와 key가 얼마나 유사한가? 
- 내적 결과 음수가 될 수 있음
- 합이 꼭 1이 아님

-> exp()을 사용해서 전부 양수로 만들고 각 weight의 전체합으로 각각의 weight를 나눠서 합을 1로 만듬

query: 검색어 정도..? + 계정 정보 + ..등등

weight가 큰 순서대로 결과를 보여줌.

인도 카레 전문점 vs 인도 주행 금지
- 둘의 의미가 다르다는 것을 알 수 없음..
- 동음이의어 처리 x
