## 과대적합과 과소적합(Over fitting, Under fitting)

과대적합 : 훈련데이터에선 우수하게 예측하지만 새로운 데이터에 제대로 예측하지 못하는 현상

과소적합 : 훈련데이터에서도 성능이 좋지 않다

공통점: **성능 저하**

- 모델 선택 실패
    - 과대, 모델구조 복잡해 훈련 데이터에 의존
    - 과소, 모델구조 단순해 특징 학습을 제대로 못함
- 편향-분산 트레이드오프
    - 모델이 훈련과 새로운데이터에 우수한 성능을 보이려면 낮은 편향과 낮은 분산을 가져야 함.
    - 분산이 높으면 추정치(Estimate)에 대한 변동이 커짐, 데이터가 갖고 있는 노이즈까지 학습 
    과정에 포함하게 되 과대적합 문제 발생
    - 편향이 높으면 추정치가 항상 일정한 값을 갖게 될 확률이 높아져 데이터의 특징을 제대로 학습하지 못함
        - 편향과 분산은 서로 반비례, 모델이 복잡해질수록 분산은 커지고 편향은 작아진다.

### 과대적합과 과소적합 문제 해결

과대적합은 모델의 일반화(Generalizatiion)능력을 저하해 발생, 과서적합은 모델이 데이터 특징을 제대로 학습할 수 없을 때 발생

- 데이터 수집 : 학습 데이터의 수를 늘린다.
- 피처 엔지니어링 : 학습하기 쉬운 형태로 데이터를 변환
- 모델 변경 : 과대 적합 경우 간단한 모델로 과소의 경우 더 복잡한 모델로 변경
- 조기 중단 : 과대 적합이 발생하기 전에 모델 학습을 중단
- 배치 정규화 : 모델의 계층 마다 평균과 분산을 조정해 내부 공변량을 줄여 과대 적합을 방지
- 가중치 초기화 : 적절한 초기 가중치를 설정해 과대적합을 방지
- 정칙화(Regularization) : 목적 함수에 페널티를 부여하는 방법

## 배치 정규화(Batch Normalization)

내부 공변량 변화(INternal Covariate Shift)를 줄여 과대 적합을 방지하는 기술

상위 계층의 매개변수가 갱신될 때 마다 현재 계층에 전달되는 데이터의 분포로 변경

- 일반적으로 입력값을 배치 단위로 나눠 학습을 진행

내부 공변량이란 계층마다 입력 분포가 변경되는 현상을 의미

이 문제를 해결하기 위해 각 계층에 배치 정규화를 적용

- 미니 배치의 입력을 정규화 하는 방식으로 동작
    - 입력이 일반화 되고 독립적으로 정규화가 수행되 더 빠르게 값을 수렴

### 정규화 종류

배치 정규화 : 미니 배치에 계산된 평균 및 분산을 기반으로 계층의 입력을 정규화, CNN,MLP,FNN

계층 정규화(Layer Normalization) : 이미지 데이터 전체를 대상으로 하지 않고 각각의 이미지 데이터에 채널별로 정규화를 수행, RNN,TS

- 채널 축으로 계산하기 때문에 미니배치 샘플간의 의존관계가 없음, 샘플이 서로 다른길이를 가져도 정규화 수행가능

인스턴스 정규화(Instance Normalization): 채널과 샘플을 기준으로 정규화, 입력이 다른 분포를 갖는 작업에 적합, GAN,(Style Transfer모델)

그룹 정규화(Group Normalization): 채널을 N개의 그룹으로나누고 각 그룹내에서 정규화, 배치 정규화의 대안

### 배치 정규화

$y_i=\dfrac{x_i-E[X]}{\sqrt{Var[X]+\epsilon}}*\gamma+\beta$

$E[X]$ : 산술 평균, $Var[X]$ : 분산, $\epsilon$ : 분모가 0이 되는 현상을 방지하는 작은 상수, 
$\gamma,\beta$ : 매개변수 활성화 함수에서 발생하는 스케일(Scale), 시프트(Shift) 값

## 가중치 초기화(Weight Initialzation)

모델의 초기 가중치 값을 설정하는 것, 적절한 초깃값을 설정한다면 기울기 폭주나 기울기 소실문제를 완화할 수 있다.

### 상수 초기화

대표적으로 0, 1, 특정값, 단위행렬, 디랙 델타 함수(Dirac Delta Function)

- 간단하고 계산 비용이 거의 들지 않음, 일반적으로 사용되지 않는 초기화 방법
    - 배열 구조의 가중치에서 문제 발생, 대칭 파괴(Breaking Symmetry)현상으로 모델 학습 어려움

### 무작위 초기화

초기 가중치 값을 무작위 값이나 특정 분포 형태로 초기화 하는 것

대표적으로 무작위, 균등 분포(Unifomr Distribution), 정규 분포(Normal Distribution), 잘린 정규 분포(Truncated Normal Distribution), 희소 정규 분포 초기화(Sparse Normal Distribution Initialization)

- 네트워크가 할습할 수 있고 대칭 파괴 문제를 방지할 수 있음, 간단하고 많이 사용하는 방법
    - 계층이 적거나 하나만 있는 경우에는 보편적, 계층이 많아질수록 기울기 소실 현상 발생
        - 결론적으로 상수 초기화와 같은 문제 발생

### 제이비어 & 글로럿 초기화(Xavier & Glorot Initialization)

균등 분포나 정규 분포를 사용해 가중치를 초기화 하는 방법

확률 분포 초기화 방법과 차이점은 동일한 표준 편차를 사용하지 않고 은닉층의 노드 수에 따라 다른 표준 편차를 할당한다는 점

이전 계층의 노드 수와 다음 계층으 노드 수에 따라 표준 편차가 계산됨

시그모이드, 하이퍼볼릭 탄전트 활성화 함수로 사용되는 네트워크에 효과적

### 카이밍&허 초기화(Kaiming, He)

균등 분포나 정규 분포를 사용해 가중치를 초기화하는 방법, 글로럿과 차이점은 **현재 계층**의 입력 뉴런 수를 기반으로만 가중치를 초기화

각 노드의 출력 분산이 입력 분산과 동일하게 만들어 ReLU 함수의 죽은 뉴런 문제를 최소화,

ReLU함수를 사용하는 네트워크에서 효과적

### 직교 초기화(Orthogoanl)

특잇값 분해(singular value Decomposition, SVD)를 활용해 자신을 제외한 모든 열, 행 벡터들과 
직교이면서 동시에 단위 벡터인 행렬을 만드는 방법, LSTM GRU 같은 순환 신경망(RNN) 주로 사용

직교 행렬의 고윳값의 절댓값은 1이기 때문에 행렬 곱을 여러 번 수행해도 기울기 소실,폭주 문제 방지

가중치 행렬의 고윳값이 1에 가까워지도록 해 RNN에서 기울기가 소실 문제를 방지하는데 사용

- 모델이 특정 초기화 값에 지나치게 민감해지므로 순방향 신경망에는 사용하지 않음

## 정칙화(Regularization)

모델 학습 시 발생하는 과대적합 문제를 방지하기 위해 사용,  손실함수에  규제(Penalty)주는 방법

- 일반화 성능을 향상시킴
- 작은 차이에 덜 민감해져 분산 값이 낮아짐
- 데이터 학습시 읜존하는 특징의 수를 줄여 모델의 추론 능력을 개선

일반화란 새로운 데이터에서도 정확한 예측을 할 수 있음을 의미

### L1 정칙화(L1 Regularization)

라쏘 정규화라고도 함, L1 Norm 방식을 사용해 규제하는 방법, 벡터 또는 행렬값의 절댓값 합계 계산

- 손실함수에 가중치 절대값의 합을 추가해 과대적합을 방지
- 학습은 비용이 0이 되는 방향으로 진행, 손실함수에 값을 추가하면 오차가 더 커짐
    - 모델의 가중치 절댓값의 합도 최소가 되는 방향으로 진행 (작은 값은 0으로 수렴)

$L_1=\lambda*\displaystyle\sum^n_{i=0}\left|w_i\right|$

$\lambda$:규제 강도 0이상의 값: 0에 가까워질수록 모델은 더 많은 특징을 사용

- 모델의 가중치를 0으로 만드는 경우로 희소한 모델이 될 수 있음
- 가중치 절댓값의 합을 사용해 계산 복잡도를 높이게 됨
- 미분할수 없어 역전파 계산에 더 많은 리소스 소모
- 주로 선형 모델에 적용, 선형 회귀모델에 적용해 라쏘회귀

### L2 정칙화(L2 Regularization)

릿지 정규화라고도 함, L2 Norm 방식을 사용해 규제하는 방법, 벡터 또는 행렬 값의 크기를 계산

- 손실함수에 가중치 제곱의 합을 추가해 과대적합을 방지
- L1 과 다른점 하나의 특징이 너무 중요한 요소가 되지 않도록 규제
- 가중치를 0으로 만들지 않고 0에 가깝게 만듬
- 계산 복잡도 문제는 여전히 있음

$L_2=\lambda*\displaystyle\sum^n_{i=0}\left|w^2_i\right|$

정칙화를 적용하는 방법은 손실 함수에 규제 값을 더해주는 방법으로 적용되므로 평균 제곱 오차 값에 정칙화 값을 더한다.

조기 중지(early stop),드롭아웃(drop out)과 같은 기술과 함께 사용, 주로 심층 신경망 모델에서 사용, 선형 회귀 모델에서 사용할 때 릿지회귀라 함

### 가중치 감쇠(Weight Decay)

더 작은 가중치를 갖도록 손실함수에 규제를 가하는 방법, 손실함수에 규제 항을 추가하는 기술 자체를 의미

L2 정칙화를 간단하게 적용하는 방법, 갖고 있는 장점과 단점 그대로 포함

### 모멘텀(Momentum)

경사 하강법 알고리즘의 변형, 이전에 이동했던 방향과 기울기의 크기를 고려해 가중치를 갱신

- 지수 가중 이동평균 사용, 이전 기울기 값의 일부를 현재 기울기 값에 추가해 가중치 갱신
    - 일종의 관성효과를 얻을 수 있음

$v_i=\gamma v_{i-1}+\alpha\nabla f(W_i)$,  $W_{i+1}=W_i-v_i$ $v_i$: 이동벡터

$v_{i-1}$: 이전 모멘텀 값, 감마: 모멘텀 계수 0.0~1.0 주로 0.9 사용

### 엘라스틱 넷(Elastic-Net)

L1과 L2 정칙화를 결합해 사용하는 방식, 희소성과 작은 가중치의 균형을 맞춤

- L1 희박한 가중치를 갖게 규제
- L2큰 가중치를 갖지 않게 규제

두 정칙화 방식의 선형 조합으로 사용하며 혼합 비율을 설정해 가중치를 규제, 혼합 비율은 $\alpha$로 설정

- 0~1 값 사용

$Elastic-Net=\alpha\times L_1+(1-\alpha)\times L_2$

각 장점을 최대한 활용할 수 있지만, 혼합비율 조정을 위해 더 많은 튜닝 또 더 많은 리소스가 필요

### 드롭아웃(Drop out)

모델 훈련과정에서 일부 노드를 일정 비율로 제거하거나 0으로 설정해 과대적합을 방지하는 간단하고 효율적인 방법

과대적합이 발생하는 이유 중 노드 간 동조화현상, 특정 노드에 의존성이 생겨 일부 제거해 억제

- 모델 평균화 효과를 얻기 위해 여러번 훈련해야 해서 시간이 늘어남
- 모든 노드를 사용하지 않기 때문에 데이터세트가 많아야 효과
    - 모든 노드가 균일하게 학습될 수 없음

충분한 데이터세트와 깊은 모델에 적용, 일반적으로 배치 정규화와 동시에 사용하지 않음

- 서로 정칙화 효과를 방해할 수 있음
- 고로 드롭아웃 후 배치 정규화 순으로 적용
    - 둘다 모델 학습할 때만 적용, 추론하는 과정에서는 삭제하지 않고 모든 노드를 사용해 예측

### 그레디언트 클리핑(Gradient Clipping)

기울기가 커지는 현상을 방지하는데 사용되는 기술, 높은 가중치는 높은 분산 값을 갖아 성능 저하

가중치의 최댓값을 규제, 최대 임곗값을 초과하지 않도록 기울기를 잘라 설정한 임곗값으로 변경

$w=\gamma\dfrac{w}{\|w\|}\space\space if:\|w\|>\gamma$

가중치 노름이 최대 임곗값 감마보다 높은 경우에 수행

최대 임곗값을 넘는 경우 기울기 벡터의 방향을 유지하며 기울기를 잘라 규제함 
일반적으로 그레디언트 클리핑은 L2 노름을 사용해 최대 기울기를 규제함

감마값을 하이퍼파라미터로 최대 임곗값 설정, 0.1 or 1과 같은 작은 크기의 임곗값을 적용함

- 학습률을 조절하는 것과 비슷한 효과를 얻을 수 있음

RNN이나 LSTM모델을 학습하는 데 주로 사용, 두 모델은 기울기 폭주에 취약해 유용함

- 가징치 값에 대한 엄격한 제약 조건을 요구, 큰 기울기에 민감한 상황에 유용

매개변수 기울기의 전체 노름 단일 벡터를 반환, 정규화된 기울기는 반환하지 않고 매개변수를 직접 수정함

역전파를 수행한 이우 최적화 함수를 반영하기 전에 호출

- 모델의 매겨변수와 임곗값을 인수로 사용, 임겠값을 초과하는 경우 기울기를 임곘값으로 자르기때문에 구문 사이에 사용

## 데이터 증강 및 변환(Data Augmentation)

데이터가 가진 **고유한 특징을 유지한 채** 변형하거나 노이즈를 추가해 데이터세트의 크기를 인위적으로 늘리는 방법, 과대적합을 줄이고 일반화 능력을 향상시킬수 있음

- 데이터의 형질이 유지되 모델의 분산과 편향을 줄인다
- 데이터 수집 시 잘못된 정보가 들어오는 문제가 발생하지 않는다
- 특정 클래스의 데이터수가 적은 경우 데이터 증강을 통해 데이터 블균형을 완화함
- 기존 데이터가 가진 특징을 파괴하지 않게 사용하는 것이 중요
- 특정 알고리즘을 적용해 생성하므로 데이터 수집보다 더 많은 비용이 들 수도 있음

### 텍스트 데이터

방법: 삽입, 삭제, 교체, 대체, 생성, 반의어, 맞춤법 교정, 역번역 등

자연어 처리 데이터 증강(NLPAUG)라이브러리를 활용, 음성 데이터 증강도 지원

삽입 : 의미 없는 문자나 단어 또는 문장 의미에 영향을 끼치지 않는 수식어 등을 추가하는 방법