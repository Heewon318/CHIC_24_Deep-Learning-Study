# 토큰화(230p ~ 263p) & 임베딩(264p ~354p)

5장, 6장 내용을 읽고 정리한 내용입니다.

# 토큰화

## 자연어처리(NLP, Natural Language Processing)

컴퓨터가 인간의 언어를 이해하고 해석 및 생성하기 위한 기술

### 모델 개발을 위해 해결해야하는 문제

- 모호성 : 인간의 언어는 단어와 구가 사용되는 맥락에 따라 여러 의미를 갖게 됨. 알고리즘은 다양하 의미를 이해하고 명확하게 구분할 수 있어야 함.
- 가변성 : 다양한 사투리, 강세, 신조어, 작문 스타일로 인해 매우 가변적
- 구조 : 문장이나 구의 의미를 이해할 때 구문을 파악하여 의미를 해석함. 문장의 구조와 문법적 요소를 이해하여 의미를 추론하거나 분석할 수 있어야 함.

토큰화를 위해서 토크나이저를 사용함.

**토크나이저** : 텍스트 문자열을 토큰으로 나누는 알고리즘 또는 소프트웨어

어떻게 토큰을 나누었느냐에 따라 시스템의 성능이나 처리 결과가 크게 달라지기도 함.

### 토크나이저 구축 방법

- 공백 분할 : 텍스트를 공백 단위로 분리해서 개별 단어로 토큰화
- 정규표현식 적용 : 정규 표현식으로 특정 패턴을 식별해 텍스트를 분할.
- 어휘 사전 적용 : 사전에 정의된 단어 집합을 토큰으로 사용
- 머신러닝 활용 : 데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝 적용

### OOV

어휘 사전(Vocabulary, Vocab)을 적용하는 방법은 사전에 정의된 단어를 활용해 토크나이저를 구축하는 방법. -> 직접 어휘 사전을 구축하기 때문에 없는 단어나 토큰이 존재할 수 있음.

이러한 토큰을 **Out of Vocab**이라고 함.

- 더 간략하게 설명하자면 Train Data로 만든 단어 사전에 없는 단어가 발생한 것.

### OOV 추가 자료

**+) "단어 사전에 없는 단어"란**

- 학습 데이터에 대해 모든 단어를 토큰화하여 Vocabulary를 만들고, 그 Vocabulary를 기준으로 정수 인코딩을 하게 됨.
- 이때 실 예측(Test) 데이터에 학습(Train) 데이터에 없는 새로운 단어 토큰이 들어올 경우, 이를 OOV 문제라고 함.

> **OOV가 미치는 영향**

> 이전 단어를 기반으로 **다음 단어를 예측하는 task**에서 치명적임. 해결 방안은 BPE(Byte Pair Encording) 알고리즘.

**한국어의 경우**, 띄어쓰기가 제멋대로인 경우가 많으므로, normaluzaiton 없이 바로 subword segmentation을 적용하는 것은 위험함. 따라서 형태소 분석기를 통한 tokenization을 진행한 이후, subword segmentation을 적용하는 것을 권장.

### 큰 어휘 사전 구축 시 발생할 수 있는 문제

- 학습 비용의 중대
- 차원의 저주 (Curse of Dimensionality)

모든 토큰이나 단어를 벡터화하면 어휘 사전에 등장하는 **토큰 개수만큼의 차원이 필요**하고, 벡터값이 거의 모두 0의 값을 가지는 **희소(sparse) 데이터**로 표현됨.

또한 **희소 데이터의 표현 방법**은 어휘 사전에 등장하는 **출현 빈도만 고려**하기 때문에 문장에서 발생한 **토큰들의 순서관계를 잘 표현하지 못함.**

## 단어 토큰화 (Word Tokenization)

자연어 처리 분야에서 핵심적인 전처리 작업 중 하나로 텍스트 데이터를 의미있는 단위인 단어로 분리하는 작업.

`띄어쓰기`(공백, Whitespace), `문장부호`, `대소문자` 등의 특정 구분자를 활용해 토큰화 수행.

### 장점

품사 태깅, 개체명 인식, 기계 번역 등 작업에서 널리 사용되며 가장 일반적인 토큰화 방식.

### 단점

- 말뭉치 내에 'cg.'와 마침표가 없는 다른 'cg'라는 토큰이 있다면, 이 두 토큰은 서로 다른 토큰으로 나뉨.
- 'cg'라는 토큰이 단어 사전 내에 있더라도, 'cg.', 'cg는' 등은 OOV가 됨.
- 한국어 접사, 문장 부호, 오타, 띄어쓰기 오류에 취약함.

## 글자 토큰화 (Character Tokenization)

띄어쓰기뿐만 아니라 글자 단위로 문장을 나누는 방식.

언어 모델링과 같은 시퀀스 예측 작업에서 활용됨. 예를 들어 다음 문자를 예측하는 언어 모델링에서 글자 토큰화는 유용함.

### 장점

- 비교적 작은 단어 사전을 구축할 수 있다는 장점이 있음.
- 작은 단어 사전을 사용하면 학습 시 컴퓨터 자원을 아낄 수 있음.
- 전체 말뭉치를학습할 때 각 단어를 더 자주 학습할 수 있다는 장점이 있음.
- 단어 토큰화의 단점이었던 '도', '는' 같은 접사와 문장 부호의 의미 학습 가능.
- 작은 크기의 단어 사전으로도 OOV를 획기적으로 줄일 수 있음.

하지만, 한글의 경우 하나의 글자는 여러 자음과 모음의 조합으로 이루어져 있고, 자소 단위로 나눠서 **자소 단위 토큰화**를 수행함.

책에서는 자모 라이브러리 활용함.

**`자모 라이브러리`** : 한글 문자 및 자모 작업을 위한 한글 음절 분해 및 합성 라이브러리.

### 단점

- 개별 토큰은 아무런 의미가 없으므로 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야 함.
- 토큰 조합 방식을 사용해 문장 생성이나 개체명 인식 등을 구현할 경우, 다의어나 동음이의어가 많은 도메인에서 구별하는 것이 어려울 수 있음.
- 모델 입력 시퀀스의 길이가 길어질수록 연산량이 증가함.

## 형태소 토큰화 (Morpheme Tokenization)

텍스트를 형태소 단위로 나누는 토큰화 방법으로 언어의 문법과 구조를 고려해 단어를 분리하고 이를 **의미있는 단어로 분류**하는 작업

한국어와 같이 **교착어**인 언어에서 중요하게 수행됨.

**`형태소`** : 실제로 의미를 가지고 있는 최소의 단위

- 자립형태소 : 스스로 의미를 가지고 있음. (명사, 동사, 형용사)
- 의존형태소 : 스스로 의미를 갖지 못하고 다른 형태소와 조합되어 사용됨. (조사, 어미, 접두사, 접미사 등)

### 형태소 어휘 사전

**`형태소 어휘 사전`** 은 각 단어의 형태소 정보를 포함하는 사전을 말함.

텍스트 데이터를 형태소 분석하여 각 형태소에 해당하는 **품사**(Part Of Speech, POS)를 태깅하는 작업을 품사 태깅(POS Tagging)이라고 함.

이를 통해 자연어 처리 분야에서 문맥을 고려할 수 있어 더욱 정확한 분석 가능해짐.

### 형태소 토큰화 라이브러리

- KoNLPy, NLTK, spaCy 등

## 하위 단어 토큰화 (Subword Tokenization)

> 형태소 분석기는 전문 용어나 고유어가 많은 데이터를 처리할 때 약점을 보임. 즉, 형태소 분석기는 모르는 단어를 적절한 단위로 나누는 것에 취약하며, 잠재적으로 어휘 사전의 크기를 크게 만들고 OOV에 대응하기 어렵게 만듦.

**현대 자연어 처리**에서는 **신조어의 발생, 오탈자, 축약어** 등을 고려해야 하기 때문에 **분석할 단어의 양이 많아져 어려움**을 겪음. 이를 해결하기 위한 방법 중 하나로 하위 단어 토큰화가 있음.

### 장점

- 단어의 길이를 줄일 수 있어서 처리 속도가 빨라짐
- OOV 문제, 신조어, 은어, 고유어 등으로 인한 문제 완화

### 하위 단어 토큰화 방법

- 바이트 페어 인코딩, 워드피스, 유니그램 모델 등

### 바이트 페어 인코딩 (BPE)

텍스트 데이터에서 **가장 빈번하게 등장하는 글자 쌍의 조합을 찾아 부호화하는 압축 알고리즘**으로 **초기에는 데이터 압축을 위해 개발됨**. 자연어 처리 분야에서는 하위 단어 토큰화를 위한 방법으로 사용됨.

### 워드피스 (Wordpiece)

워드피스 토크나이저는 바이트 페어 인코딩과 유사한 방법으로 학습되지만, 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합함.

- 학습 과정에서 확률적인 정보를 사용.
- 모델이 새로운 하위 단어를 생성할 때, 이전 하위 단어와 함께 나타날 확률을 계산해 가장 높은 확률을 가진 하위 단어를 선택함.
- BPE 토크나이저와 마찬가지로 위 과정을 반복해 연속된 글자쌍이 더 이상 나타나지 않거나 정해진 어휘 사전 크기에 도달할 때까지 학습함.

**최근 연구 동향**은 더 큰 말뭉치를 사용해 모델을 학습하고 OOV의 위험을 줄이기 위해 **`하위 단어 토큰화`** 를 사용.
