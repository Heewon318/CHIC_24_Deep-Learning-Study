자연어 처리: 사람이 사용하는 언어를 컴퓨터가 이해하고 생성하기 위한 기술

- 인간 언어의 구조, 의미, 맥락을 분석하고 이해할 수 있는 알고리즘을 개발
    - 모호성(Ambiguily): 여러 의미를 간게 되어 모호한 경우를 구분해야함
    - 가변성(Variability): 사투리, 강세, 신조어, 작문 스타일로 가변성을 처리할 수 있어야 하고 이해 해야한다.
    - 구조(Structure): 구문을 파악해 의미를 해석해야함.

위 같은 문제를 이해하고 구분할 수 있는 모델을 만들기 위해 말뭉치(Corpus)를 일정 단위인 토큰(Token)으로 나눈다. 

모델을 훈련하고 평가하는데 사용하는 대규모의 자연어를 말뭉치라 표현

- 일련의 단어의 가능성을 예측하는 알고리즘인 언어 모델을 구축하고 평가하는데 사용

토큰은 말뭉치보다 작은 단위로, 컴퓨터가 자연어를 이해할 수 있게 나누는 과정 이를 토큰화라 함

- 컴퓨터가 텍스트를 효율적으로 분석하고 처리할 수 있도록 하는 중요한 단계, Tokenizer 사용

Tokenizer 구축 방법

- 공백 분할: 텍스트를 공박 단위로 분리해 개별 단어로 토큰화
- 정규표현식 적용: 정규 표현식으로 특정 패턴을 식별해 텍스트를 분할
- 어휘사전 적용: 사전에 정의된 단어 집합을 토큰으로 사용
- 머신러닝 활용: 데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝 적용

어휘사전에 없는 단어나 토큰은 OOV(Out of Vocab)이라 함.

- 큰 어휘사전을 구축하면 학습 비용의 증대 및 차원의 저주 (Curse of Dimensionality) 문제
    - 원-핫 인코딩에서 거의 대부분의 원소가 0인 것과 토큰의 개수만큼 차원이 커짐으로 희소(sparse)데이터로 표현되고 희소 데이터의 표현 방법은 출현 빈도만 고려하기 때문에 순서와 관계에 대해서 표현하지 못함

## 단어 및 글자 토큰화

토큰화 과정은 정확한 분석을 위해 필수, 단어나 문장의 빈도수, 출현 패턴등을 파악할 수 있다.

- 자연어 처리에서 매우 중요한 전처리 과정

### 단어 토큰화(Word Tokenization)

의미 있는 단위인 단어로 분리하는 작업

- 띄어쓰기, 문장 부호, 대소문자 등의 특정 구분자를 활용해 수행
- 품사 태깅, 개체명 인식, 기계 번역 등의 작업에서 널리 사용되고 가장 일반적인 방법

split 메서드로 구분자를 통해 문자열을 리스트 데이터로 나눈다.(구분자가 없을 땐 공백으로)

- 한국어 접사, 문장 부호, 오타 및 띄어쓰기 오류 등에 취약 ex) 최고, 최고! / cg, cg. etc

### 글자 토큰화(Character Tokenization)

비교적 작은 단어 사전 구축하는 장점, 컴퓨터 자원을 아끼고, 말뭉치를 더 자주 학습할 수 있다는 장점

언어 모델링과 같은 시퀀스 예측 작업에서 활용, 리스트 형태로 변환해 쉽게 수행

컴퓨터가 한글을 인코딩하는 방식은 조합형과 완성형으로 나눠짐 (jamo library)

- 조합형: 자모 단위로 나눠 인코딩 후 한글 표현 (j2hcj), 초성, 중성, 종성으로 나뉨
- 완성형: 조합된 글자체에 값을 부여해 인코딩 (h2j)

단어 토큰화에 비해 적은 크기의 단어 사전 구축, OOV를 획기적으로 줄일 수 있음

- 하지만 개별 토큰은 아무론 의미가 없어 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야함

## 형태소 토큰화(Morpheme Tokenization)

텍스트를 형태소 단위로 나누는 토큰화 방법(실제 의미를 갖고 있는 최소 단위를 형태소라 함)

- 자립 형태소(Free Morpheme): 스스로 의미를 가지고 있는
    - 명사, 동사, 형용사
- 의존 형태소(Bound Morpheme): 스스로 의미를 갖지 못하고 다른 형태소와 조합돼 사용하는
    - 조사, 어미, 접두, 접미사 등

### 형태소 어휘 사전(Vocabulary)

자연어 처리에서 사용되는 단어의 집합인 어휘 사전 중에서도 각 단어의 형태소 정보를 포함하는 사전, 어떤 형태의 조합으로 이루어져 있는지에 대한 정보를 담고 있어서 형태소 분석 작업에서 중요한 역활

- 일반적으로 형태소 어휘 사전에는 각 어떤 품사에 속하는지와 해당 품사의 뜻 등의 정보 제공

품사 태깅(POS Tagging) 텍스트 데이터를 형태소 분석해 각 형태소에 해당하는 품사를 태깅

- 자연어 처리 분야에서 문맥을 고려할 수 있어 더 정확한 분석이 가능

ex)그(명사) + 는(조사) + 에게(조사) + 인사(명사) + 를(조사) + 했다(동사)

### KoNLPy(자바 설치 필요)

한국어 자연어 처리를 위해 개발된 라이브러리, 명사 추출, 형태소 분석, 품사 태깅등의 기능 제공

- 자바 개발 키트(JDK) 기반으로 개발되고 Okt, 꼬꼬마, 코모란, 한나눔, 메캅 등의 다양한 형태소 분석기를 지원

### NLTK(Natural Language Toolkit)

자연어 처리를 위해 개발된 라이브러리, 토큰화, 형태소 분석, 구문 분석, 개체명 인식, 감성 분석 등의 기능 제공

### spaCy

Cython 기반으로 개발된 오픈 소스 라이브러리, NLTK와 차이점은 빠른 속도와 높은 정확도가 목표

- 효율적인 처리 속도와 높은 정확도를 제공

사전 학습된 모델을 기반으로 처리, 객체 지향적으로 구현돼 처리 결과를 doc 객체에 저장

doc 객체는 다시 여러 token 객체로 이뤄지고, token 객체에 대한 정보를 기반으로 다양한 자연어 처리

```powershell
python -m spacy download en_core_web_sm
```

띄어쓰기와 맞춤법이 잘 지켜지지 않는 경우 완벽히 대응하기 어렵다.

주요 키워드를 추출하거나 문장의 긍정부정 여부를 판단하는 언어 모델의 성능을 높이는데 중요

## 하위 단어 토큰화(Subword Tokenization)

형태소 분석은 중요한 처리 과정 중 하나

신조어, 오탈자, 축약어 등을 해결하기 위한 방법 중 하나

- 처리 속도도 빨라지고 OOV문제, 신조어, 은어, 고유어 등으로 인한 문제 완화

### 바이트 페어 인코딩(Byte Pair Encoding,BPE)

다이그램 코딩이라고도 하며 하위 단어 토큰화의 한 종류

데이터 압축을 위해 개발됐었음.

정해진 어휘 사전 크기에 도달할 때까지 조합탐지와 부호화를 반복해 이 과정에서 자주 등장하는 단어는 하나의 토큰으로 덜 등장하는 단어는 여러 토큰으로 조합된다

### 워드피스

BPE와 유사한 방법으로 학습하지만 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합

이전 하위 단어와 함께 나타날 확률을 계산해 가장 높은 확률을 가진 하위 단어 선택

Byte-level Byte-Pair-Encoding, BBPE: 바이트 단위에서 토큰화

유니그램(Unigram): 큰 어휘사전에서 덜 필요한 토큰을 제거하며 학습하는 방법

---

실습 중 폴더주소를 잘 맞춰줘야함
---
텍스트를 숫자로 변환하는 텍스트 벡터화(Text Vectorization)

- 원-핫 인코딩(One-Hot Encoding):문서에 등장하는 각 단어를 고유한 색인 값으로 매핑한 후, 해당 위치를 1 나머지는 0으로 표시
- 빈도 벡터화(Count Vecorization): 문서에서 단어의 빈도수를 세 벡터로 표현하는 방법

텍스트 벡터화는 단어나 문장을 벡터 형태로 변환하기 쉽고 간단하다는 장점이 있지만, 벡터의 희소성(Sparsity)이 크다는 단점이 있음

- 입력 텍스트의 의미를 내포하지 않아 문장의 의미가 유사도를 비교할 수 없음

이런 문제를 해결하기 위해 Word2Vec, fastText같은 단어의 의미를 학습해 표현하는 워드 임베딩 사용

- 단어를 고정된 길이의 실수 벡터로 표현하는 방법, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 단어 간의 관계를 추론함.
- 단점으로 고정된 임베딩을 학습하기 때문에 다의어나 문맥 정보를 다루기 어려움
    - 인공 신경망을 활용한 동적 임베딩(Dynamic Embedding)기법 사용

# 언어 모델(Language Model)

입력된 문장으로 각 문장을 생성할 수 있는 확률을 계산하는 모델

- 주어진 문장을 바탕으로 문맥을 이해하고, 문장 구성에 대한 예측을 수행
    - 자동 번역, 음성 인식, 텍스트 요약 등 다양한 자연어 처리 분야에서 활용

주어진 문장 뒤에 나올 수 있는 문장은 매우 다양해 문장 단위로 확률을 계산하는 것은 어려운 일

- 하나의 토큰 단위로 예측하는 방법인 자기회귀 모델이 고안됨

## 자기회귀 언어 모델(Auoregressive Language Model)

입력된 문장들의 조건부 확률을 이용해 다음에 올 단어 예측

이전에 등장한 모든 토큰의 정보를 고려해, 문장의 문맥 정보를 파악해 다음 단어를 생성하고 다음 단어는 다시 이전 단어를 기반으로 예측하는 과정이 반복

조건부 확률의 연쇄법칙(Chain rule for conditional probability)

: 하나의 확률이 일어날 확률을 다른 사건의 조건부 확률을 이용해 계산하는 방식, 이전 단어들의 시퀀스가 주어졌을 때  다음에 등장하는 단어의 확률을 이전 단어들의 조건부 확률을 이용해 계산

각 시점에서 다음에 올 토큰을 예측하는 것이 중요함, 출력값이 입력값으로 사용되는 특징으로 자기회귀라 함

- 시점 별로 다음에 올 토큰을 예측하는 것이므로, 토큰 분류 문제로 정의할 수 있음

## 통계적 언어모델(Statistical Language Model)

통계적 구조를 이용해 문장이나 단어의 시퀀스를 생성또는 분석함, 시퀀스에 대한 확률 분포를 추정해 문장의 문맥을 파악해 등장할 단어의 확률을 예측

일반적으로 마르코프 체인(Markov Chain) 모델을 이용해 구현한다.

- 빈도 기반의 조건부 확률 모델 중 하나, 이전 상태와 현재 상태간의 전이 확률을 이용해 다음 상태를 예측함
- 주어진 데이터에서 각 변수가 발생한 **빈도수를 기반**으로 확률을 계산, 단어의 순서와 빈도로

데이터 희소성(Data Sparsity)관측한 적 없는 데이터를 예측하지 못하는 문제

하지만 기존에 학습한 텍스트 데이터에서 패턴을 찾아 확률 분포를 생성하므로, 이를 이용해 새로운 문장을 생성할 수 있고 다양한 종류의 텍스트 데이터를 학습할 수 있다.

GPT(Generative Pre-trained Transformer)

Bert(Bidirectional Encoder Representations from transformers)

## N-gram

N개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정, 가장 기초적인 통계적 언어 모델

입력 텍스트를 하나의 토큰 단위로 분석하지 않고 N개의 토큰을 묶어서 분석

모든 토큰을 사용하지 않고 N-1개의 토큰만 고려해 확률을 계산

작은 규모의 데이터세트에서 연속된 문자열 패턴을 분석하는 데 큰 효과, 관용적 표현 분석에도 활용

자주 등장하는 연속된 단어 또는 구를 추출하고, 이를 분석해 관용적 표현을 파악할 수 있음

단어의 순서가 중요한 자연어 처리 작업 및 문자열 패턴 분석에 활용

## TF-IDF(Term Fequency-Inverse Document Frequency)

텍스트 문서에서 특정 단어의 중요도를 계산하는 방법, 단어의 중요도를 평가할 때 통계적인 가중치를 의미 BoW(Bag-of-Words)

Bow: 문서나 문장을 단어의 집합으로 표현하는 방법, 중복을 허용해 빈도를 기록, 모든 단어는 동일할 가중치를 갖음. 따라서 자주 등장하지 않는 단어를 토큰화나 분류 모델 진행 시 성능이 낮을 수 있음 

### 단어 빈도(Term Frequency, TF)

특정 단어의 빈도수를 나타내는 값, 값이 높을 수록 중요한 수도 있지만 전문 용어나 관용어로 간주할 수 있다. 단순히 단어의 등장 수를 계산하므로 문서가 길면 TF값도 높아질 수 있음( 중요도는 낮음)

### 문서 빈도(Document Frequency,DF)

한 단어가 얼마나 많은 문서에서 나타는지를 의미, 특저 단어가 많은 문서에 나타나면 문서 집합에서 단어가 나타나는 횟수를 계산함

몇 개의 문서에서 등장하는지 계산, 일반적으로 널리 사용되거나 중요도가 낮을 수 있음

반대로 DF값이 낮은 단어는 적은 수의 문서에만 등장한다는 뜻으로 중요도가 높을 수 있다

### 역문서 빈도(Inverse Document Frequency, IDF)

전체 문서 수를 문서 빈도로 나누고 로그를 취한 값을 말한다, 문서 내에서 특정 단어가 얼마나 중요한지를 나타냄

문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는 의미를 갖는데

문서 빈도의 역수를 취하면 단어의 빈도수가 적을수록 IDF값이 커지게 보정하는 역활을 함

이를 통해 특정 단어의 등장횟수가 적으면 IDF는 상대적으로 커진다.(중요도가 높다)

- 한번도 등장하지 않는 단어의 경우가 있어 수식에서 분모에 1을 더한다
- 너무 큰 값이 나올수 있기 때문에 로그를 취함

### TF-IDF

문서 빈도와 역문서 빈도를 곱한 값을 사용

문서 내에 단어가 자주 등장하지만, 전체 문서 내에 해당 단어가 적게 등장한다면 TF-IDF값은 커짐

전체 문서에서 자주 등장할 확률이 높은 관사나 관용어 등의 가중치는 낮음

의미: 중요한 문서의 핵심 단어?

TF-IDF클래스를 이용해 문서마다 중요한 단어만 추출할 수 있고 벡터값을 활용해 문서 내 핵심 단어를 추출할 수 있다

- 문제점은 문장의 순서나 문맥을 고려하지 않음, 문장 생성과 같이 순서가 중요한 작업 부적합
- 문서내 중요도를 의미할 뿐, 벡터가 단어의 의미를 담고 있지는 않음

# Word2Vec

구글에서 분포 가설(distributional hypothesis)를 기반으로 개발

- 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정
- 단어 간의 동시 발생(co-occurrence)확률 분포를 이용해 단어간의 유사성을 측정함
    - ex)내일 자동차를 타고 부산에 간다, 내일 비행기를 타고 부산에 간다

단어의 분산 표현(Distributed Representation)을 학습, 단어를 고차원 벡터 공간상 위치를 갖게 됨

분포 가설에 따라 단어의 의미는 문맥상 분포적 트겅을 통해 나타냄, 즉 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공간상 위치를 갖게 된다. 자동차, 비행기는 벡터 공간에서 서로 가까운 위치에 표현

- 단어의 의미 정보를 저장하고 단어간의 관계를 파악하고 유사한 단어를 군집해 의미 정보를 표현

다운 스트림 작업에서 더 뛰어난 성능을 보여줌

# 단어 벡터화

- 희소 표현(sparse representation): 원-핫 인코딩, TF-IDF 등의 빈도 기반 방법
    - 대부분의 벡터 요소가 0으로 표현, 공간적 낭비 발생
- 밀집 표현(dense representation): Word2Vec, 밀집표현을 위해 CBoW, skip-gram 사용
    - 고정된 크기의 실수 벡터로 표현, 단어사전의 크기가 커져도 벡터의 크기가 커지진 않음
        - 벡터 공간상 단어 간의 거리 효과적으로 계산, 효율적인 공간 활용

## CBoW(Continuous Bag of Words)

주변 단어로 중간 단어를 예측, Window(범위)를 설정해야하고 Sliding Window로 이동해가며 학습

- 한번의 학습으로 여러 개의 중심 단어와 주변 단어를 학습
- 입력 단어의 원핫벡터를 입력값으로 받고, 입력 문장 내 모든 단어의 임베딩 벡터 평균으로 예측

## Skip-gram

중심 단어를 입력 받아 주변 단어를 예측, Sliding Window 학습

- CBoW 학습 데이터 구성 방식에 차이, 주변 단어와 중심 단어를 쌍으로 여러 학습 데이터 생성
    - 더 많은 학습 데이터세트를 추출해, 일반적으로 CBoW보다 뛰어난 성능
    - 드믈게 등장하는 단어 더 잘 학습, 벡터 공간에서 더 유의미한 거리 관계 형성

두 방법 모두 소프트맥스를 사용하는데 모든 단어를 대상으로 내적 연산을 수행하기에 말 뭉치가 커지면 단어 사전의 크기고 커지므로 학습 속도가 느려짐

- 이를 해결하기 위해 계층적 소프트맥스와 네커티브 샘플링 기법을 적용해 문제를 완화

## 계층적 소프트맥스(Hierachical Softmax)

출력 층을 이진 트리 구조로 표현, 자주 등장하면 상위 노드, 드믈게 등장하면 하위노드

- 일반적인 소프트맥스보다 빠른 속도와 효율성이 있음
- 각 노드는 학습이 가능한 벡터를 갖음
- 입력값은 해당 노드의 벡터와 내적값을 시그모이드 함수를 통해 확률 계산

잎 노드(Leaf Node) 가장 깊은 노드로, 각 단어를 의미하고 모델은 각 노드의 벡터를 최적화해 단어를 잘 예측할수 있게 한다. 각 단어의 확률은 경로 노드의 확률을 곱해 구함

### 네거티브 샘플링(Negative Sampling)

Word2Vec 모델에서 사용되는 확률적인 샘플링 기법으로 전체 단어 집합에서 일부 단어를 샘플링해 오답 단어로 사용

학습 윈도 내에 등장하지 않는 단어 n개를 추출해 정답 단어와 함께 소프트맥스 연산을 수행

- 전체 단어의 확률을 계산할 필요 없이 모델을 효율적으로 학습할 수 있음

입력 단어 쌍이 데이터로부터 추출된 단어 쌍인지, 네거티브 샘플링으로 생성된 단어 쌍인지 이진 분류 한다, 이를 위해 로지스틱 회귀 모델을 사용하고, 이 모델의 학습 과정에서 추출할 단어의 확률 분포를 구하기 위해 먼저 각 단어에 대한 가중치를 학습한다.

### Gensim

- 라이브러리

대용량 텍스트 데이터의 처리를 위한 메모리 효율적인 방법을 제공해 대규모 데이터 세트에서 효과적으로 모델 학습

---

# fastText

단어와 문장을 벡터로 변환하는 기술을 기반, 머신러닝 알고리즘이 텍스트 데이터를 분석하고 이해

- Word2Vec과 다른점은 하위 단어를 고려해 더 높은 정확도와 성능을 제공
- 특수 기호(<,>)로 단어의 시작과 끝을 나타냄, 하위 문자열을 고려하는데 중요
- 하위 단어 집합이 만들어지면 각 단어는 고유 벡터값을 갖음

하위 단어 벡터들은 단어의 벡터 표현을 구성하고, 이를 사용해 자연어 처리 작업을 수행

- 말뭉치에 등장하지 않은 단어라도 유사한 하위 단어를 갖고 있으면 유사한 임베딩 벡터를 갖음

OOV 단어를 대상으로도 의미 있는 임베딩 추출 가능

---

# 순환 신경망(Recurrent Neural Network, RNN)

순서가 있는 연속적인 데이터(Sequence data)를 처리하는 데 적합한 구조, 각 시점(Time step)의 데이터가 이전 시점의 데이터와 독립적이지 않다는 특성 때문에 효과적으로 작동

- 자연어는 연속형 데이터 특성을 갖음, 긴 문장일수록 단어들 사이에 강한 상하관계(Correlation)가 존재

연속적인 데이터를 처리하기 위해 개발된 인공 신경망의 한 종류

시계열 데이터, 자연어 처리, 음석 인식 및 기타 시퀀스 데이터와 같은 도메인

- 데이터 길이가 가변적이고 순서에 따라 의미가 있기 때문에 이런 데이터를 처리하는데 적합

연속형 데이터를 순서대로 입력받아 처리하며 각 시점마다 은닉 상태의 형태로 저장

은닉 상태와 출력값을 계산하는 노드를 순환신경망의 셀(Cell)이라 함.

- 일대다 구조(One-to-Many) 하나의 입력 시퀀스에 대해 여러 개의 출력값을 생성하는 순환 신경망
    - 문장을 입력으로 받고 각 단어의 품사를 예측하는 작업이 가능
        - 입력 시퀀스는 문장, 출력시퀀스는 각 단어의 품사
        - 이미지 데이터로는 입력은 이미지, 출력은 이미지의 설명을 출력하는 이미지 캡셔닝
    - 일대다 구조를 구현하기 위해서는 출력 시퀀스의 길이를 미리 알고 있어야 함

- 다대일 구조(Many-to-One) 여러 개의 입력 시퀀스에 대해 하나의 출력값을 생성하는 순환 신경망
    - 감성 분류(Sentiment Analysis) 분야에서 특정 문장의 감정을 예측하는 작업을 할 수 있음
        - 입력 시퀀스는 문장, 출력값은 해당 문장의 감정(긍정,부정)
        - 두 문장 간의 관계를 추론하는 자연어 추론(Natural Language Inference)에 적용 가능

- 다대다 구조(Many-to-Many) 입출력 시퀀스의 길이가 여러 개인 경우 사용하는 순환 신경망
    - 다양한 분야에 활용
    - 입출력 시퀀스의 길이가 서로 다른 경우도 있음, 이럴땐 길이를 맞추기 위해 패딩 또는 잘라내는 등의 전처리 과정이 수행됨
    - Seq2Seq 구조로 이뤄짐 > 인코더와 디코더로 구성
        - 인코더는 입력 시퀀스를 고정 크기의 벡터로 출력, 디코더는 이 벡터를 입력 받아 출력 시퀀스 생성

- 양방향 순환 신경망(Bidirectional Recurrent Neural Network, BiRNN) 기본에 시간 방향을 양방향으로 처리할 수 있게 설계
    - 현재 상태를 이전 상태와 이후 상태도 함께 이용
        - 입력 데이터를 순방향으로 처리하는 것만 아니라 역방향으로 처리하는 방식이 이뤄짐
        
- 다중 순환 신경망(Stacked Recurrent Neural Network) 여러개의 순환 신경망을 연결해, 각 순환 신경망이 서로 다른 정보를 처리하도록 설계
    - 여러개의 순환 신경망 층으로 구성, 각 층의 출력값은 다음 층에 전달되 처리

---

# 장단기 메모리(Lon Short-Term Memory, LSTM)

순환 신경망이 갖고 있던 기억렬 부족과 기울기 소실 문제를 해결

- 학습 데이터가 커질수록 전달 되지 않고, 하이퍼볼릭 탄젠트 함수나 ReLU함수 특성으로 역전파 과정에서 기울기 소실 또는 폭주 문제 가능성

장단기 메모리: 메모리 셀과 게이트라는 구조로 문제해결

- 셀 상태(Cell state), 망각 게이트(Forget gate), 기억 게이트(Input gate),출력 게이트(Output gate)로 정보의 흐름 제어
    - 셀 상태: 정보를 저장 및 유지하는 메모리 역활, 출력 게이트와 망각 게이트에 의해 제어
    - 망각 게이트: 정보 삭제 결정하는 역활, 현재와 이전 상태를 입력으로 받아 어떤 정보를 삭제할지 결정
    - 입력 게이트: 새로운 정보를 어떤 부분에 추가할지 결정, 현재와 이전 셀 상태를 입력으로 받아 결정
    - 출력 게이트:  셀 상태의 정보 중 어떤 부분을 출력할지 결정, 현재와 이전 셀 상태 그리고 새로 추가된 정보를 입력으로 받아 결정

메모리 셀: 출력값 계산에 직접 사용되지 않음. 게이트의 결정에 추가적인 연산 수행

세 가지 게이트 모두  활성화 함수로 시그모이드 사용

기억 게이트: 시그모이드(중요도)를 거치고 하이퍼볼릭 탄젠트 함수를 거친 값의 곱으로 새로운 기억 값 계산

- 어떤 정보를 얼마나 추가할지 결정

---

# 합성곱 신경망(Convolutional Neural Network, CNN)

이미지 인식과 같은 컴퓨터비전 분야의 데이터를 분석하기 위해 사용되는 인공 신경망의 한 종류

- 지역적인 특징을 추출하는데 특화된 구조를 갖고 있고 이를 위해 합성곱(Convolution) 연산 사용

합성곱 연산: 이미지의 특정 영역에서 입력값의 분포 또는 변화량을 계산해 출력 노드를 생성

- 지역 특징(Local Features)을 효과적으로 추출

자연어 처리 작업에서도 우수한 성능

순환 신경망은 연산 순서의 제약으로 병렬 처리가 어렵다는 단점

## 합성곱 계층

입력 데이터와 필터를 합성곱해 출력 데이터를 생성하는 계층

- 이미지나 음성 데이터와 같은 고차원 데이터를 처리하는데 주로 사용
    - 필터를 사용해 지역적인 패턴을 인식할 수 있고 모든 위치에서 동일한 필터를 사용해 모델 매개변수를 공유

모델의 **매개변수를 공유**해 과대적합을 방지하고 특징 추출 시 다른 위치에 존재해도 필터로 인해 동일하게 추출 가능, 계층이 많을수록 복잡도가 올라가고 더 다양한 특징을 추출할 수 있다.

필터(Filter)

커널(Kernel) 또는 Window로 불리기도 함, 일정 간경을 이동하며 합성곱 연산을 수행해 특징 맵 생성

- 이 과정에서 필터의 가중치가 모델 학습 과정에서 갱신 됨
- 보통 여러 개의 필터를 사용해 다양한 특징을 추출하고, 이를 통해 입력 이미지의 다양한 특징을 인식하고 분류함.

패딩(Padding)

합성곱 신경망이 더 깊게 쌓는데 제약이 되는 것과 이미지의 가장자리에 있는 정보는 학습하기 어렵다는 문제를 방지하기 위해 사용, 가장자리에 0값을 할당(Zero padding)

간격(Stride)

필터가 한 번에 움직이는 크기를 의미, 출력 데이터의 크기를 조절할 수도 있다.

- 공간적 정보를 유지하거나 감소시킬 수 있음.

채널(Channel)

입력 데이터와 필터가 3차원으로 구성되어 있을 때 같은 위치의 값끼리 연산함, 입력데이터의 공간적 정보를 유지하면서 추출되는 특징을 확장

채널의 개수가 많아질수록 학습할 수 있는 특징의 다양성이 증가해 모델의 표현력이 높아지는 효과

- 모델의 매개변수가 많아지므로 학습 시간과 메모리 사용량이 증가하는 단점, 과대적합의 위험

팽창(Dilation)

합성곱 연산 수행 시 입력 데이터의 더 넓은 범위의 영역을 고려하는 기법, 필터와 입력 데이터 사이에 간격을 두는 방법

일반적으로는 1로 사용, 값이 커질수록 필터가 입력 데이터를 바라보는 범위가 넓어짐,

입력 데이터의 각 픽셀이 출력에 미치는 영향이 강화되는 효과가 있음
- 적절히 사용하면 모델의 매개변수를 줄여 메모리 사용량을 줄일 수 있지만, 너무 큰 값을 사용하면 인접한 픽셀값을 고려하지 않게 되므로 공간적인 정보가 보존되지 않아 특징 추출 효과가 감소

### 활성화 맵(Activation Map)

특징 맵에 활성화 함수를 적용해 얻어진 출력값을 의미

- 일반 적으로 합성곱 신경망에는 ReLU 함수가 적용

다음 계층의 입력값으로 사용되고 여러 번 반복해 신경망을 구성하면, 입력 이미지에서 추출된 추상적인 특징을 학습하게 됨

- 활성화 함수를 적용하지 않으면 선형적인 결합만 수행돼 복잡한 패턴이나 특징을 학습하는 것이 어려워짐.
- 합성곱 모델을 분석하는 데 매우 중요한 정보 제공, 시각화해 어떤 특징을 학습하는지 이해 가능(XAI)

### 풀링(Pooling)

특징 맵의 크기를 줄이는 연산, 연산량을 감소시키고 입력 데이터의 정보를 압축하는 효과를 갖음

합성곱 연산과 비슷하게 필터와 간격을 이용

- Max Pooling: 필터 내 원솟값 중 가장 큰 값을 선택
- Average Pooling: 필터 내 원솟값의 평균값

입력 데이터의 특징 위치가 변경되더라도 인근 영역에 대한 연산을 적용하기 때문에 공간적 정보 유지

세밀한 위치정보가 필요한 작업에는 성능 저하를 초래할 수 있다.

최근엔 공간적 크기를 감소하는 방법보다 연산량이 많더라도 합성곱 계층의 간견을 설정해 입력데이터의 공간적 크기를 줄이는 방법을 사용

### 완전 연결 계층(Fully Connected Layer, FC)

각 입력 노드가 모든 출력 노드와 연결된 상태, 이를 통해 입력과 출력 간의 모든 가능한 관계를 학습

이 계층에서는 출력 노드의 수를 조절할 수 있어 모델의 복잡성과 용량을 조절하는데 사용 됨

---

# 정리

토큰화 > 자연어(문장)를 작은 단위로 나눠 컴퓨터가 자연어 처리하기 쉽게 만드는 전처리?

임베딩 >