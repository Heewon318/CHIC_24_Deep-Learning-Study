# 5. 토큰화

자연어(National Language)는 자연어라고도 부르며, 인공적으로 만들어진 프로그래밍 언어와 다르게 사람들이 쓰는 언어 활동을 위해 자연히 만들어진 언어를 의미합니다.

자연어 처리(Natural Language Processing, NLP)는 컴퓨터가 인간의 언어를 이해하고 해석 및 생성하기 위한 기술을 의미합니다.

자연어 처리는 인공지능의 하위 분야 중 하나로 컴퓨터가 인간과 유사한 방식으로 인간의 언어를 이해하고 처리하는 것이 주요 목표 중 하나입니다.

인간 언어의 구조, 의미 , 맥락을 분석하고 이해할 수 있는 알고리즘과 모델을 개발합니다.

모델을 개발하여 몇몇 문제가 해결됩니다.

- 모호성(Ambiguity): 인간의 언어는 단어와 구가 사용되는 맥락에 따라 여러 의미를 갖게 되어 모호한 경우가 많습니다. 알고리즘은 이러한 다양한 의미를 이해하고 명확하게 구분할 수 있어야 합니다.
- 가변성(Variability): 인간의 언어는 다양한 사투리(Dialects), 강세(Accent), 신조어(Coined word), 작문 스타일로 인해 매우 가변적입니다. 알고리즘은 이러한 가변성을 처리할 수 있어야 하며 사용 중인 언어를 이해할 수 있어야 합니다.
- 구조(Structure): 인간의 언어는 문장이나 구의 의미를 이해할 때 구문(Syntactic)을 파악하여 의미(Semantic)를 해석합니다. 알고리즘은 문장의 구조와 문법적 요소를 이해하며 의미를 추론하거나 분석할 수 있어야합니다.

위와 같은 문제를 이해하고 구분할 수 있는 모델을 만들려면 우선 말뭉치(Corpus)를 일정한 단위인 토큰(Token)으로 나눠야 합니다.

말뭉치란 자연어 모델을 훈련하고 평가하는데 사용되는 대규모의 자연어를 뜻합니다.

말뭉치는 뉴스 기사, 사용자 리뷰, 저널이나 칼럼 등에서 목적에 따라 구축되는 텍스트 데이터를 의미합니다.

말뭉치는 일련의 단어의 사능성을 예측하는 알고리즘인 언어 모델을 구축하고 평가하는데 자주 사용합니다.

토큰은 개별 단어나 문장 부호와 같은 텍스트를 의미하며 말뭉치보다 더 작은 단위 입니다.

토큰은 텍스트의 개별 단어, 구두점 또는 기타 의미 단위일 수 있습니다.

토큰으로 나누는 목적은 컴퓨터가 자연어를 이해할 수 있게 나누는 과정입니다.

이러한 과정을 토큰화(Tokenization)라고 합니다.

토큰화는 컴퓨터가 텍스트르 보다 효율적으로 분석하고 처리할 수 있도록 하는 중요한 단계입니다.

많은 자연어 처리 과정에서 중요한 단계입니다.

토큰화를 위해 토크나이저(Tokenizer)를 사용합니다.

토크나이저란 텍스트 문자열을 토큰으로 나누는 알고리즘 또는 소프트웨어를 의미합니다.

토크나이저 알고리즘은 텍스트에서 발생하는 다양한 단어와 구문을 식별하고 분석할 수 있게 하므로 언어 모델링 또는 기계 번역과 같은 다양한 자연어 처리 작업에서 사용됩니다.

토크나이저를 사용해 문장을 토큰화한다면 아래와 같이 표현 할 수 있습니다.

- 입력: ‘형태소 분석기를 이용해 간단하게 토큰화활 수 있다.’
- 결과: [’형태소’, ‘분석기’, ‘를’, ‘이용’, ‘하’, ‘어’, ‘간단’, ‘하’, ‘게’, ‘토큰’, ‘화’, ‘하’, ‘ㄹ’, ‘수’, ‘있’, ‘다’, ‘.’]

일반적으로 토큰을 나누는 기준은 구축하려는 시스템이나 주어진 상황에 따라 달라집니다.

싴지어 어떻게 토큰을 나누었느냐에 따라 시스템의 성능이나 처리 결과가 크게 달라지기도 합니다.

토크나이저를 구축하는 방법은 다음과 같습니다.

- 공백 분할: 텍스트를 공백 단위로 분리해 개별 단어로 토큰화 합니다.
- 정규표현식 적용: 정규 표현식으로 특정 패턴을 식별해 텍스트를 분할합니다.
- 어휘 사전(Vocabulary) 적용: 사전에 정의된 단어 집합을 토큰으로 사용합니다.
- 머신러닝 활용: 데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝을 적용합니다.

어휘 사전(Vocabulary, Vocab)은 사전에 정의된 단어를 활용해 토크나이저를 구축하는 방법입니다.

직접 어휘 사전을 구축하기 때문에 없는 단어나 토큰이 존재할 수 있습니다.

이러한 토큰을 OOV(Out Of Vocab)라고 합니다.

어휘 사전 방법은 OOV 문제를 고려해 토큰화하는 것이 중요합니다.

큰 어휘 사전을 구축하면 학습 비용이 증대합니다.

자칫 차원의 저주(Curse of Dimensionality)에 빠질 수 있습니다.

모든 토큰이나 단어를 백터화하면 어휘 사전에 등장하는 토큰 개수만큼의 차원이 필요하고, 벡터값이 거의 모두 0의 값을 가지는 희소(sparse) 데이터로 표현될 수 있습니다.

희소 데이터의 표현 방법은 어휘 사전에 등장하는 출현 빈도만 고려하기 때문에 문장에서 발생한 토큰들의 순서 관계를 잘 표현하지 못 합니다.

## 단어 및 글자 토큰화

토큰화는 자연어 처리에서 매우 중요한 전처리 과정으로, 텍스트 데이터를 구조적으로 분해하여 개별 토큰으로 나누는 작업을 의미합니다.

이러한 토큰화 과정은 정확한 분석을 위해 필수이며, 단어 문자으이 빈도수, 출현 패턴 등을 파악할 수 있습니다.

작은 단위로 분해된 텍스트 데이터는 컴퓨터가 이해하고 처리하기가 용이해 기계 번역, 문서 분류, 감성 분석 등 다양한 자연어 처리 작업에 활용할 수 있습니다.

입력된 텍스트 데이터를 단어(Word)나 글자(Character) 단위로 나누는 기법으로는 단어 토큰화와 글자 토큰화가 있습니다.

이러한 기법으로 각각의 토큰은 의미를 갖는 최소 단위로 분해됩니다.

단어와 글자 단위로 토큰화 하는 방법을 통해 자연어 처리 모델을 개선하고 더욱 정확한 분석 결과를 얻을 수 있습니다.

### 단어 토큰화

단어 토큰화(Word Tokenization)는 자연어 처리 분야에서 핵심적인 전처리 작업중 하나로 텍스트 데이터를 의미 있는 단위인 단어로 분리하는 작업입니다.

모든 언어가 띄어쓰기나 특정한 문장 부호를 활용하는 것은 아니지만, 대부분의 언어는 띄어쓰기를 이용해 문장을 의미 있는 단어위로 나눠 표현합니다.

그러므로 단어 토큰화는 띄어 쓰기, 문장 부호, 대소문자 등의 특정 구분자를 활용해 토큰화가 수행됩니다.

단어 토큰화는 품사 태깅, 개체명 인식, 기계 번역 등의 작업에서 널리 사용되며 가장 일반적인 토큰화 방법입니다.

```python
review = "현실과 구분 불가능한 cg. 시각적으로 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
tokenized = review.split()
print(tokenized)
```

```python
# 결과

['현실과', '구분', '불가능한', 'cg.', '시각적', '즐거움은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']
```

문자열 데이터 형태는 split 메서드를 이용하여 쉽게 토큰화할 수 있습니다.

split 메서드는 주어진 구분자를 통해 문자열을 리스트 데이터로 나눕니다.

구분자를 이력하지 않으면 공백(Whitespace)을 기준으로 나눕니다.

토큰으로 나뉜 데이터들을 보면 의미 단위로 잘 나뉜 것처럼 보입니다.

하지만, ‘최고!’와 ‘최고!!’를 비교해 보면 느낌표 하나의 차이로 다른 의미 단위로 나뉩니다.

‘cg.’도 마찬가지입니다.

말뭉치 내에 마침표가 없는 다른 ‘cg’라는 토큰이 있다면, 이 두 토큰은 서로 다른 토큰으로 나뉩니다.

‘cg.’와 ‘cg’가 비슷한 의미가 있는 것을 알고 있지만, 단어 토큰화를 통해 만들어진 단어 사전에서는 다른 토큰이 됩니다.

‘cg’라는 도큰이 단어 사전 내에 있더라도, ‘cg.’, ‘cg는’, ‘cg도’ 등은 OOV가 됩니다.

이처럼 단어 토큰화는 한국어 접사, 문장 부호, 오타 혹은 띄어쓰기 오류 등에 취약합니다.

### 글자 토큰화

글자 토큰화(Character Tokenization)는 띄어쓰기뿐만 아니라 글자 단위로 문장을 나누는 방식으로, 비교적 작은 단어 사전을 구축할 수 있다는 장점이 있습니다.

작은 단어 사전을 사용하면 학습시 컴퓨터자원을 아낄  수 있으며, 전체 말뭉치를 학습할 때 각 단어를 더 자주 학습할 수 있다는 장점이 있습니다.

글자 토큰화는 언어 모델링과 같은 시퀀스 예측 작업에서 활용됩니다.

eg) 다음 문자를 예측하는 언어 모델링에서 글자 토큰화는 유용한 방식입니다.

```python
review = "현실과 구분 불가능한 cg. 시각적으로 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
tokenized = list(review)f
print(tokenized)
```

```python
# 결과
['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']
```

글자 토큰화는 리스트 형태로 변환하면 쉽게 수행할 수 있습니다.

단어 토큰화와는 다르게 공백도 토큰으로 나뉜 것을 볼 수 있습니다.

영어의 경우 글자 토큰화를 진행하면 각 알파벳으로 나뉩니다.

하지만 한글의 경우에는 하나의 글자는 여러 자음과 모음의 조합으로 이루어져 있습니다.

그러므로 자소(字素) 단위로 나눠서 자소 단위 토큰화를 수행합니다.

자모(jamo) 라이브러리는 한글 문자 및 자모 작업을 위한 한글 음절 분해 및 합성 라이브러리입니다.

이 라이브러리를 이용하여 텍스트를 자소 단위로 분해해 토큰화를 수행합니다.

자모 라이브러리에서 사용하는 자모 변환 함수와 한글 호환성 자모 변환 함수입니다.

```python
retval = jamo.h2j(hangul_string)
```

자모 변환 함수는 입력된 한글 문자열 유니코드 U+1100 ~ U+11FE 사이의 조합형 한글 자모로 변환하는 함수입니다.

컴퓨터가 한글을 인코딩하는 방식은 크게 조합형과 완성형으로 나눌 수 있습니다.

조합형은 글자를 자모 단위로 나눠 인코딩한 뒤 이를 조합해 한그을 표현합니다.

완성형은 조합된 글자 자체에 값을 부여해 인코딩하는 방식입니다.

h2j 함수는 완성형으로 입력된 한그을 조합형 한글로 변환합니다.

```python
retval = jamo.j2hcj(jamo)
```

j2hcj 함수는 조합형 한글 문자열을 자소 단위로 나눠 반환하는 함수입니다.

조합형 한글로 입력된 문자열은 초성, 중성, 종성으로 나뉩니다.

이 함수를 통해 완성형 한글을 쉽게 자소 단위로 나눌 수 있습니다.

```python
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적으로 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
```

```python
# 결과
['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 
'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']
```

자모 변환 함수와 한글 호환성 자모 변환 함수를 활용해 자소 단위로 분리 했습니다.

글자 단위로 토큰화하면 단어 단위로 토큰화하는 것에 비해 비교적 적은 크기의 단어 사전을 구축할 수 있습니다.

토큰화의 단점이었던 ‘cg도’, ‘cg는’, ‘cg.’ 등에서도 ‘도’, ‘는’, ‘.’과 같은 접사와 문장 부호의 의미를 학습할 수 있습니다.

또, 작은 크기의 단어 사전으로  OOV를 획기적으로 줄일 수 있습니다.

개별 토큰은 아무런 의미가 없으므로 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야 합니다.

토큰 조합 방식을 사용해 문장 생성이나 개체명 인식(Name Entity Recognition)등을 구현할 경우, 다의어나 동음이의어가 많은 도메인에서 구별하는 것이 어려울 수 있습니다.

모델 입력 시퀀스(sequence)의 길이가 길어질수록 연산량이 증가한다는 단점도 있습니다.

## 형태소 토큰화

형태소 토큰화(Morpheme Tokenization)란 텍스트를 형태소 단위로 나누는 토큰화 방법으로 언어의 문법과 구조를 고려해 단어를 분리하고 이를 의미 있는 단위로 분류하는 작업입니다.

형태소 토큰화는 한국어와 같이 교착어(Agglutinative Language)인 언어에서 중요하게 수행되니다.

한국어는 대부분의 언어와 달리 각 단어가 띄어쓰기로 구분되지 않스니다.

한국어는 어근에 다양한 접사와 조사가 조합되어 하나의 낱말을 이루므로 각 형태소를 적절히 구분해 처리해야합니다.

eg) ‘그는 나에게 인사를 했다’라는 문장을 보면 ‘그는’은 ‘그’라는 단어와 ‘는’이라는 조사가 결합해 하나의 단어로 이루어져있고, ‘나’라는 단어와 ‘에게’라는 조사가 결합해 하나의 단어로 이루어져 있다.

자립 형태소: 그, 나, 인사

의존 형태소: -는, -에게, -를, 했-, -다

이처럼 ‘그’, ‘-는’, ‘나’, ‘-에게’ 등과 같이 실제로 의미를 가지고 있는 최소의 단위를 형태소(Morpheme)라고 합니다.

형태소는 크게 스스로 의미를 가지고 있는 자립 형태소(Free Morpheme)와 스스로 의미를 갖지 못하고 다른 형태소와 조합되어 사용되는 의존 형태소(Bound Morpheme)로 구분됩니다.

자립 형태소는 단어의 기본이 되는 형태소로서, 명사, 동사, 형용사와 같은 단어를 이루는 기본 단위입니다.

의존 형태소는 자립 형태소와 함께 조합되어 문장에서 특정한 역할을 수행하며, 조사, 어미, 접두사, 접미사 등이 해당됩니다.

이러한 형태소 분석을 통해 문장 내 각 형태소의 역하을 파악할 수 있으며, 이를 바탕으로 문장을 이해하고 처리할 수 있습니다.

### 형태소 어휘 사전

형태소 어휘 사전(Morpheme Vocabulary)은 자연어 처리에서 사용되는 단어의 집합인 어휘 사전 중에서도 각 단어의 형태소 정보를 포함하는 사전을 말합니다.

단어가 어떤 형태소들의 조합으로 이루어져 있는지에 대한 정보를 담고 있어 형태소 분석 작업에는 매우 중요한 역할을 합니다.

eg) “그는 나에게 인사를 했다”라는 문장과 “나는 그에게 인사를 했다”라는 문장이 있다고 가정하면, 띄어쓰기를 활용해 토큰화를 수행한다면 어휘 사전은 [’그는’, ‘나에게’, ‘인사를’, ‘했다’, ‘나는’, ‘그에게’]로 구성이됩니다.

말뭉치에 “그녀는 그에게 인사를 했다”라는 문장이 추가되면 모델은 ‘그는’, ‘나는’, ‘그녀는’과 같은 데이터를 같은 의미 단위로 인식하지 못 하고 학습을 진행합니다.

이러한 문제를 방지하기 위해 형태소 단위로 어휘 사전을 구축한다면 [’-는’, ‘-에게’, ‘그’, ‘나’]와 같은 정보에서 ‘그녀’의 토큰 정보만 새로 학습해 ‘그녀는’이나 ‘그녀에게’와 같은 어휘를 쉽게 학습할 수 있습니다.

일반적으로 형태소 어휘 사전에는 각 형태소가 어떤 품사에 속하는지와 해당 품사의 뜻 등의 정보도 함께 제공됩니다.

텍스트 데이터를 형태소 분석하여 형태소에 해당하는 품사(Part Of Speech, POS)를 태깅하는 작업을 품사 태깅(POS Tagging)이라고 합니다.

이를 통해 자연어 처리 분야에서 문맥을 고려할 수 있어 더욱 정확한 분석이 가능해집니다.

eg) “그는 나에게 인사를 했다”라는 문장에 대해 품사 태깅을 수행한다면, 그(명사) + 는(조사) + 나(명사) + 에게(조사) + 인사(명사) + 를(조사) + 했다(동사)로 태깅을 할 수 있습니다.

### KoNLPy

KoNLPy는 한국어 자연어 처리를 위해 개발된 라이브러리로 명사 추출, 형태소 분석, 품사 태깅 등의 기능을 제공합니다.

텍스트 데이터를 전처리하고 분석하기 위한 다양한 도구와 함수를 제공해 텍스트 마이닝, 감정 분석, 토픽 모델링 등 다양한 NLP 작업에서 사용합니다.\

KoNLPy는 자바 언어 기반의 한국어 혀애소 분석기로 자바 개발 키트(Java Development Kit, JDK) 기반으로 개발되었습니다.

KoNLPy는 Okt(Open korean Text), 꼬꼬마(Kkma), 코모란(Komoran), 한나눔(Hannanum), 메캅(Mecab)등의 다양한 형태소 분석기를 지원합니다.

```python
from konlpy.tag import Okt

okt = Okt()

sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

nouns = okt.nouns(sentence)
phrases = okt.phrases(sentence)
morphs = okt.morphs(sentence)
pos = okt.pos(sentence)

print("명사 추출: ", nouns)
print("구 추출: ", phrases)
print("형태소 추출: ", morphs)
print("품사 태깅:", pos)
```

```python
# 결과
명사 추출:  ['무엇', '상상', '수', '사람', '무엇', '낼', '수']
구 추출:  ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']
형태소 추출:  ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']
품사 태깅: [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]
```

Okt 객체는 문장을 입력받아 명사, 구, 형태소, 품사, 등의 정보를 추출하는 여러 가지 메서드를 제공합니다.

Okt에서 지원하는 대표적인 메서드는 명사 추출(okt.nouns), 구문 추출(okt.phrases), 형태소 추출(okt.morphs), 품사 태깅(okt.pso)입니다.

명사추출, 구문 추출, 형태소 추출은 입력된 문장에서 각각 명사, 어절 구 단위, 형태소만 추출해 리스트를 반환합니다.

품사 태깅은 입력된 문장에서 각 단어에서 각 단어에 대판 푸맛 정보를 추출하여(형태소, 품사) 형태의 튜플로 구성된 리스트를 반환합니다.

```python
from konlpy.tag import Kkma

kkma = Kkma()

sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

nouns = kkma.nouns(sentence)
sentences = kkma.sentences(sentence)
morphs = kkma.morphs(sentence)
pos = kkma.pos(sentence)

print("명사 추출: ", nouns)
print("구 추출: ", sentences)
print("형태소 추출: ", morphs)
print("품사 태깅:", pos)
```

```python
# 결과
명사 추출:  ['무엇', '상상', '수', '사람', '무엇']
구 추출:  ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']
형태소 추출:  ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']
품사 태깅: [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]
```

꼬꼬마는 Kkma 클래스를 통해 명사, 문장, 형태소, 품사를 추출할 수 있습니다.

꼬꼬마는 구문 추출 기능은 지원하지 않지만, 명사 추출(kkma.sentences) 기능을 추출합니다.

동일한 메서드라도 형태소 분석기의 특징에 따라 다른 결과가 나타납니다

eg) 명사 추출의 경우 Okt에서 반환하던 ‘낼’과 ‘수’를 반환하지 않았으며, 형태소 추출의 경우 ‘할’을 더 분리해 ‘하’, ‘ㄹ’로 반환 한 것을 확인 할 수 있습니다.

품사 태깅 역시 상이한 결과를 제공합니다.

Okt는 총 19개의 품사를 구분해 반환하는 한편, 꼬꼬마는 더 세분화해 56개의 품사로 태깅합니다.

태깅하는 품사의 수가 많으면 더 자세한 단위로 분석이 가능하지만, 품사 태깅에 소요되는 시간도 길어지며, 더 많은 품사로 분리해 모델의 성능이 저하될 수 도 있습니다.

Okt 품사 태그

| 태그 | 품사 | 태그 | 품사 |
| --- | --- | --- | --- |
| Noun | 명사 | Punctuation | 구두점 |
| Verb | 동사 | Foreign | 외국어 |
| Adjective | 형용사 | Alph | 알파벳 |
| Determiner | 관형사 | Number | 숫자 |
| Adverb | 부사 | KoreanParticle | 자음/모음 |
| Conjunction | 접속사 | URL | 웹 주소 |
| Exclamation | 감탄사 | Hashtag | 해시태그(#) |
| Josa | 조사 | Email | 이메일 |
| PreEomi | 선어말어미 | ScreenName | 아이디 표기(@) |
| Eomi | 어미 | Unknown | 미등록 |
| Suffix | 접미사 |  |  |

꼬꼬마 품사 태그

| 태그 | 품사 | 태그 | 품사 |
| --- | --- | --- | --- |
| NNG | 보통명사 | EPH | 존칭 선어말 어미 |
| NNP | 고유명사 | EPT | 시제 선어말 어미 |
| NNB | 일반 의존 명사 | EPP | 공선 선어말 어미 |
| NNM | 단위 의존 명사 | EFN | 평서형 종결 어미 |
| NR | 수사 | EFQ | 의문형 종결 어미 |
| NP | 대명사 | EFO | 명령형 종결 어미 |
| VV | 동사 | EFA | 청유형 종결 어미 |
| VA | 형용사 | EFI | 감탄형 종결 어미 |
| VXV | 보조 동사 | EFR | 존칭형 종결 어미 |
| VXA | 보조 형용사 | ECE | 대등 연결 어미 |
| VCP | 긍정 지정사 | ECS | 보조적 연결 어미 |
| VCN | 부정 지정사 | ECD | 의존적 연결 어미 |
| MDN | 수 관형사 | ETN | 명사형 전성 어미 |
| MDT | 일반 관형사 | ETD | 관형형 전성 어미 |
| MAG | 일반 부사  | XPN | 체언 접두사 |
| MAC | 접속 부사 | XPV | 용언 접두사 |
| JKS | 주격 조사 | XSN | 명사 파생 접미사 |
| JKC | 보격 조사 | XSV | 동사 파생 점미사 |
| JKG | 관형격 조사 | XSA | 형용사 파생 접미사 |
| JKO | 목적격 조사 | SR | 어근 |
| JKM | 부사격 조사 | SF | 마침표, 물음표, 느낌표 |
| JKI | 호격 조사 | SE | 줄임표 |
| JKQ | 인용격 조사 | SS | 따옴표, 괄호표, 줄표 |
| JC | 접속 조사 | SP | 쉼표, 가운뎃점, 콜론, 빗금 |
| JX | 보자사 | SO | 붙임표(물결, 숨김, 빠짐) |
| OH | 한자 | SW | 기티기호(논리수학기호, 화폐기호) |
| OL | 외국어 | IC | 감탄사 |
| ON | 숫자 | UN | 명사추정범주 |

### NLTK

NLTK(Natural Language Toolkit)는 자연어 처리를 위해 개발된 라이브러리로, 토큰화, 형태소, 분석, 구문 분석, 개체명 인식, 감성 분석 등과 같은 기능을 제공합니다.

NLTK는 주로 영어 자연어 처리를 위해 개발 됐지만 네덜란드어, 프랑스어, 독일어 등과 같은 다양한 언어의 자연어 처리를 위한 모델을 제공합니다.

NLTK 라이브러리를 활용해 토큰화나 품사 태깅 작업을 하기 위해서는 해당 작업을 수행할 수 있는 패키지나 모델을 다운로드 해야합니다.

Punkt 모델과 Averaged Perceptron Tagger 모델 다 트리뱅크(Treebank)라는 대규모의 언어 말뭉치를 기반으로 학습 됐습니다.

Punkt 모델은 통계기반 모델이며, Averaged Perceptron Tagger는 퍼셉트론을 기반으로 품사 태깅을 수행합니다.

```python
from nltk import tokenize

sentence = "Those who can imagine anything, can create the impossible."

word_token = tokenize.word_tokenize(sentence)
sent_token = tokenize.sent_tokenize(sentence)

print(word_token)
print(sent_token)
```

```python
# 결과
['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']
['Those who can imagine anything, can create the impossible.']
```

영문 토큰화는 punkt 모델을 기반으로 단어나 문장을 토큰화 합니다.

단어 토크나이저(word_tokenize)는 문장을 입력받아 공백을 기준으로 단어를 분리하고, 구두점 등을 처리해 각각의 단어(token)를 추출해 리스트로 반환합니다.

문장 토크나이저(sent_tokenize)는 문장을 입력받아 마침표(.), 느낌표(!), 물음표(?) 등의 구두점을 기준으로 문장을 분리해 리스트로 반환합니다.

영어는 한국어보다 토큰화가 쉬운 구조를 가지고 있어, 위와 같은 방법으로 간단하게 단어나 문장으로 토큰화를 수행할 수 있습니다.

```python
from nltk import tag
from nltk import tokenize

sentence = "Those who can imaging anything, can create the impossible."

word_tokens = tokenize.word_tokenize(sentence)
pos = tag.pos_tag(sentence)
print(pos)
```

```python
# 결과
[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imaging', 'VBG'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]
```

NTLK 라이브러리의 품사 태깅(pos_tag) 메서드는 토큰화된 문장에서 품사 태깅을 수행합니다.

품사 태깅을 수행하기 위해서는 토큰화된 단어들이 들어가야 하며, Averaged Perceptron Tagger 모델이 설치되어 있어야합니다.

Averaged Perceptron Tagger 모델은 총 35개의 품사를 태깅할 수 있습니다.

| 태그 | 품사 | 태그 | 품사 |
| --- | --- | --- | --- |
| CC | 접속사 | PDT | 관사 |
| CD | 기수(Cardinal Number) | POS | 소유격 조사 |
| DT | 관형사 | PRP | 인칭 대명사 |
| EX | there의 대명사형태 | PRP$ | 소유격 대명사 |
| FW | 외래어 | RB | 부사 |
| JN | 전치사 | RBR | 비교급 부사 |
| JJ | 형용사 | RBS | 최상급 부사 |
| JJR | 비교급 형용사 | RP | 전치사 또는 조동사 |
| JJS | 최상금 형용사 | VB | 기본형 동사 |
| LS | 목록표시 | VBD | 과거형 동사 |
| MD | 조동사 | VBG | 현재 부사형 동사 |
| NN | 단수형 명사 | VBN | 과거 분사형 동사 |
| NNS | 복수형 명사 | VBP | 1인칭 단수 현재형 동사 |
| NNP | 단수형 고유 명사와 서수(Ordinal Number) | VBZ | 3인칭 단수 현재형 동사 |
| NNPS | 복소수형 고유 명사 | WDT | 관계사 |
| SYM | 기호 | WP | 주격 관계대명서 |
| TO | To 부정사 | WP$ | 소유격 관계대명사 |
| UH | 감탄사 | WRB | 관계부사 |

### spaCy

spaCy는 사이썬(Cython) 기반으로 개발된 오픈 서소 라이브러리로서, NLTK 라이브러리와 마찬가지로 자연어 처리를 위한 기능을 제공합니다.

NLTK 라이브러리와의 주요한 차이점은 빠른 속도와 높은 정확도를 목표로 하는 머신러닝 기반의 자연어 처리 라이브러리라는 점입니다.

NLTK는 학습 목적으로 자연어 처리에 대한 다양한 알고리즈모가 예제를 제공하는 반면, sapCy는 효율적인 처리 속도와 높은 정확도를 제공하는 것을 목표로 합니다.

그러므로 NLTK에서 사용하는 모델보다 더 크고 복잡하며 더 많은 리소스를 요구합니다.

scpCy는 GPU 가속을 비롯해 영어, 프랑스어, 한국어, 일본어 등을 비롯해 24개 이상의 언어로 사전 학습된 모델을 제공합니다.

```python
import spacy

nlp = spacy.load("en_core_web_sm")
sentence = "Those who can imaginge anything, can crate the impossible."
doc = nlp(sentence)

for token in doc:
    print(f"[{token.pos_:5} - {token.tag_:3}: {token.text}]")
```

```python
# 결과

[PRON  - DT : Those]
[PRON  - WP : who]
[AUX   - MD : can]
[VERB  - VB : imaginge]
[PRON  - NN : anything]
[PUNCT - ,  : ,]
[AUX   - MD : can]
[VERB  - VB : crate]
[DET   - DT : the]
[ADJ   - JJ : impossible]
[PUNCT - .  : .]
```

sapCy는 사전 학습된 모데을 기반으로 처리하기 때문에 sapCy 모델 불러오기 함수(load)를 통해 모델을 설정할 수 있습니다.

sapCy는 객체 지형적(Object Oriented)으로 구현돼 처리한 결과를 doc 객체에 저장합니다.

doc 객체는 다시 여러 token 객체로 이뤄져 있으며, 이 token 객체에 대한 정보를 기반으로 다양한 자연어 처리 작업을 수행합니다.

nlp 인스턴스에 문장을 입력하면 doc 객체가 반환되며 token 객체의 tag_나 pos_와 같은 속성에 접근해 값을 확인 할 수 있습니다.

toekn 객체에는 기본 품사 속성(pos_), 세분화 품사 속성(tag_), 원본 텍스트 데이터(text), 토큰 사이의 공백을 포함하는 텍스트 데이터(text_with_ws), 벡터(vector), 벡터 노름(vector_norm) 등의 속성이 포함되어 있습니다.

| pos_ 속성 | tag_ 속성 | 품사 | pos_ 속성 | tag_ 속성 | 품사 |
| --- | --- | --- | --- | --- | --- |
| ADJ | JJ | 형용사 | NUM | CD | 기수(Cardinal Number) |
| ADJ | JJR | 비교급 형용사 | PROPN | NNP | 고유 명사 |
| ADJ | JJS | 최상급 형용상 | PROPN | NNPS | 복수형 고유 명사 |
| ADP | IN | 전치사 | PUNCT | -LRB- | 왼쪽 괄호 |
| ADV | RB | 부사 | PUNCT | -RRB- | 오른쪽 괄호 |
| ADV | RBR | 비교급 부사 | PUNCT | , | 쉼표 |
| ADV | RBS | 최상급 부사 | PUNCT | : | 콜론 |
| ADV | WRB | 관계 부사 | PUNCT | . | 마침표, 물음표, 느낌표 |
| AUX | MD | 조동사 | PUNCT | … | 생략 부호 |
| CCONJ | CC | 접속사 | PUNCT | “ | 여는 따옴표 |
| DET | DT | 관사, 한정사 | PUNCT | “ | 닫는 따옴표 |
| DET | PDT | 전치사 한정사 | PUNCT | HYPH | 하이픈 |
| DET | PRP$ | 소유격 대명사 | PUNCT | NFP | 불필요한 문장 부호 |
| DET | WDT | 관계 대명사 | SYM | $ | 통화 |
| DET | WP$ | 소유격 관계 대명사 | VERB | VB | 동사 기본형 |
| INTJ | UH | 감탄사 | VERB | VBD | 과거형 동사 |
| NOUN | NN | 명사 | VERB | VBG | 동명사, 현재분사 |
| NOUN | NNS | 복수형 명사 | VERB | VBN | 과거분사 |
| PART | POS | 소유격 조사 | VERB | VBP | 동사 현재형 |
| PART | RP | 부사적 불변화사 | VERB | VBZ | 3인칭 단수 동사 현재형 |
| PART | TO | To 부정사 | X | ADD | 이메일 |
| PRON | EX | 존재문(there) | X | FW | 외래어 |
| PRON | PRP | 인칭 대명사 | X | LS | 목록 |
| PRON | WP | 관계 대명사 | X | SYM | 기호 |

형태소 분석과 품사 태깅은 문장에서 의미를 갖는 최소 단위 이므로 자연어 처리에서 매우 중요한 전처리 작업입니다.

띄어쓰기와 맞춤법이 잘 지켜지지 않는 경우나 신조어 외래어, 특정 도메인에에서 사용되는 축약어 등은 완벽히 대응하기가 어렵습니다.

그럼에도 불구하고 형태소 분석과 품사 태깅은 자연어 처리에서 필수적인 작업으로, 이를 통해 단어의 의미를 파악하고 문자으이 구조를 이해할 수 있습니다.

형태소 분석과 품사 태깅은 주요 키워드를 추출하거나 문장의 긍정/부정 여부를 판단하는 언어 모델의 성능을 높이는 데도 중요한 역하을 합니다.

자연어 처리 분야에서 핵심적인 기술로 자리 잡고 있으며, 이를 활용하여 다양한 자연어 처리 응용 프로그램을 개발할 수 있습니다

## 하위 단어 토큰화

자연어 처리에서 형태소 분석은 중요한 전처리 과정 중 하나 입니다.

형태소는 자연어의 최소 의미 단위이며, 대부분의 자연어가 형태소의 조합으로 이뤄져 있습니다.

컴퓨터가 자연어르 인간이 이해하는 방식과 비슷하게 처리할 수 있게 하려면 형태소 단위의 토큰화가 효과적인 방법입니다.

언어는 시간이 지남에 따라 변화하고 새로운 단어나 표현이 등장하며 더 이상 사용되지 않는 단어나 표현도 생깁니다.

현대의 일상 언어에서는 맞춤법이나 띄어쓰기가 엄격하게 지켜지지 않는 경우가 많고 형태소 분석기의 취약점인 신조어나 고유어 등이 빈번하게 생겨납니다.

또한, 시스템에서 분석하려는 텍스트 데이터는 컴퓨터나 스마트폰 등 다양한 디지털 매체에서 생성되는데, 이러한 텍스트는 오탈자가 발생할 확률이 높습니다.

외래어, 띄어쓰기 오류, 오탈자 등이 있는 문장은 기존 형태소 분석기로 토큰화하면 이러한 결과가 나옵니다.

- 원문: 시보리도 짱짱해고 허리도 어방하지안구 죠하효
- 결과[’시’, ‘보리’, ‘도’, ‘짱짱해고’, ‘허리’, ‘도’, ‘어’, ‘벙하지안구’, ‘죠’, ‘하’, ‘효’]

형태소 분석기 결과를 보면 ‘시보리’라는 외래어를 인식하지 못 하고 ‘시’와 ‘보리’로 나뉜 것을 볼 수 있습니다.

또한 ‘어벙하다’라는 표현 역시 ‘어’라는 토큰과 ‘벙하지안구’ 토크능로 나뉘었습니다.

형태소 분석기는 전문용어나 고유어가 많은 데이터를 처리할 떄 약점을 보입니다.

형태소 분석기는 모르는 단어를 적절한 단위로 나누는 것에 취약하며, 잠재적으로 어휘 사전의 크기를 크게 만들고 OOV에 대응하기 어렵게 만듭니다.

- 원문: 진짜 멋있네요! 돈쭐날만 하네요.
- 결과: [’진짜’, ‘멋있네요’, ‘!’, ‘돈쭐날’, ‘만’, ‘하네요’, ‘.’]

‘돈쭐내다’란 ‘돈’과 ‘혼쭐내다’의 합성어로 어떠한 사람이나 식당이 사회적으로 옳은 행동을 함으로써 타의 귀감이 된 가게의 물건을 팔아주자는 신조어 입니다.

이를 기존 형태소 분석기로 토큰화하면 ‘돈쭐날’로 분해됩니다.

만약 ‘돈쭐날만’ 이라는 문장을 토큰화 하면 ‘돈쭐’, ‘날’, ‘만’ 또는 ‘돈쭐’, ‘날만’으로 토큰화하는 것이 적절합니다.

겨로가처럼 토큰화된다면 ‘돈쭐-’ 이라는 어근에 올 수 있는 모든 어미에 대한 조합이 하나의 토큰으로 인식될 것입니다.

어휘 사전에 ‘돈쭐내러’, ‘돈쭐나’, ‘돈쭐내다’ 등이 존재하게 돼 어휘 사전의 크기를 크게 만듭니다.

현대 자연어 처리에서는 신조어의 발생, 오탈자, 축약어 등을 고려해야 하기 때문에 분석할 단어의 양이 많아져 어려움을 겪습니다.

이를 해결하기 위한 방법 중 하나로 하위 단어 토큰화(Subword Tokenization)가 있습니다.

하위 단어 토큰화란 하나의 단어가 빈번하게 사용되는 하위 단어(Subword)의 조합으로 나누어 토큰화 하는 방법입니다.

eg) ‘Reinforcement’라는 단어는 길이가 비교적 길어 처리가 어려울 수 있습니다.

하위 단어 토큰화를 적용하면 ‘Rein’, ‘force’, ‘ment’ 등으로 나눠 처리할 수 있습니다.

하위 단어 토큰화를 적용하면 단어의 길이를 줄일 수 있어 처리 속도가 빨라지 뿐만 아니라, OOV 문제, 신조어, 은어, 고유어 등으로 인한 문제를 완화 할 수 있습니다.

하위 단어 토큰화 방법으로는 바이트 페어 인코딩, 워드피스, 유니그램 모델 등이 있습니다.

### 바이트 페어 인코딩

바이트 페어 인코딩(Byte Pair Encoding, BPE)이란 다이그램 코딩(Digram Coding)이라고도 하며 하위 단어 토큰화의 한 종류 입니다.

텍스트 데이터에서 가장 빈번하게 등장하는 글자 쌍의 조합을 찾아 부호화하는 압축 알고리즘으로 초기에는 데이터 압축을 위해 개발 됐으나 자연어 처리 분야에서 하위 단어 토큰화를 위한 방법으로 사용됩니다.

이 알고리즘은 연속된 글자 쌍이 더 이상 나타나지 않거나 정해진 어휘 사전 크기에 도달할 때까지 조합 탐지와 부호화를 반복하며 이 과정에서 자주 등장하는 단어는 하나의 토큰으로 토큰화되고, 덜 등장하는 단어는 여러 토큰의 조합으로 표현됩니다.

eg) ‘abracadabra’라는 단어를 바이트 페어 인코딩을 하게 된다면

- 원문:  abracaadabra
- Step #1: AracadAra(ab = A)
- Step #2: ABcadAB(ra = B)
- Step #3: CcadC(AB = C)

바이트 페어 인코딩은 입력 데잍어ㅔ서 가장 많이 등장한 글자의 빈소수를 측정하고, 가장 빈도수가 높은 글자 쌍을 탐색합니다.

현재 원문 ‘abracadabra’에서 ‘ab’글자 쌍이 가장 빈도수가 높으므로 입력 데이터에 없는 새로운 글자인 ‘A’로 치환합니다.

치환된 새로운 글자는 어떠한 글자를 사용해도 상관 없습니다.

‘AracadAra’에선 ‘ra’ 글자 쌍이 가장 빈도수가 높으므로 ‘B’로 치환합니다.

치환된 데이터는 ‘ABcadAB’로 아직 ‘AB’라는 글자 쌍이 존재합니다.

치환된 글자도 글자 싸엥 포함 될 수 있으므로 ‘AB’를 ‘C’로 치환합니다.

더 이상 치환할 수 있는 글자 쌍이 존재하지 않으므로 입력 데이터를 더 이상 압축할 수 없습니다.

그러므로 ‘abracadabra’는 ‘CcadC’로 압축됩니다.

토크나이저로써 바이트 페어 인코딩은 자주 등장하는 글자 쌍을 찾아 치환하는 대신 어휘 사전에 추가됩니다.

말뭉치에서 바이트 페어 인코딩을 적용을 하게 된다면, 말뭉치에서 각 단어가 등장한 빈도를 계산해 다음과 같은 빈도 사전과 어휘 사전을 만들었다고 가정해 봅시다.

- 빈도 사전: ; (’low’, 5), (’lower’, 2), (’newest’, 6), (’widest’, 3)
- 어휘 사전: [’low’, ‘lower’, ‘newest’, ‘widest’]

빈도 사전을 보면 말무잋에 ‘low’, ‘lower’, ‘newest’, ‘widest’가 각각 5번, 2번, 6번, 3번 등장했다는 것을 알 수 있습니다.

빈도 사전을 바이트 페어 인코딩으로 재구성한다고 가정해 보면. 이 알고리즘을 적용하기 위해 빈도 사전 내 모든 단어를 글자 단위로 나눕니다.

- 빈도 사전: (’l’, ‘o’, ‘w’, 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)
- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']

빈도 사전을 기준으로 가장 자주 등장한 글자 쌍을 찾습니다.

빈도 사전에서 ‘e’, ‘s’ 쌍이 ‘newset’ 에서 6번 ‘widest’에서 3번 등장해 총 9번으로 가장 많이 등장했습니다.

빈도 사전에는 ‘e’, ‘s’를 ‘es’로 병합하고 어휘 사전에서 ‘es’를 추가합니다.

- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'es', 't', 6), ('w', 'i', 'd', 'e's', 't', 3)
- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es']

동일한 과정을 다시 반복합니다.

빈도 사전에서 빈도수가 가장 높은 ‘es’와 ‘t’쌍을 ‘est’로 병합하고 ‘est’를 어휘 사전에 추가합니다.

- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)
- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est']

BPE 알고리즘을 사용해 말무잋에서 자주 등장하는 글자 쌍을 찾아 어휘 사전을 구축해봤습니다.

따라서 ‘newer’, ‘wider’, ‘lowest’와 같이 기존 말뭉치에 등장하지 않았던 단어가 입력되더라도 OOV로 처리 되지 않고 기존 어휘 사전을 참고해 ‘new e r’, ‘wid e r’, ‘low est’와 같이 토큰화할 수 있습니다.

대량의 말뭉치를 사용해 토크나이저를 학습한다면 효율적인 학습을 위해 이미 구현되어 있는 토크나이저 라이브러리를 사용합니다.

#### 센텐스피스

센텐스피스(sentencepiece) 라이브러리와 코포라(Korpora) 라이브러리를 활용해 토크나이저를 학습을 해보겠습니다.

센텐스피스 라이브러리는 구글에서 개발한 오픈소스 하위 단어 토크나이저 라이브러리 입니다.

바이트 페어 인코딩과 유사한 알고리즘을 사용해 입력 데이터를 토큰화하고 단어 사전을 생성합니다.

워디피스 유니코드 기반의 다양한 알고리즘을 지원하며 사용자가 직접 설정할 수 있는 하이퍼파라미터들을 제공해 세밀한 토크나이징 기능을 제공합니다.

코포라 라이브러리는 국립국어원이나 AIHub에서 제공하는 말뭉치 데이터를 쉽게 사용할 수 있게 제공하는 오픈서스 라이브러리 입니다.

#### 토크나이저 모델 학습

센텐스피스 라이브럴리르 통해 바이트 페어 인코딩을 수행하는 토크나이저 모델을 학습해 봅시다.

코포라 라이브러리를 통해 2017년 8월 부터 2019년 3월 가지 청와대 청원 게시판에 올라온 청원 말뭉치를 가져옵니다.

```python
from Korpora import Korpora

corpus = Korpora.load("korean_petitions")
dataset = corpus.train
petition = dataset[0]

print("청원 시작일: ", petition.begin)
print("청원 종료일: ", petition.end)
print("청원 동의 수: ", petition.num_agree)
print("청원 범주: ", petition.category)
print("청원 제목: ", petition.title)
print("청원 본문: ", petition.text[:30])
```

```python
# 결과

청원 시작일: 2017-08-25
청원 종료일: 2019-09-24
청원 동의 수: 88
청원 범주: 육아/교육
청원 제목: 학교는 인력세넡, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.
청원 본문: 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비
```

코포라 라이브럴리는 말뭋이 불러오기(Korpora.load)를 통해 말뭉치 데이터를 다운로드 할 수 있습니다.

이를 통해 청와대 청원 데이터를 다운로드 합니다.

corpus는 총 433,631개의 청원 데이터가 저장돼 있으며, train 속성으로 학습 데이터를 가지고 올 수 있습니다.

첫 번째 청원 데이터는 dataset[0]으로 불러올 수 있으며, petition에 청원 시작일, 동의 수, 제목 등의 정보가 포함돼 있습니다.

```python
from Korpora import Korpora

corpus = Korpora.load("korean_petitions")
petitions = corpus.get_all_texts()

with open("datasets/corpus.txt", "w", encoding = "utf-8") as f:
    for petition in petitions:
        f.write(petition + "\n")
```

corpus의 get_all_texts 메서드로 본문 데이터세트를 한 번에 불러올 수 있습니다.

청원 데이터를 하나의 텍스트 파일로 저장합니다.

저장된 corpus.txt 파일을 활용해 센텐스피스 라이브러리 SentencePieceTrainer로 토크나이저 모델 학습을 진행 해보겠습니다.

```python
from sentencepiece import SentencePieceTrainer

SentencePieceTrainer.Train(
    "--input=datasets/corpus.txt --model_prefix=petition_bpe --vocab_size=8000 model_type=bpe"
)
```

센텐스피스 라이브럴리는 SentencePieceTrainer 클래스의 Train 메서드로 토크나이저 모델을 학습 할 수 있습니다.

| 매개변수 | 의미 |
| --- | --- |
| input | 말뭉치 텍스트 파일의 경로 |
| model_prefix | 모델 파일 이름 |
| vocab_size | 어휘 사전 크기 |
| character_coverage | 말뭉치 내에 존재하는 글자 중 토크나이저가 다룰 수 있는 글자의 비율 |
| model_type | 토크나이저 알고리즘
- unigram
- bpe
- char
- word |
| max_sentence_length | 최대 문장 길이 |
| unk_id | 어휘 사전에 없는 OOV를 의미하는 unk 토큰의 id (기본값: 0) |
| bos_id | 문장이 시작되는 시점을 의미하는 bos 토큰의 id (기본값: 1) |
| eos_id | 문장이 끝나는 지점을 의미하는 eos 토큰의 id (기본값: 2) |

센텐스피스 라이브러리에서 지원하는 모든 하이퍼파라미터는 깃허브에서 확인 할 수 있습니다.

토크나이저 모델 학습이 완료되면 petition_bpe.model 파일과 petition_bpe.vocab 파일이 생성됩니다.

model 파일은 학습된 토크나이저가 저장된 파일이며, vocab 파일은 어휘 사전이 저장된 파일입니다.

어휘 사전 파일을 열어보면 밑줄 문자(underscore)가 포함된 데이터를 볼 수 있습니다.

센텐스피스 라이브러리는 띄어쓰기가 공백도 특수문자로 취급해 토큰화 과정에서 ‘_’(U+2581)로 공백을 표현합니다.

‘Hello World’라는 문장은 ‘Hello_World’로 표현되며 토큰화하면 ‘_Hellow + _Wor + ld’로 토큰화 됩니다.

이제 토크나이저 모데로가 어휘 사전 파일을 활용해 바이트 페어 인코딩을 수행해볼 것입니다.

```python
from sentencepiece import SentencePieceProcessor

tokenizer = SentencePieceProcessor()
tokenizer.load("petition_bpe.model")

sentence = "안녕하세요, 토크나이저가 잘 학습됐군요!"
sentences = ["이렇게 입력값을 리스트로 받아서", "쉽게 토크나이저를 사용할 수 있습니다!"]

tokenized_sentence = tokenizer.encode_as_pieces(sentence)
tokenized_sentences = tokenizer.encode_as_pieces(sentences)
print("단일 문장 토큰화 :", tokenized_sentence)
print("여러 문장 토큰화 :", tokenized_sentences)

encoded_sentence = tokenizer.encode_as_ids(sentence)
encoded_sentences = tokenizer.encode_as_ids(sentences)
print("단일 문장 정수 인코딩 :", encoded_sentence)
print("여러 문장 정수 인코딩 :", encoded_sentences)

decode_ids = tokenizer.decode_ids(encoded_sentences)
decode_pieces = tokenizer.decode_pieces(encoded_sentences)
print("정수 인코딩에서 문장 변환 :", decode_ids)
print("하위 단어 토큰에서 문장 변환 :", decode_pieces)
```

```python
# 결과

단일 문장 토큰화 : ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '됐', '군요', '!']
여러 문장 토큰화 : [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있습니다', '!']]
단일 문장 정수 인코딩 : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 7120, 787, 6648]
여러 문장 정수 인코딩 : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 70, 6648]]
정수 인코딩에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있습니다!']
하위 단어 토큰에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있습니다!']
```

센텐스피스 토크나이저 모델은 SentencePieceProcessor 클래스를 통해 학습된 모델을 불러올 수 있습니다.

토크나이저 모델 불러오기(tokenizer.load) 메서드를 통해 petition_bpe.model 모델을 불러옵니다.

encode_as_pieces 메서드는 문장을 토큰화하며, encode_as_ids 메서드는 토큰을 정수로 인코딩해 제공합니다.

이 정수 데이터는 어휘 사전의 토큰에 매핑된 ID 값을 의미합니다.

이 ID 값을 활용해 자연어 처리 모델을 구축합니다.

토크나이저 모델이나 자연어 처리 모델에서 나온 정수는 decode_ids 메서드나 decode_pieces 메서드를 통해 문자열 데이터로 변환할 수 있습니다.

어휘 사전을 한번 불러와 보겠습니다.

```python
from sentencepiece import SentencePieceProcessor

tokenizer = SentencePieceProcessor()
tokenizer.load("petition_bpe.model")

vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}
print(list(vocab.items())[:5])
print("vocab size: ", len(vocab))
```

```python
# 결과

[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]
vocab size:  8000
```

get_piece_size 메서드는 센텐스피스 모델에서 생성된 하위 단어의 개수를 반환하며, id_to_piece 메서드는 정숫값을 하위 단어로 변환하는 메서드 입니다.

그러므로 하위 단어의 개수만큼 반복해 하위 단어 딕셔너리를 구성합니다.

토큰 딕셔너리를 출력한다면 <nuk>, <s>, </s>가 존재하는 것을 확인할 수 있습니다.

<unk> 토큰은 unknown의 약자로 OOV 발생 시 매핑되는 토큰입니다.

<s>와 </s>는 문장의 시작 지점과 종료 지점을 표시하는 토큰입니다.

### 워드피스

워드피스(Wordpiece) 토크나이저는 바이트 페어 인코딩 토크나이저와 유사한 방법으로 학습됩니다.

하지만 빈도 기반이 아니라 확률 기반으로 글자 쌍을 병합합니다.

워드피스는 학습 과정에서 확률적인 정보를 사용합니다.

모델이 새로운 하위 단어를 생성할 때 이전 하위단어와 함께 나타날 확률을 계산해 가장 높은 확률을 가진 하위 단어를 선택합니다.

이렇게 선택된 하위 단어는 이후에 더 높은 확률로 선택될 가능성이 높으며, 이를 통해 모델이 좀 더 정확한 하위 단어로 분리할 수 있습니다.

각 글자 쌍에 대한 점수는 다음 수식과 같습니다.

$$
score = \frac{f(x, y)}{f(x), \ f(y)}
$$

$f$는 빈도(frequency)를 나타내는 함수이며, $x$와 $y$는 병합하려는 하위 단어를 의미합니다.

그러므로 $f(x, y)$는 $x$와 $y$가 조합된 글자 쌍의 빈도를 의미합니다.

즉, $xy$글자 싸으이 빈도가 됩니다.

그러므로 $score$는 $x$와 $y$를 병합하는 것이 적절한지 판단하기 위한 점수가 됩니다. (병합이 적절한지 판단함)

이 수식을 적용해 워드피스의 어휘 사전 구축 방법을 알아 볼 수 있습니다.

바이트 페어 인코딩의 예시로 사용한 말뭉치를 활용해 빈도 사전과 어휘 사전을 구축했다고 가정해 봅시다.

워드피스도 바이트 페어 인코딩과 마찬가지로 빈도 내의 모든 단어를 글자 단위로 나눕니다.

- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)
- 어휘 사전: ['d', 'e', 'i', 'l', 'i', 'n', 'o', 'r', 's', 't', 'w']

가장 빈번하게 등장한 쌍은 9번 등정한 ‘e’와 ‘s’입니다.

하지만 ‘e’는 17번, ‘s’는 9번 등장해 점수는 $\frac{9}{17 \times 9} \simeq 0.06$가 됩니다.

‘i’와 ‘d’ 쌍은 3번밖에 등장하지 않았지만 ‘i’, ‘d’가 각각 3번씩 등장하므로 점수는 $\frac{3}{3 \times 3} \simeq 0.33$이 됩니다.

따라서 ‘e’와 ‘s’ 쌍 대신 ‘i’와 ‘d’ 쌍을 병합합니다.

- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'id', 'e', 's', 't', 3)
- 어휘 사전: ['d', 'e', 'i', 'l', 'i', 'n', 'o', 'r', 's', 't', 'w', ‘id’]

동일한 과정을 반복해 각 글자 쌍에 대한 점수를 계산합니다.

‘l’과 ‘o’글자 쌍이 3번 등장하고, ‘l’과 ‘o’가 각각 7번 등장하므로 점수가 $\frac{7}{7 \times 7} \simeq 0.14$로 가장 높아 ‘l’과 ‘o’글자 쌍을 병합합니다.

- 빈도 사전: ('lo', 'w', 5), ('lo', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'id', 'e', 's', 't', 3)
- 어휘 사전: ['d', 'e', 'i', 'l', 'i', 'n', 'o', 'r', 's', 't', 'w', ‘id’, ‘lo’]

워드피스 토크나이저는 파이트 페어 인코딩 토크나이저와 마찬가지로 이러한 과정을 반복해 연속된 글자 쌍이 더 이상 나타타지 않거나 정해진 어휘 사전크기에 도달할 때 까지 학습합니다.

#### 토크나이저

토크나이저스 라이브러리의 워드피스 API를 이용하면 쉽고 빠르게 토크나이저를 구현하고 학습할 수 있습니다.

토크나이저스 라이브러리는 정규화(Normalization)와 사전 토큰화(Pre-tokenization)를 제공합니다.

정규화는 일관된 형식으로 텍스트를 표준화하고 모호한 경우를 방지하기 위해 일부 분자를 대체하거나 제거하는 등의 작업을 수행합니다.

불필요한 공백 제거, 대소문자 변환, 유니코드 정규화, 구두점 처리, 특수문자 처리 등을 제공합니다.

사전 토큰화는 입력 문장을 토큰화하기 전에 단어와 같은 작은 단위로 나누는 기능을 제공합니다.

공백혹은 구두점을 기준으로 입력 문장을 나눠 텍스트 데이터를 효율적으로 처리하고 모델의 성능을 향상시킬 수 있습니다.

토크나이저스 라이브러리를 활용해 워드피스 토크나이저를 학습할 수 있습니다.

학습에 사용되는 말뭉치는 바이트 페어 인코딩 토크나이저 학습에 사용된 말뭉치를 사용했습니다.

```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.normalizers import Sequence, NFD, Lowercase
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(WordPiece())

tokenizer.normalizer = Sequence([NFD(), Lowercase()])
tokenizer.pre_tokenizer = Whitespace()

tokenizer.train(["datasets/corpus.txt"])
tokenizer.save("petition_wordpiece.json")
```

```python
# 결과

[00:00:52] Pre-processing files (541 Mo)  ███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                100%
[00:00:13] Tokenize words                 ███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ 3813805  /  3813805
[00:01:03] Count pairs                    ███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ 3813805  /  3813805
[00:00:43] Compute merges                 ███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ 22526    /    22526
```

토크나이저스 라이브러리의 토크나이저(Tokenizer)로 워드피스(WordPiece) 모델을 불러옵니다.

모델을 불러왔다면 정규화 방식과 사전 토큰화 방식을 설정합니다.

정규화 방식은 normalizers 모듈에 포함된 클래스를 불러와 시퀀스(Sequence) 형식으로 인스턴스를 전달합니다.

위의 코드에 적용된 정규화 방식은 NFD 유니코드 정규화(NFD), 소문자 변환(Lowercase)을 사용했습니다.

사전 토큰화 방식도 pre_tokenizers 모듈에 모함된 클래스를 불러와 적용합니다.

위의 코드에 적용된 사전 토큰화 방식은 공배고가 구두점을 기준으로 분리했습니다.

정규화 방식과 사전 토큰화 방식 설정이 완료됐다면, 훈련(train) 메서드를 통해 학습에 사용하려는 데이터세트 경로를 전달합니다.

학습이 완료됐다면 저장(save) 메서드로 학습 결과를 저장합니다.

정규화 클래스

| 이름 | 설명 |
| --- | --- |
| NFD() | NFD 유니코드 정규화 |
| NFKD() | NFKD 유니코드 정규화 |
| NFC() | NFC 유니코드 정규화 |
| NFKC() | NFKC 유니코드 정규화 |
| Lowercase() | 입력 문장의 모든 대문자를 소문자로 변환 |
| Strip() | 입력 문장의 양 끝에 있는 공백 제거 |
| StripAccents() | 모든 엑센트 기호 제거 |
| Replace() | 문자 혹은 정규 표현식을 기반으로 입력 문장 변환 |
| BertNormalizer() | BERT 모델에 사용된 정규화 적용 |
| Sequence() | 여러 개의 정규화 모듈을 병합해 순서대로 수행 |

사전 토큰화 클래스

| 클래스  | 설명 |
| --- | --- |
| Sequence() | 여러개의 사전 토큰화 모듈을 병합하여 순서대로 수행 |
| ByteLevel() | Open AI에서 GPT-2를 학습할 때 사용한 방법으로 문자열을 그래픽 문자열로 매핑하고 공백을 기준으로 나눔 |
| CharDelimiterSplit() | 문자 기준으로 문자열을 나눔 |
| Digits() | 모든 형태의 숫자 표현을 기준으로 문자열을 나눔 |
| Punctuation() | 구두점 기준으로 입력 문자열을 나눔 |
| Metaspace(replacement = “_”) | Whitespace 기준으로 사전 토큰화를 진행하고, replacement로 Whitespace를 치환함 <br> - 기본값: _(U+2581) |
| Whitespace() | 단어 경계(공백과 구두점)를 기준으로 사전 토큰화를 진행함 <br> - \w+[^\w\s]+ 정규 표현식을 적용함|
| WhitespaceSplit() | 공백 문자를 기준으로 입력 문자열을 나눔 |
| Split(pattern, behavior) | 정규 표현식(pattern)과 동작(behavior)을 기준으로 문자열을 나눔 <br> - behavior<br>    - remove: 삭제 <br> - isolated: 개별 토큰 처리 <br> - merged_with_previous: 이전 문자열과 결합해 토큰 처리 <br> - merged_with_next: 다음 문자열과 결합해 토큰 처리 - contiguous:  다음 문자열이 토큰화되지 않아도 결합해 토큰 처리 |

워드 피스 모델의 학습이 완료됐다면 petition_wordpiece.json 파일이 생성됩니다.

학습 결과는 JSON 형태로 저장되며 정규화 및 사전 토큰화 등의 메타데이터와 함께 어휘 사전이 저장됩니다.

```python
from tokenizers import Tokenizer
from tokenizers.decoders import WordPiece as WordPieceDecoder

tokenizer = Tokenizer.from_file("petition_wordpiece.json")
tokenizer.decoder = WordPieceDecoder()

sentence = "안녕하세요, 토크나이저가 잘 학습되었군요!"
sentences = ["이렇게 입력값을 리스트로 받아서", "쉽게 토크나이저를 사용할 수 있습니다"]

encode_sentence = tokenizer.encode(sentence)
encode_sentences = tokenizer.encode_batch(sentences)

print("인코더 형식", type(encode_sentence))

print("단일 문장 토큰화: ", encode_sentence.tokens)
print("여러 문장 토큰화: ", [enc.tokens for enc in encode_sentences])

print("단일 문장 정수 인코딩: ", encode_sentence.ids)
print("여러 문장 정수 인코딩: ", [enc.ids for enc in encode_sentences])

print("정수 인코딩에 문장 변환: ", tokenizer.decode(encode_sentence.ids))
```

```python
# 결과

단일 문장 토큰화:  ['안녕하세요', ',', '토', '##크', '##나이', '##저', '##가', '잘', '학습', '##되었', '##군요', '!']
여러 문장 토큰화:  [['이렇게', '입력', '##값을', '리스트', '##로', '받아서'], ['쉽게', '토', '##크', '##나이', '##저', '##를', '사용할', '수', '있습니다']]
단일 문장 정수 인코딩:  [8760, 11, 8693, 8415, 16269, 7536, 7488, 7842, 15016, 8670, 8734, 0]
여러 문장 정수 인코딩:  [[8187, 19643, 13834, 28119, 7495, 12607], [9739, 8693, 8415, 16269, 7536, 7510, 14129, 7562, 7698]]
정수 인코딩에 문장 변환:  안녕하세요, 토크나이저가 잘 학습되었군요!
```

워드피스 토큰화를 수행하기 위해 모델 결과가 저장된 petition_wordpiece.json 파일을 불러와 Tokenizer 객체를 생성합니다.

그리고 WordPieceDecoder()를 사용해 Tokneizer의 디코더를 워드피스 디코더로 설정합니다.

인코딩(encode) 메서드로 문장을 토큰화할 수 있으며, 인코딩 배치(encode_batch) 메서드로 여러 문장을 한 번에 토큰화할 수 있습니다.

인코딩 메서드와 인코딩 배치 메서드 모두 워드피스 토크나이저를 통해 문장을 토큰화하고 각 토큰의 색인 번호를 반환합니다.

토큰화된 데이터는 토큰(tokens) 속성을 통해 값을 확인할 수 있으며, 토큰 정수(ids) 속성으로 인코딩된 문장의 ID 값을 출력할 수 있습니다.

저수를 다시 문장으로 변환하는 경우 디코딩(decode) 메서드를 통해 정수 인코딩된 결과를 다시 문장으로 디코딩해 출력할 수 있습니다.

최근 연구 동향은 더 큰 마뭉치를 사용해 모델을 학습하고 OOV의 위험을 줄이기 위해 하위 단어 토큰화를 활용합니다.

유니코드 단위가 아닌 바이트 단위에서 토큰화하는 바이트 수준 바이트 페어 인코딩(Byte-level Byte-Pair-Encoding, BBPE)이나

크기가 큰 어휘 사전에서 덜 필요한 토큰을 제거하며 학습하는 유니그래(Unigram) 등이 있습니다.

# 6. 임베딩
토큰화만으론느 모델을 학습할 수 없습니다.

컴퓨터는 텍스트 자체를 이해할 수 없어서 텍스트를 숫자로 변환하는 텍스트 텝터화(Text Vectorization) 과정이 필요합니다.

텍스트 벡터화란 텍스트를 숫자로 변환하는 과정을 의미합니다.

기초적인 텍스트 벡터화로는 원-핫 인코딩(One-Hot Encoding), 빈도 벡터화(Count Vectorization) 등이 있습니다.

원-핫 인코딩이란 문서에 등장하는 각 단어를 고유한 색인 값으로 매핑한 후, 해당 색인 위치를 1로 표시하고 나머지 위치는 모두 0으로 표시하는 방식입니다.

eg) ‘I like apple’ 문장과 ‘I like bananas’ 문장을 띄어쓰기 기준으로 토큰화하고 단어를 고유한 색인 값으로 매핑합니다.

| 색인 | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| 토큰 | I | like | apples | bananas |

이를 바탕으로 각 문장을 원-핫 인코딩한다면

- I like apples: [1, 1, 1, 0]
- I like bananas: [1, 1, 0, 1]

이렇게 표현이 됩니다.

빈도 벡터화는 문서에서 단어의 빈도수를 세어 해당 단어의 빈도를 벡터로 표현하는 방식입니다.

eg) apple라는 단어가 총 4번 등장한다며, 해당 단어에 대한 벡터값은 4가 됩니다.

원-핫 인코딩 방식은 같은 단어가 여러 번 등장하더라도 1과 0으로 표현하지만, 빈도 벡터화는 해당 단어의 빈도로 표시됩니다.

이러한 방법은 단어나 문장을 벡터 형태로 변환하기 쉽고 간단하다는 장점이 있지만, 벡터의 희소성(Sparsity)이 크다는 단점도 있습니다.

물뭉치 내에 존재하는 토큰의 개수만큼의 벡터 차원을 가져야 하지만, 입력 문장 내에 존재하는 토큰의 수는 그에 비해 현저히 적기 때문에 컴퓨팅 비용의 증가와 차원의 저주와 같은 문제를 겪을 수 있습니다.

→ 가지고 있는 말뭉치(양많음)가 있는데 입력 문장(말뭉치 보단 적음)

말뭉치 내에 존재하는 토큰의 개수만큼의 벡터 차원을 가져야만 하지만, 입력 문장 내에 존재하는 토큰의 수는 그에 비해 현저히 적기 때문에 컴퓨팅 비용의 증가와 차원의 저주와 같은 문제를 겪을 수 있습니다.

또한 텍스트의 벡터가 입력 텍스트의 의미를 내포하고 있지 않으므로 두 문장이 의미적으로 유사하다고해도 벡터가 유사하게 나타나지 않을 수 있습니다.

eg) ‘i scream for ice cream’과 ‘ice cream for i scream’에 원-핫 인코딩이나 빈도 벡터화를 적용하면 둘 다 [1, 1, 1, 1, 1] 벡터로 반환됩니다.

이러한 문제를 해결하기 위해 워드 투 벡터(Word2Vec)나 패스트 텍스트(fast Text) 등과 같이 단어의 의미를 학습해 표현하는 워드 임베딩(Word Embedding) 기법을 사용합니다.

## 언어 모델

언어 모델(Language Model)이란 입력된 문장으로 각 문장을 생성할 수 있는 확률을 계산하는 모델을 의미합니다.

이를 위해 주어진 문장을 바탕으로 문맥을 이해하고, 문장 구성에 대한 예측을 수행합니다.

주로 자동 번역, 음성 인식, 텍스트 요약 등 다양한 자연어 처리 분야에서 활용되고 있습니다.

주어진 문장 뒤에 나올 수 있는 문장은 매우 다양하기 때문에 완성된 문장 단위로 확률을 계산하는 것은 어려운 일입니다.

이러한 문제를 해결하기 위해 문장 전체를 예측하는 방법 대신에 하나의 토큰 단위로 예측하는 방법인 자기회귀 언어 모델이 고안됐습니다.

### 자기회귀 언어 모델 $(\simeq Greedy search? \simeq Beam search?)$

자기회귀 언어 모델(Autoregressive Language Model)은 입력된 문장들의 조건부 확률을 이용해 다음에 올 단어를 예측합니다.

즉, 언어 모델에서 조건부 확률은 이전 단어들의 시퀀스가 주어졌을 때, 다음 단어의 확률을 계산하는 것을 의미합니다.

이를 위해 이전에 등장한 모든 토큰의 정보를 고려하며, 문자으이 문맥 정보를 파악하여 다음 단어를 생성 합니다.

다음 단어는 다시 이전 단어를 기반으로 예측이 이루어지며 이 과정을 반복합니다.

언어 모델의 조건부 확률 수식화

$$
P(w_t|w_1, w_2, \cdots , w_{t-1}) = \frac{P(w_1, w_2, \cdots , w_{t})}{P(w_1, w_2, \cdots , w_{t-1})}
$$

언어 모델에서 조거눕 확률을 계산하기 위해 이전에 등장한 단어 시퀀스 $(w_1, w_2, \cdots , w_{t-1})$를 기반으로 다음 단어($w_t$)의 확률을 계산합니다.

이전 수식에서 조건부 확률의 연쇄법칙(Chain rule for conditional probability)을 적용한다면 다음 수식과 같습니다.

$$
P(w_t|w_1, w_2, \cdots , w_{t-1})  = P(w_1)P(w_2|w_1) \cdots P(w_t|w_1, w_2, \cdots , w_{t-1})
$$

언어 모델에서 조건부 확률은 연쇄법칙을 이용해 계산됩니다.

조건부 확률의 연쇄법칙은 하나의 사건이 일어날 확률을 다른 사건들의 조건부 확률을 이용해 계산하는 방식으로 이전 단어들의 시퀀스가 주어졌을 때, 다음에 등장하는 단어의 확률을 이전 단어들의 조건부 확률을 이용해 계산합니다.

언어 모델에서 조건부 확률은 위 수식과 같이 표현됩니다.

문장 전체의 확률은 단어의 확률 $P(w_1)$과 각 단어가 이전 단어들의 조건부 확률에 따라 발생할 확률의 곱으로 나타낼 수 있습니다.

이를 통해 언어 모델은 문장 전체의 확률을 계산하고, 그것을 이용해 다음 단어를 예측합니다.

‘안녕하세요’ 다음에 등장할 수 있는 문장을 조건부 확률의 연쇄법칙 방법으로 시각화하면 다음 그림과 같습니다.

![image_1.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/42481cbe-9a93-4ded-962a-2932560f66b9/image_1.png)

자기회귀 언어 모델에서는 각 시점에서 다음에 올 토큰을 예측하는 것이 중요합니다.

이를 위해 입력 토큰 $w_1$을 바탕으로 모델은 다음 토큰 $w_2$가 등장할 확률 $P(w_2 | w_1)$을 예측합니다

그다음 모델의 출력값 $w_2$가 다시 입력값이 되어, 모델은 확률 $P(w_3 | w_1, w_2)$를 예측합니다.

이러한 방식으로 모델의 출력값이 모델의 입력값으로 사용되는 특징 떄문에 자기회귀라는 이름이 붙었습니다.

자기회귀 언어 모델은 시점별로 다음에 올 토큰을 예측하는 것이므로, 토큰 분류 문제로 정의할 수 있습니다.

자기회귀 모델은 이전에 등장한 모든 토큰의 정보를 활용해 입력된 문장의 문맥 정보를 파악하고 다음 토큰을 예측합니다.

그러므로 언어 모델은 문장 전체의 확률을 계산하고, 이를 이용해 다음 단어를 예측합니다.

### 통계적 언어 모델

통계적 언어 모델(Statistical Language Model)은 언어의 통계적 구조를 이용해 문장이나 단어의 시퀀스를 생성하거나 분석합니다.

시퀀스에 대한 확률 분포를 추정해 문장의 문맥을 파악해 다음에 등장할 단어의 확률을 예측합니다.

일반적으로 통계적 언어 모델은 마르코프 체인(Markov Chain)을 이용해 구현됩니다.

마르코프 체인은 빈도 기반의 조건부 확률 모델 중 하나로 이전 상태와 현재 상태간의 전이 확률을 이용해 다음 상태를 예측합니다.

빈도 기반의 조건부 확률 모델에서는 주어진 데이터에서 각 변수가 발생한 빈도수를 기반으로 확률을 계산합니다.

eg) 말뭉치에 ‘안녕하세요’라는 문장이 1000번 등장하고 이어서 ‘안녕하세요 만나서’가 700번, ‘안녕하세요 반갑습니다’가 100번 등장했다고 가정하면 빈도의 기반 조건부 확률 수식은 다음과 같이 표현됩니다.

$$
P(만나서|안녕하세요) = \frac{P(안녕하세요 \ 만나서)}{P(안녕하세요)} = \frac{700}{1000}
$$

$$
P(반갑습니다|안녕하세요)= \frac{안녕하세요 \ 반갑습니다)}{P(안녕하세요)} = \frac{100}{1000}
$$

이 방법은 단어의 순서와 빈도에만 기초해 문장의 확률을 예측하므로 문맥을 제대로 파악하지 못 하면 불완전 하거나 부적절한 결과를 생성할 수 있습니다.

한 번도 등장하지 않은 단어나 문장에 대해서는 정확한 확률을 예측하기 어렵습니다.

이렇게 관측한 적이 없는 데이터를 예측하지 못 하는 문제를 데이터 희소성(Data sparsity) 문제라고 합니다.

→ 관측한 적이 없는 데이터는 예측할 수 없다

하지만 통계적 언어 모델은 기존에 학습한 텍스트 데이터에서 패턴을 찾아 확률 분포를 생성하므로, 이를 이용하여 새로운 문장을 생성할 수 있고, 다양한 종류의 텍스트 데이터를 학습할 수 있습니다.

통계적 언어 모델은 대규모 자연어 데이터를 처리하는데 효과적이며, 딥러닝 등의 인공지능 기술이 발전하면서 더욱 강력한 모델을 구현할 수 있게 됩니다.

최근 연구되는 자연어 처리 기법은 언어 모델을 활용해 가중치를 사전 학습합니다.

eg) 트랜스포머 모델에 기반한 언어 모델인 GPT(Generative Pre-trained Transformer)나 BERT(Bidirectional Encoder Representations from Transformers)는 다음과 같은 문장 생성 기법을 이용해 모델을 사전 학습합니다.

GPT

- Raw Sentence: GPT는 Open AI에서 개발한 인과적 언어 모델입니다.
- Input: GPT는 Open AI에서 개발한
- prediction - 1: 인과적
- prediction - 2: 언어 모델입니다.
- 문장 생성
- 다음 단어가 무엇인지 맞히는 과정
- 문장의 왼쪽부터 오른쪽으로 순차적으로 계산
- 일반향: unidirectional

BERT

- Raw Sentence: BERT는 Google에서 개발한 마스킹된 언어 모델 입니다.
- Input:  BERT는 [MASK]에서 개발한 [MASK] 언어 모델입니다.
- predictiona: Google, 마스킹된
- 문장의 의미 추출 → 번역
- 빈칸에 어떤 단어가 적절한지 맞히는 과정
- 빈칸 앞뒤 문맥을 모두 살필 수 있다는 점
- 양방향: bidirectional

GPT나 BERT 모델과 같이 신경망을 통해 확률 P를 계산할 수도 있지만, 통계적인 방법을 통해 확률을 계산할 수도 있습니다.

## N-gram

가장 기초적인 통계적 언어 모델은 N-gram 모델입니다.

N-gram 모델은 텍스트에서 N개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정합니다.

N-gram 모델은 입력 텍스트를 하나의 토큰 단위로 분석하지 않고 N개의 토큰을 묶어서 분석합니다.

연속된 N개의 단어를 하나의 단위로 취급하여 추론하는 모델이며, N이 1일 때는 유니그랩(Unigram), N이 2일 때는 바이그램(Bigram), N이 3일 때는 트라이그램(Trigram)으로 부릅니다.

N이 4 이상이면 N-gram이라고 부릅니다.

![image (9).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/991c2ec3-0dd1-4464-81dc-7ae9b465912e/image_(9).png)

N-gram 언어 모델은 모든 토큰을 사용하지 않고 N-1개의 토큰만을 고려해 확률을 계산합니다

각 N-gram 언어 모델에서 t번째 토큰의 조건부 확률을 계산하는 수식은 다음과 같습니다.

$$
P(w_t|w_{t-1}, w_{t-2}, \cdots, w_{t-N+1})
$$

$w_t$는 예측하려는 단어, $w_{t-1}$부터 $w_{t-N+1}$까지는 예측에 사용되는 이전 단어들을 의미합니다.

이전 단어들의 개수를 결정하는 $N$의 값을 조정해 N-gram 모델의 성능을 조절할 수 있습니다.

```python
import nltk

def ngrams(sentence, n):
    words = sentence.split()
    ngrams = zip(*[words[i: ] for i in range(n)])
    return list(ngrams)

sentence = "안녕하세요 만나서 진심으로 반가워요"

unigram = ngrams(sentence, 1)
bigram = ngrams(sentence, 2)
trigram = ngrams(sentence, 3)

print(unigram)
print(bigram)
print(trigram)

unigram = nltk.ngrams(sentence.split(), 1)
bigram = nltk.ngrams(sentence.split(), 2)
trigram = nltk.ngrams(sentence.split(), 3)

print(list(unigram))
print(list(bigram))
print(list(trigram))
```

```python
# 결과

[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]
[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]
[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]
[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]
[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]
[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]
```

통계적 언어 모델은 상대적으로 구현이 쉽습니다.

간략한 파이썬 코드로 구현한 N-gram과 NLTK 라이브러리에서 지원하는 N-gram 함수는 동일한 출력값을 반환합니다.

N-gram은 작은 규모의 데이터세트에서 연속된 문자열 패턴을 분석하는데 큰 효과를 보이고, 관용적 표현 분석에서도 활용됩니다.

eg) ‘입이 무겁다’라는 표현은 ‘비밀을 잘 지킨다’라는 뜻이 있습니다.

그러므로 자주 등장하는 연속된 단어나 구를 추출하고, 이를 분석함으로써 관용적 표현을 파악할 수 있습니다.

## TF-IDF

TF-IDF(Term Frequency-Inverse Document Frequency)란 텍스트 문서에서 특정 단어의 중요도를 계산하는 방법으로, 문서 내에서 단어의 중요도를 평가하는데 사용되는 통계적인 가중치를 의미합니다.

TF-IDF는  BoW(Bag-of-Words)에 가중치를 보여하는 방법입니다.

BoW는 문서나 문장을 단어의 집합으로 표현하는 방법으로, 문서나 문장에 등장하는 단어의 중복을 허용해 빈도를 기록합니다

원-핫 인코딩은 단어의 등장 여부를 판별해 0과 1로 표현하는 방식이지만, BoW는 등장 빈도를 고려해 표현합니다.

eg) [’That movie is famous movie’, ‘I like that actor’, ‘I don’t like that actor’] 말뭉치를 BoW로 벡터화하면 다음과 같습니다.

|  | I | like | this | movie | don’t | famous | is |
| --- | --- | --- | --- | --- | --- | --- | --- |
| This movie is famous movie | 0 | 0 | 1 | 2 | 0 | 1 | 1 |
| I like this movie | 1 | 1 | 1 | 1 | 0 | 0 | 0 |
| I don’t like this movie | 1 | 1 | 1 | 1 | 1 | 0 | 0 |

BoW를 이용해 벡터화하는 경우 모든 단어는 동일한 가중치를 갖습니다.

BoW 벡터를 활용해 영화 리뷰의 긍/부정 분류 모델을 만든다고 가정한다면 높은 성능을 얻기는 어렵습니다.

‘I like this movie’와 ‘I don’t like this movie’ 문장은 ‘don’t’라는 단어가 문장 분류에 결정적인 역할을 합니다.

하지만 ‘don’t’라는 단어는 자주 등장하지 않으므로 토큰화나 분류 모델 진행 시 해당 데이터가 무시 될 수 있습니다.

### 단어 빈도

단어 빈도(Term Frequency, TF)란 문서 내에서 특정 단어의 빈도수를 나타내는 값입니다.

eg) 3개의 문서에서 ‘movie’라는 단어가 4번 등장한다면 해당 단어의 TF는 4가 됩니다.

이를 수식으로 나타내면 다음과 같습니다.

$$
TF(t, d) = count(t, d)
$$

앞선 BoW 벡터 표현 방법과 같이 문서 내에서 단어가 증당한 빈도수를 계산하며, 해당 단어의 상대적인 중요도를 측정하는데 사용됩니다.

TF 값이 높을수록 해당 단어가 특정 문서에서 중요한 역할을 한다고 생각할 수 있지만, 단어 자체가 특정 문서 내에서 자주 사용되는 단어이므로 전문 용어나 관용어로 간주할 수 있습니다.

### 문서 빈도

문서 빈도(Document Frequency, DF)란 한 단어가 얼마나 많은 문서에서 나타나는지를 의미합니다.

특정 단어가 많은 문서에서 나타나면 문서 집합에서 단어가 나타나는 횟수를 계산합니다.

eg) 3개의 문서에서 ‘movie’라는 단어가 4번 등장한다면 해당 단어의 DF 값은 3이 됩니다.

이를 수식으로 나타내면 다음과 같습니다.

$$
DF(t, D) = count(t \in d:d \in D)
$$

DF는 단어가 몇 개의 문서에서 등장하는지 계산합니다.

DF값이 높으면 특정 단어가 많은 문서에서 등장 한다고 볼 수 있습니다.

그 단어는 일반적으로 널리 사용되며, 중요도가 낮을 수 있습니다.

반대로 DF값이 낮다면 특정 단어가 적은 수의 문서에만 등장한다는 뜻으로 특정한 문맥에서만 사용되는 단어일 가능성이 있으며, 중요도가 높을 수 있습니다.

### 역문서 빈도

역문서 빈도(Inverse Document Frequency, IDF)란 전체 문서 수를 빈도로 나눈 다음에 로그를 취한 값을 말합니다.

이는 문서 내에서 특정 단어가 얼마나 중요한지를 나타냅니다.

문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는의미가 됩니다.

그러므로 문서빈도의 역수를 취하면 단어의 빈도수가 적을수록 IDF 값이 커지게 보정하는 역할을 합니다.

이를 통해 문서에서 특정 단어의 등장 횟수가 적으면 IDF는 상대적으로 커집니다.

이를 수식으로 나타내면 다음과 같습니다.

$$
IDF(t, D) = \log \left( \frac{coutn(D)}{1 + DF(t, D)} \right)
$$

IDF는 분모의 DF값에 1을 더한 값을 사용합니다.

특정 단어가 한 번도 등장하지 않는다면 분모가 0이 되는 경우가 발생합니다.

그러므로 1과 같은 작은 값을 더해 분모가 0이 되는 결과를 방지합니다.

추가로 IDF는 로그를 취합니다.

전체 문서 수를 문서 빈도로 나눈 값을 사용한다면 너무 큰 값이 나올 수 있습니다.

10000개의 문서에서 특정한 단어가 1번만 등장한다면 IDF 값은 5000이 됩니다.

이러한 문제점을 방지하고자 로그를 취해 정교한 가중치를 얻습니다.

### TF-IDF

TF-IDF는 앞선 문서 빈도와 역문서 빈도를 곱한 값으로 사용됩니다.

$$
TF-IDF(t, d, D) = TF(t, d) \times IDF(t, d)
$$

문서 내에 단어가 자주 등장하지만, 전체 문서 내에 해당 단어가 적게 등장한다면 TF-IDF 값은 커집니다.

전체 문서에서 자주 등장할 확률이 높은 관사나 관용어 등의 가중치는 낮아집니다.

사이킷런 라이브러리는 말뭉치를 쉽게 TF-IDF 형태로 변환할 수 있게 TF-IDF 클래스를 제공합니다.

```python
tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(
	input = "content",
	encodding = "utf-8",
	lowercase = True,
	stop_words = None,
	ngram_range = (1, 1),
	max_df = 1.0,
	min_df = 1,
	vocabulary = None,
	smooth_idf = True,
)
```

TF-IDF(TfidfVectorizer) 클래스의 입력값(input)은 입력될 데이터의 형태를 의미합니다.

기본값으로 설정된 content는 문자열 데이터 혹은 바이트 형태의 입력값을 의미합니다.

파일 객체를 사용한다면 file로 입력하며, 파일 경로를 사요하는 경우 filename으로 입력합니다.

인코딩(encoding)은 바이트 혹은 파일을 입력값으로 받을 경우 사용할 텍스트 인코딩 값을 의미합니다.

소문자 변환(lowercase)은 입력받은 데이터를 소문자로 변환할지 여부를 말합니다.

참값으로 설정하면 모든 입력 텍스트를 소문자로 변환합니다.

불용어(stop_words)는 분석에 도움이 되지 않는 의미 없는 단어들을 의미하며, 입력받은 단어들은 단어 사전에 추가되지 않습니다.

N-gram 범위(ngram_range)는 사용할 N-gram의 범위를 의미합니다.

(최솟값, 최댓값) 형태로 입력하며, (1, 1)은 유니그램, (1, 2)는 유니그램과 바이그램을 사용한다는 것을 의미합니다.

최대값 문서 빈도(max_df)는 전체 문서 중 일정 횟수 이상 등장한 단어는 불용어로 처리합니다.

정수를 입력하면 해당 등장 횟수를 초과해 등장하는 단어를 불용어 처리하며, 1이하의 실수를 입력하면 해당 비율을 초과해 등장한 단어를 불용어 처리합니다.

최솟값 문서 빈도(min_df)는 전체 문서 중 일정 횟수 미만으로 등장한 단어를 불용어 처리한다는 뜻으로 최댓값 문서 빈도와 동일한 패턴으로 입력할 수 있습니다.

단어사전(vocabulary)은 미리 구축한 단어사전이 있다면 해당 단어 사전을 사용합니다.

만약 입력하지 않는다면 TF-IDF 학습 시 자동으로 구축합니다.

IDF 분모처리(smooth_idf)는 $IDF(t, D) = \log \left( \frac{coutn(D)}{1 + DF(t, D)} \right)$와 같이 IDF 계산 시 분모에 1을 더합니다.

말뭉치인 [’That movie is famous movie’, ‘I like that actor’, ‘I don’t like that actor’]를 적용해 백터화하는 코드입니다.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "That movies is famous movie",
    "I like that actor",
    "I don't like that actor"
]

tfidf_vectorizer = TfidfVectorizer()
tfidf_vectorizer.fit(corpus)
tfidf_matrix = tfidf_vectorizer.transform(corpus)

print(tfidf_matrix.toarray())
print(tfidf_vectorizer.vocabulary_)
```

```python
# 결과

[[0.         0.         0.47952794 0.47952794 0.         0.47952794
  0.47952794 0.28321692]
 [0.61980538 0.         0.         0.         0.61980538 0.
  0.         0.48133417]
 [0.4804584  0.63174505 0.         0.         0.4804584  0.
  0.         0.37311881]]
{'that': 7, 'movies': 6, 'is': 3, 'famous': 2, 'movie': 5, 'like': 4, 'actor': 0, 'don': 1}
```

TF-IDF 클래스는 fit 메서드를 통해 학습해야 데이터의 변환 또는 추론이 가능합니다.

TF-IDF 클래스를 통해 tfidf_vectorizer 객체를 불러오고, fit 메서드를 통해 준비된 말뭉치를 학습합니다.

객체의 학습이 완료 되면 transform 메서드를 이용해 데이터 변환을 수행할 수 있습니다.

또는 fit_transform 메서드를 통해 학습과 변환을 동시에 수행할 수 있습니다.

toarray 메서드는 TF-IDF를 넘파이 배열로 변환하며, (문서 수) $\times$(단어 수)의 형태를 가집니다.

각 행은 하나의 문서에 해당하며, 각 열은 단어를 의미합니다.

vocabulary_ 속성은 TF-IDF에 사용된 단어 사전을 의미합니다.

딕셔너리의 키는 고유햔 단어를 의미하며, 값은 해당 단어에 대한 색인 값을 의미합니다.

TF-IDF에서 점수가 가장 높은 값을 세 개만 추려 색인으로 정리면 [[2, 3, 5], [0, 4, 6,], [0, 1, 4]]가 됩니다.

이를 단어 사전과 매핑하면 [[famous, is, movie], [actor, like that], [actor, don, like]]가 됩니다.

이를 통해 문서마다 중요한 단어만 추출할 수 있으며, 벡터값을 활용해 문서 내 핵심 단어를 추출 할 수 있습니다.

하지만 출력 결과에서 확인할 수 있듯이 빈도 기반 벡터화는 문장의 순서나 문맥을 고려하지 않습니다.

그러므로 문장 생성성과 같이 순서가 중요한 작업에는 부적합합니다.

또한, 벡터가 해당 문서 내의 중요도를 의미할 뿐, 벡터가 단어의 의미를 담고 있지는 않습니다.

eg) [’나는 바나나를 먹었다’, ‘그녀가 과일을 섭취한다’, ‘고양이는 야옹 운다’]를 벡터화 한다면 ‘나는 바나나를 먹었다’와 ‘그녀가 과일을 섭취한다’가 ‘고양이는 야옹 운다’ 보다 더 가까운 의미를 가지고 있지만, 세 문장간의 유사도는 동일합니다.

## Word2Vec

Word2Vec은 2013년 구글에서 공개한 임베딩 모델로 단어 간의 유사성을 측정하기 위해 분포 가설(distributional hypothesis)을 기반으로 개발됐습니다.

분포 가설이란 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정입니다.

분포 가설은 단어간의 동시 발생(co-occurrence) 확률 분포를 이용해 단어 간의 유사성을 측정합니다.

eg) ‘내일 자동차를 타고 부산에 간다’와 ‘내일 비행기를 타고 부산에 간다’라는 두 문장에서 ‘자동차’와 ‘비행기’는 주변에 분포한 단어들이 동일하거나 유사하므로 비슷한 의미를 가질 것이라고 예상합니다.

이러한 가정을 통해 단어의 분산 표현(Distributed Representation)을 학습할 수 있습니다.

분산 표현이란 단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담는 것을 의미합니다.

분포 가설에 따라 단어의 의미는 문맥상 분포적 특성을 통해 나타납니다.

유사한 문맥에서는 등장하는 단어는 비슷한 벡터 공간상 위치를 갖게 됩니다.

‘자동차’와 ‘비행기’라는 단어는 벡터 공간에서 서로 가까운 위치에 표현됩니다.

이러한 방법으로 빈도 기반의 벡터화 기법에서 발생했던 단어의 의미 정보를 저장하지 못 하는 한계를 극복했으며, 대량의 텍스트 데이터에서 단어 간의 관계를 파악하고 벡터 공간상에서 유사한 단어를 군집화 해 단어의 의미정보를 효과적으로 표현합니다.

분산 표현 방식은 다양한 자연어 처리 작업에서 높은 성능을 보여주며, 다운스크림 작업에서 더 뛰어난 성능을 보입니다.

### 단어 벡터화

단어를 벡터화하는 방법은 크게 희소 표현(sparse representatnion)과 밀집 표현(dense representation)으로 나눌 수 있습니다.

원-핫 인코딩, TF-IDF 등의 빈도 기반 방법은 희소 표현이며, Word2Vec은 밀집 표현입니다.

단어의 희소 표현

| 소 | 0 | 1 | 0 | 0 | 0 |
| --- | --- | --- | --- | --- | --- |
| 잃고 | 1 | 0 | 0 | 0 | 0 |
| 외양간 | 0 | 0 | 0 | 1 | 0 |
| 고친다 | 0 | 0 | 0 | 0 | 1 |

단어의 밀집 표현

| 소 | 0.3914 | -0.1749 | … | 0.5912 | 0.1321 |
| --- | --- | --- | --- | --- | --- |
| 잃고 | -0.2893 | 0.3814 | … | -0.1492 | -0.2814 |
| 외양간 | 0.4812 | 0.1214 | … | -0.2745 | 0.0132 |
| 고친다 | -0.1314 | -0.2809 | … | 0.2014 | 0.3016 |

원-핫 인코딩과 TF-IDF와 같은 방법은 대부분의 벡터 요소가 0으로 표현되는 희소 표현 방법입니다.

eg) 모델의 단어 사전이 5000개의 단어로 이루어져있다면 10개 토큰으로 이루어진 입력 텍스트를 원-핫 인코딩으로 표현하면 최소한 4990개의 0이 포함된 벡터로 표현이 됩니다.

이러한 방법은 단어 사전의 크기가 커지면 벡터의 크기도 커지므로 공간적 낭비가 발생합니다.

또한 단어간의 유사성을 반영하지 못 하고, 벡터 간의 유사성을 계산하는 데도 많은 비용이 발생합니다.

Word2Vec과 같은 밀집 표현은 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기가 커지지 않습니다.

벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수 있으며, 벡터의 대부분이 0이 아닌 실수로 이루어져 있어 효율적으로 공간을 활용할 수 있습니다.

밀집 표현 벡터화는 학습을 통해 단어를 벡터화 하기 때문에 단어의 의미를 비교할 수 있습니다.

밀집 표현된 벡터를 단어 임베딩 벡터(Word Embedding Vector)라고 하며, Word2Vec은 대표적인 단어 임베딩 기법중 하나입니다.

Word2Vec은 밀집 표현을 위해 CBoW와 Skip-gram이라는 두 가지 방법을 사용합니다.

### CBoW

CBoW(Continuous Bag of Words)란 주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법입니다.

중심 단어(Center Word)는 예측해야 할 단어를 의미하며, 예측에 사용되는 단어들을 주변 단어(Context Word)라고 합니다.

중심 단어를 맞추기 위해 몇 개의 주변 단어를 고려할지를 정해야 합니다.

이 범위를 윈도(Window)라고 합니다.

이 윈도를 활용해 주어진 하나의 문장에서 첫 버너째 단어부터 중심 단어로 하여 마지막 단어까지 학습합니다.

학습을 위해 윈도를 이동해 가며 학습하는데, 이러한 방법을 슬라이딩 윈도(Sliding Window)라 합니다.

CBoW는 슬라이딩 윈도를 사용해 한 번의 학습으로 여러 개의 중심 단어와 그에 대한 주변 단어를 학습할 수 있습니다.

다음 그림은 윈도 크기가 2일 때 학습 데이터가 어떻게 구성되는지 보여줍니다.

![화면 캡처 2024-07-31 144346.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/16820fa5-1e55-47e8-8c88-359726ab3a8c/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-07-31_144346.png)

학습 데이터는 (주변 단어 | 중심 단어)로 구성되어 있습니다.

이를 통해 대량의 말뭉치에서 효율적으로 단어의 분산 표현을 학습할 수 있습니다.

얻어진 학습 데이터는 다음 그림과 같은 구조의 인공 신경망을 학습하는데 사용됩니다.

![image (11).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/4b29eb9c-6b96-40d0-8b43-902ea14badc1/image_(11).png)

CBoW 모델은 각 이력 단어의 원-핫 벡터를 입력값으로 받습니다.

입력 문장 내 모든 단어의 임베딩 벡터를 평균내어 중심 단어의 임베딩 벡터를 예측합니다.

입력 단어는 원-핫 벡터로 표현된 투사층(Projection Layer)에 입력됩니다.

투사층이란 원-핫 벡터의 인덱스에 해당하는 임베딩 벡터를 반환하는 순람표(Lookup table, LUT) 구조가 됩니다.

투사층을 통과하면 각 단어는 E크기의 임베딩 벡터로 변환됩니다.

다음 그림에서 입력된 세 단어의 임베딩 벡터를 $V_{세상의}, V_{일들은}, V_{모두}$라고 가정한다면 이 벡터의 평균값을 계산합니다.

계산된 평균 벡터들 가중치 행렬 $W^{\prime}_{E \times V}$와 곱하면 V 크기의 벡터를 얻습니다.

이 벡터에 소프트맥스 함수를 이용해 중심 단어를 예측합니다.

### Skip-gram

Skip-gram은 CBoW와 반대로 중심 단어를 입력받아 주변 단어를 예측하는 모델입니다.

따라서 Skip-gram은 중심 단어를 기준으로 양쪽으로 윈도 크기만큼의 단어들을 주변 단어로 삼아 훈련 데이터테스를 만듭니다.

이때 중심 단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킵니다.

![화면 캡처 2024-07-31 152932.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/a3f35c2d-bdc9-457d-aaca-e9f48c58eb39/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-07-31_152932.png)

Skip-gram과 CBoW는 학습 데이터의 구상 방식에 차이가 있습니다.

CBoW는 하나의 윈도에서 하나의 학습 데이터가 만들어지는 반면, Skip-gram은 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터가 만들어집니다.

이러한 차이 때문에 Skip-gram은 하나의 중시 ㅁ단어를 통해 여러 개의 주변 단어를 예측하므로 더 많은 학습 데이터 세트를 추출할 수 있으며, 일반적으로 CBoW보다 더 뛰어난 성능을 보입니다.

또한 Skip-gram은 비교적 드물게 등장하는 단어를 더 잘 학습할 수 있게 되고 단어 벡터 공간에서 더 유의미한 거리 관계를 형성할 수 있습니다.

![화면 캡처 2024-07-31 153437.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/6db7c277-685d-4957-8a50-da4f710683e1/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-07-31_153437.png)

Skip-gram 모델도 CBoW와 마찬가지로 입력 단어의 원-핫 벡터를 투사층에 입력하여 해당 단어의 임베딩 벡터를 가져옵니다.

입력 단어의 임베딩과 $W^{\prime}_{E \times V}$ 가중치와의 곱셈을 통해 V크기의 벡터를 얻고, 이 벡터에 소프트맥스 연산을 취함으로써 주변 단어를 예측합니다.

소프트맥스 연산은 모든 단어를 대상으로 내적 연산을 수행합니다.

말뭉치의 크기가 커지면 필연적으로 단어 사전의 크기도 커지므로 대량의 말뭉치를 통해 Word2Vec 모델을 학습할 때 학습 속도가 느려지는 단점이 있습니다.

이를 해결하기 위해 계층적 소프트맥스와 네거티브 샘플링 기법을 적용해 학습 속도가 느려지는 문제를 완화합니다.

### 계층적 소프트맥스

계층적 소프트맥스(Hierachical Softmax)는 출력층을 이진 트리(Binary tree) 구조로 표현해 연산을 수행합니다.

이떄 자주 등장하는 단어일수록 트리의 상위 노드에 위치하고, 드물게 등장하는 단어일 수록 하위 노드에 배치됩니다.

이러한 방식으로 확률을 계산하면 일반적인 소프트맥스 연산에 비해 빠른 속도와 효율성을 보입니다.

다음 그림은 계층적 소프트맥스 구조입니다.

![image (12).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/6cf4cab8-117e-4963-8180-fcd4cfada9af/image_(12).png)

각 노드는 학습이 가능한 벡터를 가지며, 입력값은 해당 노드의 벡터와 내적값을 계산한 후 시그모이드 함수를 통해 확률을 계산합니다.

잎 노드(Leaf Node)는 가장 깊은 노드로, 각 단어를 의미하며, 모델은 각 노드의 벡터를 최적화하여 단어를 잘 예측할 수 있게 합니다.

각 단어의 확률은 경로 노드의 확률을 곱해서 구할 수 있습니다.

위의 그림에서 ‘추천해요’ 단어는 $0.43 \times 0.74 \times 0.27 = 0.085914$의 확률을 갖게 됩니다.

이 경우 학습시 1, 2번 노드의 벡터만 최적화하면 됩니다.

단어 사전의 크기를 V라고 했을 때 일반적인 소프트맥스 연산은 $O(V)$의 시간 복잡도를 갖지만, 계층적 소프트맥스의 시간 복잡도는 $O(\log_2V)$의 시간 복잡도를 갖습니다.

### 네거티브 샘플링

네거티브 샘플링(Negative Sampling)은 Word2Vec 모델에서 사용되는 확률적인 샘플링 기법으로 전체 단어 집합에서 일부 단어를 샘플링하여 오답 단어로 사용합니다.

학습 윈도 내에 등장하지 않는 단어를 n개 추출하여 정답 단어와 함께 소프트맥스 연산을 수행합니다.

이를 통해 전체 단어의 확률을 계산할 필요 없이 모델을 효율적으로 학습할 수 있습니다.

이 때 추출한 단어 n은 일반적으로 5 ~ 20개를 사용하며, 각 단어가 추출된 확률은 다음 수식과 같이 계산이 됩니다.

$$
P(w_i) = \frac{f(w)^{0.75}}{\sum^{V}_{j = 0}f(w_j)^{0.75}}
$$

네거티브 샘플링 추출 확률을 계산하기 위해 먼저 각 단어 $w_i$의 출현 빈도수를 $f(w_i)$로 나타냅니다.

가령 말뭉치 내에 단어 ‘추천해요’가 100번 등장했고, 전체 단어의 빈도가 2000이라면 $f(추천해요) = \frac{100}{2000} = 0.05$가 됩니다.

$P(w_i)$는 단어 $w_i$가 네거티프 샘플로 추출될 확률입니다.

이때 출현 빈도수에 0.75제곱한 값을 정규화 상수로 사용하는데, 이 값은 실험을 통해 얻어진 최적의 값입니다.

네거티프 샘플링에서는 입력단어 쌍이 데이터로부터 추출된 단어 쌍인지, 아니면 네거티프 샘플링으로 생성된 단어 쌍인지 이진 분류를 합니다.

이를 위해 로지스틱 회귀 모델을 사용하며, 이 모델의 학습 과정에서는 추출할 단어의 확률 분포를 구하기 위해 먼저 각 단어에 대한 가중치를 학습합니다.

다음 그림은 일반 Skip-gram 모델과 네거티프 샘플링 Word2Vec 모델의 훈련 데이터가 어떻게 다른지 보여줍니다.

![image (13).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/384e7b4d-d4e8-474c-9fd6-cb824d1546a8/image_(13).png)

‘재미있는’ 이라는 중심 단어를 통해 세 쌍의 학습 데이터를 추출합니다.

네거티프 샘플링 Wod2Vec모델은 실제 데이터에서 추출된 단어 쌍은 1로, 네거티프 샘플링을 통해 추출된 가짜 단어쌍은 0으로 레이블링합니다.

다중 분류에서 이진 분류로 학습 목적이 바뀌게 되는겁니다.

다음 그림은 일반적인 Word2Vec 모델과 네거티브 샘플링 Word2Vec 모델의 출력 구조의 차이를 보여줍니다.

![image (14).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/0719dc38-0771-434d-90b4-a260b30f78f3/image_(14).png)

네거티프 샘플링 모델에서는 입력 단어의 임베딩과 해당 단어가 맞는지 여부를 나타내는 레이블(1 또는 0)을 가져와 내적 연산을 수행합니다.

내적 연산을 통해 얻은 값은 시그모이드 함수를 통해 확률값으로 변환됩니다.

이때 레이블이 1인 경우 해당 확률값이 높아지도록, 레이블이 0인 경우 해당 확률값이 낮아지도록 모델의 가중치가 최적화됩니다.

### Skip-gram 모델 실습

Word2Vec 모델은 학습할 단어의 수를 V로, 임베딩 차원을 E로 설정해 $W_{V \times E}$행렬과 $W^{\prime}_{E \times V}$행렬을 최적화하며 학습합니다.

이때 $W_{V \times E}$행렬은 룩업(Lookup)연산을 수행하는데, 임베딩(embedding) 클래스를 사용하면 간편하게 구현이 가능합니다.

임베딩 클래스는 단어나 범주형 변수와 같은 이산 변수를 연속적인 벡터 형태로 변환해 사용할 수 있습니다.

연속적인 벡터 표현은 모델이 학습하는 동안 단어의 의미와 관련된 정보를 포착하고, 이를 기반으로 단어간의 유사도를 계산합니다.

이때 이산 변수의 값을 연속적인 벡터로 변환하는 과정을 룩업이라고 합니다.

```python
embedding = torch.nn.Embedding(
	num_embeddings,
	embedding_dim,
	padding_idx = None,
	max_norm = None,
	norm_type = 2.0
)
```

임베딩 클래스의 임베딩 수(num_embedding)는 이산 변수의 개수로 단어 사전의 크기를 의미합니다.

임베딩 차원(embedding_dim)은 임베딩 벡터의 차원 수로 임베딩 벡터의 크기를 의미합니다.

패딩 인덱스(padding_idx)는 패딩 토큰의 인덱스를 지정해 해당 인덱스의 임베딩 벡터를 0으로 설정합니다.

병렬 처리를 위해 입력 배치의 문장 길이가 동일해야 하므로 입력 문장들을 일정한 길이로 맞추는 역할을 합니다.

패딩 인덱스는 임베딩 수보다 작아야 하며 패딩 인덱스의 벡터값은 모델 학습 시 최적화되지 않습니다.

노름 타입(norm_type)은 임베이 벡터의 크기를 제한하는 방법을 선개합니다.

기본값은 2로 L2 정규화 방식을 사용한다는 의미빈다.

만약 노름 타입을 1로 설정하면 L1 정규화 방식을 사용합니다.

최대 노름(max_norm)은 임베딩 벡터의 최대 크기를 지정합니다.

각 임베딩 벡터의 크기가 최대 노름 값이사이면 임베딩 벡터를 최대 노름 크기로 잘라내고 크기를 감소시킵니다.

파이토치의 임베딩 클래스로 룩업 계층을 구현하고, 선형 변환 클래스로 가중치 행렬을 구현할 수 있습니다.

```python
from torch import nn

class VanillaSkipgram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embedding = nn.Embedding(
            num_embeddings = vocab_size,
            embedding_dim = embedding_dim
        )

        self.linear = nn.Linear(
            in_features = embedding_dim,
            out_features = vocab_size
        )
    
    def forward(self, input_ids):
        embeddings = self.embedding(input_ids)
        output = self.linear(embeddings)
        return output
```

계층적 소프트맥스나 네거티브 샘플리오가 같은 효율적인 기법을 사용하지 않은 기본 형식의 Skip-gram모델은 간단히 구현할 수 있습니다.

이 모델은 단순히 입력 단어와 주변 단어를 룩업 테이블에서 가져와서 내적 계산한 다음, 손실 함수를 통해 예측 오차를 죄소화하는 방식으로 학습됩니다.

```python
import pandas as pd
from Korpora import Korpora
from konlpy.tag import Okt

corpus = Korpora.load("nsmc")
corpus = pd.DataFrame(corpus.test)

tokenizer = Okt()
tokens = [tokenizer.morphs(review) for review in corpus.text]
print(tokens[: 3])
```

```python
# 결과
		
		Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을
    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.

    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.
    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,
    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.

    # Description
    Author : e9t@github
    Repository : https://github.com/e9t/nsmc
    References : www.lucypark.kr/docs/2015-pyconkr/#39

    Naver sentiment movie corpus v1.0
    This is a movie review dataset in the Korean language.
    Reviews were scraped from Naver Movies.

    The dataset construction is based on the method noted in
    [Large movie review dataset][^1] from Maas et al., 2011.

    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/

    # License
    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
    Details in https://creativecommons.org/publicdomain/zero/1.0/

[Korpora] Corpus `nsmc` is already installed at C:\Users\user\Korpora\nsmc\ratings_train.txt
[Korpora] Corpus `nsmc` is already installed at C:\Users\user\Korpora\nsmc\ratings_test.txt
[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]
```

corpus.test로 테스트 세트를 불러오고 이 데이터세트를 Okt 토크나이저를 사용해 형태소를 추출합니다.

```python
from collections import Counter

def build_vocab(corpus, n_vocab, special_tokens):
    counter = Counter()

    for tokens in corpus:
        counter.update(tokens)
    
    vocab = special_tokens

    for token, count in counter.most_common(n_vocab):
        vocab.append(token)

    return vocab

vocab = build_vocab(corpus = tokens, n_vocab = 5000, special_tokens = ["<unk>"])
token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for idx, token in enumerate(vocab)}

print(vocab[: 10])
print(len(vocab))
```

```python
# 결과
['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']
5001
```

Okt 토크나이저를 통해 토큰화된 데이터를 활용해 build_vocab 함수로 단어 사전을 구축합니다.

n_vocab 매개변수는 구축할 단어 사전의 크기를 의미합니다.

만약 문서 내에 n_vocab보다 많은 종류의 토큰이 있다면, 가장 많이 등장한 토큰 순서로 사전을 구축합니다.

special_tokens는 특별한 의미를 갖는 토큰들을 의미합니다.

<unk> 토큰은 OOV에 대응하기 위한 토큰으로 단어 사전 내에 없는 모든 단어는 <unk> 토큰으로 대체됩니다.

예제이서는 단어 사전의 쵀대 길이를 5000으로 설정했습니다.

특수 토큰(Special Token)은 현재 1개이므로 단어 사전은 5001개로 구성됩니다.

이제 윈도 크기를 정의하고 학습에 사용될 단어 쌍을 추출합니다.

```python
def get_word_pairs(tokens, window_size):
    pairs = []
    for sentence in tokens:
        sentence_length = len(sentence)

        for idx, center_word in enumerate(sentence):
            window_start = max(0, idx - window_size)
            window_end = min(sentence_length, idx + window_size + 1)
            center_word = sentence[idx]
            context_words = sentence[window_start:idx] + sentence[idx + 1:window_end]

            for context_word in context_words:
                pairs.append([center_word, context_word])
    return pairs

word_pairs = get_word_pairs(tokens, window_size = 2)
print(word_pairs[: 5])

```

```python
# 결과

[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]
```

get_word_pairs 함수는 토큰을 입력받아 Skip-gram모델의 입력 데이터로 사용할 수 있게 전처리합니다.

window_size는 주변 단어들 몇 개까지 고려할 것인지를 설정합니다.

각 문장에서는 중심 단어와 주변 단어를 고려하여 쌍을 생성합니다.

idx는 현재 단어의 인덱스를 나타냅니다.

center_word는 중심단어를 나타냅니다.

window_start와 window_end는 현재 단어에서 얼마나 멀리 떨어진 주변 단어를 고려할 것인지를 결정합니다.

wiodow_start와 window_end는 문장의 경계를 넘어가는 경우가 없게 조정해야합니다.

출력 결과를 보면 각 단어 쌍은 [중심 단어, 주변 단어]로 구성돼 있습니다.

임베딩 층은 단어의 인덱스를 입력으로 받기 때문에 단어 쌍을 인덱스 쌍으로 변환해야 합니다.

단어 쌍을 인덱스 쌍으로 변환하는 작업

```python
def get_index_pairs(word_pairs, token_to_id):
    pairs = []
    unk_index = token_to_id["<unk>"]

    for word_pair in word_pairs:
        center_word, context_word = word_pair
        center_index = token_to_id.get(center_word, unk_index)
        context_index = token_to_id.get(context_word, unk_index)
        pairs.append([center_index, context_index])
    return pairs

index_pairs = get_index_pairs(word_pairs, token_to_id)
print(index_pairs[: 5])
```

```python
# 결과
[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]
```

get_inedx_pairs 함수는 get_word_pairs 함수에서 생성된 단어 쌍을 토큰 인덱스 쌍으로 변환합니다.

앞서 생성한 word_pairs와 단어와 단어에 해당하는 ID를 매핑한 딕셔너리인 token_to_id로 인덱스 쌍을 생성합니다.

딕셔너리의 get 메서드로 토큰이 단어 사전 내에 있으면 해당 토큰의 인덱스를 반환하고, 단어 사전내에 없다면 <unk> 토큰의 인덱스를 반환합니다.

이렇게 생성된 인덱스 쌍은 Skip-gram 모델의 입력 데이터로 사용됩니다.

인덱스 쌍을 데이터로데 적용하는 방법

```python
import torch
from torch.utils.data import TensorDataset, DataLoader

index_pairs = torch.tensor(index_pairs)
center_indexs = index_pairs[:, 0]
context_indexs = index_pairs[:, 1]

dataset = TensorDataset(center_indexs, context_indexs)
dataloader = DataLoader(dataset, batch_size = 32, shuffle = True)
```

index_pairs는 get_index_pairs 함수에서 생성된 중심 단어와 주변 단어 토큰의 인덱스 쌍으로 이루어진 리스트입니다.

이 리스트를 텐서 형식으로 변환합니다.

이 텐서는 [N, 2]의 구조를 가지므로 중심 단어와 주변 단어로 나눠 데이터세트로 변환합니다.

인덱스 쌍을 텐서 데이터세트로 변환하고 데이터로더에 적용했다면 모델을 학습하기 위해 필요한 작업을 진행합니다.

Skip-gram 모델의 준비 작업

```python
from torch import optim

device = "cuda" if torch.cuda.is_available() else "cpu"
word2vec = VanillaSkipgram(vocab_size = len(token_to_id), embedding_dim = 128).to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.SGD(word2vec.parameters(), lr = 0.1)
```

VanillaSkipgram 클래스의 단어 사전의 크기(vocab_size)에 전체 단어 집합의 크기를 전달하고 임베딩 크기(embedding_dim)는 128로 할당합니다.

손실 함수는 단어 사전 크기만큼 클래스가있는 분류 문제이므로 교차 엔트로피를 사용합니다.

교차 엔트로피 내부적으로 소프트맥스 연산을 수행하므로 신경망의 출력값을 후처리 없이 활용할 수 있습니다.

학습 코드 정의하고 모델을 학습함

```python
for epoch in range(10):
    cost = 0.0

    for input_ids, target_ids in dataloader:
        input_ids = input_ids.to(device)
        target_ids = target_ids.to(device)

        logits = word2vec(input_ids)
        loss = criterion(logits, target_ids)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        cost += loss

    cost = cost / len(dataloader)
    print(f"Epoch: {epoch + 1: 4d}, Cost: {cost: .3f}")
```

```python
# 결과
Epoch:    1, Cost:  6.197
Epoch:    2, Cost:  5.981
Epoch:    3, Cost:  5.931
Epoch:    4, Cost:  5.901
Epoch:    5, Cost:  5.879
Epoch:    6, Cost:  5.861
Epoch:    7, Cost:  5.846
Epoch:    8, Cost:  5.833
Epoch:    9, Cost:  5.822
Epoch:   10, Cost:  5.812
```

모델 학습이 완료되면 $W_{V \times E}$ 행렬과 $W^{\prime}_{E \times V}$행렬 중 하나의 행렬을 선택해 임베딩 값읅 추출합니다.

학습된 임베딩 계층에서 임베딩 값 추출 방법

```python
token_to_embedding = dict()
embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()

for word, embedding in zip(vocab, embedding_matrix):
    token_to_embedding[word] = embedding

index = 30
token = vocab[30]
token_embedding = token_to_embedding[token]
print(token)
print(token_embedding)
```

```python
# 결과

연기
[ 0.01314399  0.24011452  0.21024072  1.2635101  -0.50235325 -0.1399459
  0.500385    0.5250144  -1.6970994  -0.88410896  1.0140911  -0.7009046
 -0.37700328  0.19923571 -0.21489818  1.3237004   0.55705637 -0.4408747
 -1.615678    0.3613069   1.1888068  -0.87226534 -1.2778207   0.14566486
 -0.5993969  -0.08963833  0.97908944 -1.1204426  -0.4379878   0.2888637
 -1.9042128   0.51930046  1.0582993  -0.71343994 -1.2868736  -0.24986917
 -0.8904092  -1.2297249   0.1704366   0.23090084  0.7463633   0.37181497
 -0.11652741  0.31624904 -0.5638099  -0.04062659  0.06364175  0.26219305
 -1.0255361  -0.44720337 -0.26104268 -0.00951209 -1.1010249  -0.07691472
  0.16696231  0.28254104 -0.4850828   0.96690655 -1.2308006   0.01143128
  2.0335987   0.42827693  0.4524225   0.34591055 -0.06727107 -0.12417756
 -0.1638674  -0.4316839  -1.2124102  -1.3904072  -0.07480224  0.22760521
 -0.0790592   0.3283616   0.09769532  0.55299044 -0.30246192 -0.4401202
 -0.39971623 -0.9397583  -0.594483   -1.0830133  -0.51177555  2.1063423
 -0.973079    0.36157158  1.7427917   0.17280129 -0.24990934  0.64064056
  0.7830109  -0.12483402  0.5779756  -0.11299171 -1.5910952  -1.7109103
 -1.5274702   1.1144924  -0.26102033 -0.77313846  1.2754378  -0.19457993
  0.17893371 -0.61208594 -0.15504366  0.5944416   1.4783516   0.36390635
  0.33284152 -1.151329    1.4936067  -0.5396368   1.6615247  -0.37711534
 -0.3151509   1.087648    0.7441833   1.0495375  -0.33283433  1.8508128
 -0.4376643  -0.44102675  1.6142983  -0.46808356 -0.9035032  -2.174988
  0.0772144  -0.3019259 ]
```

Word2Vec 모델의 임베딩 행렬을 이용해 각 단어의 임베딩 값을 매핑하고, 인덱스 30 값의 단어와 임베딩 값을 출력합니다.

이 임베딩 값으로 단어 간의 유사도를 확인할 수 있습니다.

임베딩의 유사도를 측정할 때는 코사인 유사도(Cosine Similarity)가 가장 일반적으로 사용되는 방버입니다.

코사인 유사도는 두 벡터 간의 각도를 이용하여 유사도를 계산하며, 두 벡터가 유사할 수록 값이 1에 가까워지고, 다를수록 0에 가까워 집니다.

두 벡터 간의 코사인 유사도는 두 벡터의 내적을 벡터의 크기(유클리드 노름)의 곱으로 나누어 계산할 수 있습니다.

코사인 유사도 공식

$$
cosine \ similarity(a, b) = \frac{a\cdot b}{\lVert a \rVert \lVert b \rVert}
$$

a와 b는 유사도를 계산하려는 벡터이며, 두 벡터를 내적한 벡터의 크기의 곱을 나눠 코사인 유사도를 계산할 수 있습니다.

벡터의 크기는 각 성분의 제곱합에 루트를 씌운 값입니다.

코사인 유사도는 임베딩 공간에서 단어 간의 유사도를 측정하는 데 매우 유용합니다.

코사인 유사도로 특정 단어와 가장 유사한 단어를 출력하는 코드

```python
import numpy as np
from numpy.linalg import norm

def cosine_similarity(a, b):
    cosine = np.dot(b, a) / (norm(b, axis = 1) * norm(a))
    return cosine

def top_n_index(cosine_matrix, n):
    closest_indexes = cosine_matrix.argsort()[: : -1]
    top_n = closest_indexes[1 : n + 1]
    return top_n

cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)
top_n = top_n_index(cosine_matrix, n = 5)

print(f"{token}와 가장 유사한 5 개 단어")
for index in top_n:
    print(f"{id_to_token[index]} - 유사도: {cosine_matrix[index]: .4f}")
```

```python
# 결과

연기와 가장 유사한 5 개 단어
목소리 - 유사도:  0.3191
흥 - 유사도:  0.2982
배우 - 유사도:  0.2970
연기력 - 유사도:  0.2883
스토리 - 유사도:  0.2817
```

cosine_similarity 함수는 입력 단어와 단어 사전 내의 모든 단어와의 유사도를 계산합니다.

a 매개변수는 임베딩 토큰을 의미하며, b 매개변수는 임베딩 행렬을 의미합니다.

임베딩 행렬은 [5001, 128]의 구조를 가지고 노름 계산할 때 axis = 1 방향으로 계산합니다.

top_n_index 함수는 유사도 행렬을 내림차순으로 정렬해 어떤 단어가 가장 가까운 단어인지 반환합니다.

입력 단어도 단어 사전에 포함되므로 입력 단어 자신이 가장 가까운 단어가 됩니다.

그러므로 두 번째 가까운 단어부터 계산해 반환합니다.

입력된 단어가 ‘연기’일 때 가장 유사한 단어 3개로 ‘연기력’, ‘배우’, ‘시나리오’가 추출된 것을 확인 할 수 있습니다.

### Gensim 모델 실습

매우 간단한 구조의 Word2Vec 모델을 학습할 때 데이터 수가 적은 경우에도 학습하는 데 오랜 시간이 소요됩니다.

이러한 경우, 계층적 소프트맥스나 네거티브 샘플링 같은 기법을 사용하면 더 효율적으로 학습할 수 있습니다.

젠심(Gensim) 라이브러리를 활용하면 Word2Vec과 같은 자연어 처리 모델을 쉽게 구성할 수 있습니다.

젠심 라이브러리는 대용량 텍스트 데이터 의 처리를 위한 메모리를 효율적인 방법을 제공해 대규모 데이터 세트에서도 효과적으로 모델을 학습할 수 있습니다.

또한 학습된 모델을 저장하여 관리할 수 있고, 비슷한 단어 찾기 등 유사도와 관련된 기능도 제공하여 자연어 처리에 필요한 다양한 기능을 제공합니다.

젠심 라이브러리는 사이썬(Cython)을 이용해 병렬 처리나 네거티프 샘플링 등을 적용합니다.

사이썬은 C++ 기반의 확장 모듈을 파이썬 모듈로 컴파일하는 기능을 제공합니다.

젠심으로 모델을 구성한다면 파이토치를 이용한 학습보다 훨씬 더 빠른 속도로 학습할 수 있습니다.

젠심 라이브러리의 Word2Vec 클래스는 네거티브 샘플링 등에 필요한 여러 하이퍼파라미터를 받아 쉽고 빠르게 모델을 학습할 수 있습니다.

```python
word2vec = genim.models.Word2Vec(
	sentences = None,
	corpus_file = None,
	vector_size = 100,
	alpha = 0.025,
	window = 5,
	min_count = 5,
	workers = 3,
	sg = 0,
	hs = 0,
	cbow_mean = 1,
	negative = 5,
	ns_exponent = 0,75,
	max_final_vocab = None,
	epochs = 5,
	batch_size = 10000
)
```

입력 문장(sentences)은 모델의 학습 데이터를 나타내며 토큰 리스트로 표현됩니다.

이렇게 구성된 학습 데이터는 학습 무장들의 리스트로 이루어집니다.

말뭉치(corpus_file)은 학습 데이터르 파일로 입력할 때 파일의 경로를 의임합니다.

벡터 크기vector_size)는 학습할 임베딩 벡터의 크기를 의미하며, 임베딩 차원 수를 설정합니다.

학습률(alpha)은 Word2Vec 모델의 학습률을 의미합니다.

윈도(window)는 학습 데이터를 생성할 윈도의 크기를 의미합니다.

eg) 윈도가 3이라면 중심 단어로부터 3만큼 떨어진 단어까지 주변 단어로 고려해 데이터를 생성합니다.

최소 빈도(min_count)는 학습에 사용할 단어의 최소 빈도를 의미합니다.

말뭉치 내에 최소 빈도만큼 등장하지 않은 단어는 학습에 사용되지 않습니다.

최대 최종 단어 사전(max_final_vocab)은 단어 사전의 최대 크기를 의미합니다.

최소 빈도를 충족하는 단어가 최대 최종 단어 사전보다 많으면 자주 등장한 단어 순으로 단어 사전을 구축합니다.

작업자수(workers)는 빠른 학습을 위해 병렬 학습할 스레드의 수를 의미합니다.

Skip-gram(sg)은 Skip-gram 모델을 사용여부를 설정합니다.

1이라면 Skip-gram을, 0이라면 CBoW 모델을 사용합니다.

계층적 소프트맥스(hs)는 계층적 소프트맥스 사용 여부를 설정합니다.

1이라면 계층적 소프트맥스를 사용하고, 0이라면 사용하지 않습니다.

CBoW 평균화(cbow_mean)는 CBoW 모델로 구성할 때 사용되는 하이퍼파라미터로 중심 단어와 주변단어를 합쳐서 하나의 벡터로 만들 때, 합한 벡터의 평균화 여부를 설정합니다.

1이라면 평균화하며, 0이라면 평균화하지 않고 합산합니다.

네거티브(negative)는 네거티브 샘플링의 단어 수를 의미합니다.

0이라면 네거티브 샘플링을 사용하지 않습니다.

네거티브 지수(ns_exponent)는 네거티브 샘플링 확률의 지수를 의미합니다.

위에 있던 네거티브 수식에선 네거티브 지수가 0.75일 때의 확률 계산식입니다.

에폭(epochs)은 학습 에폭 수를 의미합니다.

배치 단어 수(batch_words)는 몇 개의 단어로 학습 배치를 구성할지 결정합니다.

컴퓨팅 자원이 모자란다면 배치 단어 수를 낮추어 학습을 진행할 수 있습니다.

Word2Vec 클래스를 활용해 Skip-gram 모델을 학습하고 저장하는 것입니다.

```python
from gensim.models import Word2Vec

word2vec = Word2Vec(
    sentences = tokens,
    vector_size = 128,
    window = 5,
    min_count = 1,
    sg = 1,
    epochs = 3,
    max_final_vocab = 10000
)

word2vec.save("models/word2vec.model")
word2vec = Word2Vec.load("models/word2vec.model")
```

임베딩 벡터의 크기는 128, 윈도 크기는 5로 설정했습니다.

최소 빈도는 고려하지 않으며, 최대 단어 사전의 크기는 10000으로 설정했습니다.

빠른 학습을 위해 네거티브 샘플링을 사용하고 에폭은 3으로 설정해 모델을 학습시킵니다.

모델 학습 속도를 앞선 VanillaSkipgram 클래스와 비교해 보면 매우 빠르게 학습되는 것을 확인할 수 있습니다.

word2vec 인스턴스는 파이토치의 저장 메서드와 동일한 방법으로 저장할 수 있습니다.

임베딩 추출 및 유사도 계산 방식입니다.

```python
word = "연기"
print(word2vec.wv[word])
print(word2vec.wv.most_similar(word, topn = 5))
print(word2vec.wv.similarity(w1 = word, w2 = "연기력"))
```

```python
# 결과
[-0.36000213 -0.28067896  0.3161379   0.11792224  0.09995499 -0.21700722
  0.14466096  0.07057393 -0.63094276  0.38565454  0.18305102 -0.52033156
 -0.2894364  -0.01105568  0.08756678 -0.13170062 -0.37711215  0.2794605
 -0.0522744   0.06232541  0.51955694  0.07754096 -0.21045242 -0.18267782
 -0.1183478  -0.13531733 -0.15956044  0.04261165  0.2652962  -0.0602013
 -0.55925095 -0.01947364  0.5378081   0.03732028 -0.20875728  0.27149725
  0.3122551   0.00330161 -0.01409605 -0.13062024  0.1290557   0.14878896
 -0.14087373 -0.38186204 -0.39356813  0.07914343 -0.5230886  -0.08903944
  0.1421595   0.1460563   0.49671388  0.25990424  0.08870447  0.36725622
 -0.3623178  -0.3043796  -0.11201404 -0.17371818 -0.1682562   0.2733908
 -0.15086858 -0.26701075  0.20699084  0.24512976 -0.34672382  0.07783919
 -0.03060664  0.35519192  0.46999416 -0.22373848 -0.26846376 -0.285737
 -0.20755821  0.02464744 -0.14121124 -0.15568848 -0.24954474 -0.39099613
 -0.3431591   0.15669987 -0.49424592 -0.07006581  0.20195624  0.59702975
  0.3213821   0.13317375  0.45113996 -0.47454354  0.03665498 -0.17230849
 -0.07898959  0.10533745  0.07367785  0.4103062   0.31590638  0.04653054
 -0.19823858 -0.12761357 -0.08431658 -0.6797671  -0.41211423 -0.07506665
  0.08390998 -0.56959593  0.02115465  0.2807188   0.48193198  0.21284604
 -0.12855683 -0.20913692  0.50253105 -0.37439784 -0.3574317   0.2841156
  0.07483848 -0.32306314  0.30888772  0.2953     -0.1310999   0.39902526
 -0.24226756 -0.12893404  0.17996624 -0.05787978 -0.00716805  0.02653779
 -0.37803093 -0.11322787]
[('연기력', 0.7932191491127014), ('캐스팅', 0.7467899918556213), ('조연', 0.7370455265045166), ('여배우', 0.7355148792266846), ('연기자', 0.7291114926338196)]
0.79321915
```

word2vec 인스턴스의 wv 속성은 학습된 단어 벡터 모델을 포함한 Word2VecKeyedVecotrs 객체를 반환합니다.

이 객체는 단어 벡터 검색과 유사도 계산 등의 작업을 수행하는 데 사용합니다.

이객체는 주어진 단어에 대해 가장 유사한 단어를 찾는 most_similar 메서드와 두 단어 간의 유사도를 계산하는 similarity 메서드를 제공합니다.

출력 결과에서 확인할 수 있듯이 젠심 라이브러리는 효율적으로 모델을 학습할 수 있습니다.

Word2Vec은 분포 가설을 통해 쉽고 빠르게 단어의 임베딩을 학습할 수 있지만, 이는 단어의 형태학적 특징을 반영하지 못 한다는 한계가 있습니다.

eg) 교착어로는 한국어는 어근과 접사, 조사 등으로 이루어지는 규칙을 가지고 있기 때문에 Word2Vec 모델에서는 이러한 구조적 특징을 제대로 학습하기가 어렵습니다.

이러한 한계는 제한된 단어 사전에서 많은 OOV를 발생시키는 원인이 됩니다.

## fastText

fastText는 2015년 메타의 FAIR 연구소에서 개발한 오픈소스 임베딩 모델로, 텍스트 분류 및 텍스트 마이닝을 위한 알고리즘입니다.

fastText는 단어와 문장을 벡터로 변환하는 기술을 기반으로 하며, 이를 통해 머신러닝 알고리즘이 텍스트 데이터를 분석하고 이해하는데 사용됩니다.

이러한 벡터화 기술은 Word2Vec과 유사하지만, fastText는 단어의 하위 단어를 고려하므로 더 높은 정확도와 성능을 제공합니다.

하위 단어를 고려하기 위해 N-gram을 사용해 단어를 분해하고 벡터화하는 방법으로 동작합니다.

fastText에서는 단어의 벡터화를 위해 <,>와 같은 특수 기호를 사용하여 단어의 시작과 끝을 나타냅니다.

이러한 기호는 단어의 하위 문자열을 고려하는 데 중요한 역할을 합니다.

기호가 추가된 단어는 N-gram을 사용하여 하위 단어 집합(SubWord set)으로 분해됩니다.

eg) ‘서울특별시’를 바이그램으로 분해하면 ‘서울’, ‘울특’, ‘특별’, ‘별시’가 됩니다.

분해된 하위 단어 집합에는 나눠지지 않은 단어 자신도 포함되며, 단어 집합이 만들어지면 각 하위 단어는 고유한 벡터 값을 갖게 됩니다.

이러한 하위 단어 벡터들은 단어의 벡터 표현을 구성하며, 이를 사용하여 자언어 처리 작업을 수행합니다.

![image (15).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/c2dd8246-0cb3-40a1-8172-7623dc1e5612/image_(15).png)

fastText는 입력으로 받은 ‘서울특별시’라는 토큰에 대해 다음과 같은 단계를 거쳐 처리됩니다.

1. 토큰의 양 끝에 ‘<’와 ‘>’를 붙여 토큰의 시작과 끝을 인식할 수 있게 합니다.
2. 분해된 토큰은 N-gram을 사용하여 하위 단어 집합으로 분해합니다. 트라이그램을 사용한다면, ‘서울특별시’는 [<서울, 서울특, 울특별, 특별시. 별시>]로 분해됩니다.
3. 분해된 하위 단어 집합에는 눠지지 않은 토큰 자체도 포함됩니다. 이렇게 하위 단어 집합이 만들어지면, 각 하위 단어는 고유한 벡터값을 갖습니다.

fastText는 위와 같은 방법으로 하위 단어 집합을 생성합니다.

입력 단어를 하위 단어로 분해하면 총 6개의 하위 단어가 존재하게 됩니다.

각 하위 단어의 임베딩 벡터를 구하고, 이를 모두 합산하여 입력 단어의 최종 임베딩 벡터를 계산합니다.

일반적으로 fastText는 다양한 N-gram을 적용해 입력 토큰을 분해하고 하위 단어 벡터를 구성함으로써 단어의 부분 문자열을 고려하는 유연하고 정확한 하위 단어 집합을 생성합니다.

같은 하위 단어를 공유하는 단어끼리는 정보를 공유해 학습할 수 있습니다.

이를 통해 비슷한 단어끼리는 비슷한 임베딩 벡터를 갖게 되어, 단어 간 유사도를 높일 수 있습니다.

OOV 단어도 하위 단어로 나누어 임베딩을 계산할 수 있게 됩니다.

eg) ‘개인택시’, ‘정보처리기사’, ‘임대차보호법’ 이라는 단어가 말뭉치 내에 있을 때, 이 단어들의 하위 단어들로부터 ‘개인정보보호법’이라는 단어의 임베딩도 연산할 수 있습니다.

### fastText 모델 실습

fastText와 Word2Vec 모델 모두 단어 임베딩을 학습하는 데 사용되는 알고리즘 입니다.

두 알고리즘 모두 단어를 고정 길이 벡터 형태로 표현하기 위해 분산 표현 학습을 수행하고 주변 단어의 정보를 활용하여 단어의 의미를 파악합니다.

이를 통해 단어 간의 유사성을 측정하고, 비슷한 의미를 가진 단어를 유사한 벡터로 표현합니다.

유사한 단어들은 공간상 가깝게 임베딩 됩니다.

fastText 모델도 CBoW와 Skip-gram으로 구성되며 네거티프 샘플링 기법을 사용해 학습합니다.

Word2Vec 모델은 기본 단위로 모델을 학습했다면 fastText는 하위 단어로 학습합니다.

그러므로 Word2Vec 모델보다 입력 단어를 구성하는데 조금 더 복잡한 과정이 필요하고 모든 하위 단어크기를 갖는 임베딩 계층이 필요합니다.

젠심 라이브러리는 이러한 복잡한 과정을 FastText클래스로 제공합니다.

Word2Vec과 기술적인 공통점이 많아 Word2Vec의 대부분 매개변수를 공유합니다.

FastText 클래스

```python
fasttext = gensim.models.FastText(
	sentences = None,
	corpus_file = None,
	vector_size = 100,
	alpha = 0.025,
	window = 5,
	min_count = 5,
	workers = 3,
	sg = 0,
	hs = 0,
	cbow_mean = 1,
	negative = 5,
	ns_exponent = 0,75,
	max_final_vocab = None,
	epochs = 5,
	batch_words = 10000,
	min_n = 3,
	max_n = 6
)
```

대부분 하이퍼파라미터는 Word2Vec 클래스의 하이퍼파라미터와 동일하지만, N-gram 범위를 결정하는 하이퍼파라미터가 추가가됩니다.

최소 N(min_n)은 사용할 N-gram의 최솟값을 의미하며, 최대 N(max_n)은 사용할 N-gram의 최댓값을 의미합니다.

최소 N이 2이고 최대 N이 4라면 입력 단어를 2-gram, 3-gram, 4-gram으로 나누어 하위 단어집합을 생성합니다.

코포라 라이브러리의 KorNLI 데이터트 전처리

```python
from Korpora import Korpora

corpus = Korpora.load("kornli")
corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()
tokens = [sentence.split() for sentence in corpus_texts]

print(tokens[: 3])
```

```python
# 결과

    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을
    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.

    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.
    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,
    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.

    # Description
    Author : KakaoBrain
    Repository : https://github.com/kakaobrain/KorNLUDatasets
    References :
        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark
           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.
           (https://arxiv.org/abs/2004.03289)

    This is the dataset repository for our paper
    "KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding."
    (https://arxiv.org/abs/2004.03289)
    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.

    # License
    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)
    Details in https://creativecommons.org/licenses/by-sa/4.0/
...
[Korpora] Corpus `kornli` is already installed at C:\Users\user\Korpora\kornli\snli_1.0_train.ko.tsv
[Korpora] Corpus `kornli` is already installed at C:\Users\user\Korpora\kornli\xnli.dev.ko.tsv
[Korpora] Corpus `kornli` is already installed at C:\Users\user\Korpora\kornli\xnli.test.ko.tsv
[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```

KorNLI(Korean Natural Language Inference) 데이터세트는 한국어 자연어 추론(National Language Inference, NLI)을 위한 데이터세트입니다.

자연어 추론이란 두 개 이상의 문장이 주어졌을 때, 두 문장간의 관계를 분류하는 작업을 의미합니다.

이를 통해 주어진 문장이 함의 관계(entailment), 중립 관계(neutral), 불일치 관계(contradiction)중 어느 관계에 해당되느닞 분류할 수 있습니다.

eg) ‘나는 일본으로 여행을 가고 싶다’와 ‘파이토치를 공부하고 있다’라는 두 문장이 있다면 이 두문장의 관계는 서로 연관이 없으므로 불일치 관계로 분류될 수 있습니다.

코포라 라이브러리의 KorNLI 인스턴스는 get_all_texts와 get_all_pairs 메서드를 제공합니다.

get_all_texts 메서드는 모든 문장을 튜플 형태로 반환하며, get_all_pairs 메서드는 이벽 문장과 쌍을 이루는 대응 문장을 튜플 형태로 반환합니다.

FastText 클래스를 이용해 fastText 모델을 학습하고 저장

```python
from gensim.models import FastText

fastText = FastText(
    sentences = tokens,
    vector_size = 128,
    window = 5,
    min_count = 5,
    sg = 1,
    epochs = 3,
    min_n = 2,
    max_n = 6
)

fastText.save("models/fastText.model")
fastText = FastText.load("models/fastText.model")
```

임베딩 벡터의 크기는 128, 윈도 크기는 5로 설정했습니다.

5번 이상 등장한 단어만 사용하고 최대 단어 사전의 크기는 20000으로 설정했습니다.

N-gram 범위는 [2, 6]으로 설정했습니다.

Word2Vec 모델과는 다르게 OOV 단어를 대상으로도 의미 있는 임베딩을 추출할 수 있습니다.

다음 코드는 단어 사전에 없는 단어의 임베딩을 추출하는 방법과 유사도를 계산하는 방법을 보여줍니다.

```python
oov_token = "사랑해요"
oov_vector = fastText.wv[oov_token]

print(oov_token in fastText.wv.index_to_key)
print(fastText.wv.most_similar(oov_vector, topn = 5))
```

```python
# 결과
False
[('사랑해', 0.9130046963691711), ('사랑', 0.8724908828735352), ('사랑한', 0.859227180480957), ('사랑해서', 0.848992645740509), ('사랑해.', 0.8412243127822876)]
```

fastText 모델의 wv.index_to_key 메서드는 학습된 단어 사전을 나타내는 리스트를 의미합니다.

‘사랑해요’라는 단어는 단어 사전 리스트 내에 존재하지 않으므로 OOV 토큰으로 처리됩니다.

Word2Vec 모델은 단어 사전에 존재하지 않는 단어는 임베딩을 계산할 수 없었습니다.

하지만 fastText는 하위 단어로 나뉘어 있기 때문에 ‘사랑해요’라는 단어를 처리할 수 있습니다.

‘사랑해요’토큰은 ‘사랑’, ‘랑해’, ‘해요’ 등의 하위 단어로 분해됩니다.

이 분해된 하위 단어의 임베딩을 모두 합해 전체 토큰의 임베딩을 계산합니다.

그러므로 다른 단어에서 등장했던 ‘사랑’이나 ‘해요’와 같은 하위 단어를 통해 ‘사랑해요’ 토큰의 임베딩을 계산할 수 있습니다.

이와 같은 방식으로 fastText는 OOV 문제를 효과적으로 해결할 수 있습니다.

한국어와 같은 많은 언어는 형태적 구조를 갖고 있기 때문에 하위 단어로 나누어 임베딩을 학습하는 fastText의 접근방식을 통해 OOV 문제를 해결할 수 있습니다.

## 순환 신경망

순환 신경망(Recurrent Neural Network, RNN) 모델은 순서가 있는 연속적인 데이터(Sequence data)를 처리하는데 적합한 구조를 갖고 있습니다.

순환 신경망은 각 시점(Time step)의 데이터가 이전시점의 데이터와 독립적이지 않다는 특성 때문에 효과적으로 작동합니다.

eg) 일일 주가 데이터세트가 있다고 가정한다면 3월 7일의 주가는 3월 6일의 주가의 영향을 받았을 가능성이 높습니다.

동일하게 3월 6일의 주가는 3월 5일의 주가의 영향을 받앗을 가능성이 높습니다.

이렇게 특정 시점 $t$에서의 데이터가 이전 시점 $(t_0, t_1, \cdots, t_{n-1})$의 영향을 받는 데이터를 연속형 데이터라 합니다.

자연어 데이터는 연속적인 데이터의 일종으로 볼 수 있습니다.

자연어 데이터는 문장 안에서 단어들이 순서대로 등장하므로, 각 단어는 이전에 등장한 단어의 영향을 받아 해당 문장의 의미를 형성합니다.

다음과 같은 문장을 통해 자연어가 갖고 있는 연속형 데이터의 특징을 알 수 있습니다.

- 금요일이 지나면_
- 수요일이 지나면 목요일이고, 목요일이 지나면 금요일, 금요일이 지나면 _

예시는 단어들의 연속으로 이루어진 자연어 데이터가 가지는 특징을 잘 보여줍니다.

주어진 문장들에서 이전 단어들의 팬턴과 의미를 고려해 다음에 올 단어를 유추할 수 있습니다.

eg) 첫 번째 문장에서 ‘금요일이 지나면’이라는 문장을 보고 ‘주말’ 또는 ‘토요일’ 등의 단어가 등장할 것이라고 예측할 수 있습니다.

두 버너째 문장에서는 앞선 단어 패턴을 고려했을 때 ‘주말’보다는 ‘토요일’이 더 자연스러운 예측일 것 입니다.

이처럼 자연어는 한 단어가 이전의 단어들과 상호작용하여 문맥을 이루고 의미를 형성합니다.

이러한 특징으로 인해 자연어는 연속형 데이터의 특성을 갖습니다.

t번째 단어는 t-1 번째까지의 단어에 영향을 받아 결졍됩니다.

긴 문장일수록 앞선 단어드로가 뒤따르는 단어들 사이에 강한 상관관계(Correlation)가 존재합니다.

eg) ‘금요일이 지나면’이라는 문장에서 ‘지나면’이라는 단어는 ‘금요일이’이라는 앞선 문맥과 상호작용해 전체적인 의미를 형성합니다.

자연어 처리에서는 이러한 문맥과 상호작용을 모델링해 정확한 의미 파악이 필요합니다.

### 순환 신경망

순환 신경망은 연속적인 데이터를 처리하기 위해 개발된 인공 신경망의 한 종류입니다.

이전에 처리한 데이터를 다음 단계에 활용하고 현재 입력 데이터와 함께 모델 내부에서 과거의 상태를 기억해 현재 상태를 예측하는데 사용됩니다.

순환 신경망은 시계열 데이터, 자연어 처리, 음성 인식 및 기타 시퀀스 데이터와 같은 도메인에서 널리 사용됩니다.

이러한 데이터는 일반적으로 길이가 가변적이며, 순서에 따라 의미가 있기 때문에, 순환 신경망은 이러한 데이터를 처리하기에 적합한 구조를 가지고 있습니다.

순환 신경망은 연속형 데이터를 순서대로 입력받아 처리하며 각 시점마다 은닉 상태(Hidden state)의 형태로 저장합니다.

각 시점의 데이터를 입력으로 받아 은닉 상태와 출력값을 계산하는 노드를 순환 신경망의 셀(Cell)이라고 합니다.

순환 신경망의 셀은 이전 시점의 은닉 상태 $h_{t-1}$을 입력으로 받아 현재 시점의 은닉 상태 $h_t$를 계산합니다.

이러한 재귀적 특징으로 인해 ‘순환’ 신경망이라고 불립니다.

다음 그림은 순환 신경망의 셀입니다.

![image (16).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/dc014af6-c85c-4e60-ba83-9ed3e13188ca/image_(16).png)

순환 신경망은 각 시점 $t$에서 현재 입력값 $x_t$와 이전 시점 $t-1$의 은닉 상태 $h_{t-1}$를 이용해 현재 시점의 은닉 상태 $h_t$와 출력값 $y_t$를 계산합니다.

다음 수식은 순환 신경망의 은닉상태 수식입니다.

$$
\begin{matrix}
h_t &=& \sigma_h(h_{t-1}, x_t)\\
h_t &=& \sigma_h(W_{hh}h_{t-1} + W_{xh}x_t + b_h)
\end{matrix}
$$

$\sigma_h$는 순환 신경망의 은닉 상태를 계산하기 위한 호라성화 함수를 의미합니다.

은닉 상태 활성화 함수는 이전 시점 $t-1$의 은닉 상태 $h_{t-1}$과 입력값 $x_t$를 입력받아 현재 시점의 은닉 상태 $h_t$를 계산합니다.

$\sigma_h$는 가중치($W$)와 편향($b$)을 이용해 계산합니다.

$W_{hh}$는 이전 시점의 은닉 상태 $h_{t-1}$에 대한 가중치, $W_{xh}$는 입력값 $x_t$에 대한 가중치, $b_t$는 은닉 상태 $h_t$의 편향을 의미합니다.

다음 수식은 출력값에 대한 수식입니다.

$$
\begin{matrix}
y_t &=& \sigma_y(h_t)\\
y_t &=& \sigma_y(W_{hy}h_t + b_y)
\end{matrix}
$$

$\sigma_y$는 순환 신경망의 출력값을 계산하기 위한 활성화 함수를 의미합니다.

출력값 활성화 함수는 현재 시점의 은닉 상태 $h_t$를 입력으로 받아 출력값 $y_t$를 계산합니다.

출력값 계산 방법도 가중치$(W)$와 편향$(b)$을 이용해 계산합니다.

$W_{hy}$는 현재 시점의 은닉 상태 $h_t$에 대한 가중치, $b_y$는 출력값 $y_t$의 현퍙을 의미합니다.

순환 신경망의 출력값은 이전 시점의 정보를 현재 시점에서 활용해 입력 시퀀스의 패턴을 파악하고 출력값을 예측하므로 연속형 데이터를 처리할 수 있습니다.

순환 신경망은 다양한 구조로 모델을 설계할 수 있습니다.

가장 단순한 구조인 단순 구조부터 일대다 구조, 다대일 구조, 다대다 구조 등이 있습니다.

#### 일대다 구조

일대다 구조(One-to-Many)는 하나의 입력 시퀀스에 대해 여러 개의 출력값을 생성하는 순환 신경망 구조입니다.

eg) 자연어 처리 분야에서는 일대다 구조를 사용하여 문장을 입력으로 받고, 문장에서 각 단어의 품사를 예측하는 작업을 할 수 있습니다.

입력 시퀀스는 문장으로 이루어져 있으며, 출력 시퀀스는 각 단어의 품사로 이루어져 있습니다.

이미지 데이터를 입력으로 받으면 이미지에 대한 설명을 출력하는 이미지 캡셔닝(Image Captioning) 모델이 됩니다.

이러한 일대다 구조를 구현하기 위해서는 출력 시퀀스의 길이를 미리 알고 있어야 합니다.

이를 위해 입력 시퀀스를 처리하면서 시퀀스의 정보를 활용해 출력 시퀀스의 길이를 예측하는 모델을 함께 구현해야 합니다.

순환 신경망의 일대다 구조

![image (17).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/dfd46d12-94ae-4dc5-a1e4-7ecf9dbc8400/image_(17).png)

#### 다대일 구조

다대일 구조(Many-to-One)는 여러 개의 입력 시퀀스에 대해 하나의 출력값을 생성하는 순환 신경망 구조입니다.

eg) 감정 분류(Sentiment Analysis) 분야에서는 다대일 구조를 사용하여 특정 문장의 감정(긍정, 부정)을 예측하는 작업을 할 수 있습니다.

입력 시퀀스는 문장으로 이루어져 있으며, 출력값은 해당 문장의 감정(긍정, 부정)으로 이루어져 있습니다.

입력 시퀀스가 어떤 범주에 속하는지를 구분하는 문장 분류, 두 문장 간의 관계를 추론하는 자연어 추론(Natural Language Inference) 등에도 적용할 수 있습니다.

순솬 신경망의 다대일 구조

![image (18).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/ea670c2b-8d68-4a07-90cb-85b403dd611f/image_(18).png)

#### 다대다 구조

다대다 구조(Many-to-Many)는 입력 시퀀스와 출력 시퀀스의 길이가 여러 개인 경우에 사용되는 순환 신경망 구조입니다.

이 구조는 다양한 분야에서 활용됩니다.

eg) 입력 문장에 대해 번역된 출력 문장을 생성하는 번역기, 음성 인식 시스템에서 음성 신호를 입력으로 받아 문장을 출력하는 음성 인식기 등에서 사용됩니다.

다대다 구조에서도 입력 시퀀스와 출력 시퀀스의 길이가 서로 다른 경우가 있을 수 있습니다.

eg) 입력 문장의 길이와 출력 문장의 길이가 일치하지 않는 경우가 있습니다.

이 경우 입력 시퀀스와 출력 시퀀스의 길이를 맞추기 위해 패딩을 추가하거나 잘라내는 등의 전처리 과정이 수행됩니다.

다대다 구조는 시퀀스-시퀀스(Seq2Seq) 구조로 이루어져 있습니다.

시퀀스-시퀀스 구조는 입력 시퀀스를 처리하는 인코더(Encoder)와 출력 시퀀스를 생성하는 디코더(Decoder)로 구성되어 있습니다.

인코더는 입력 시퀀스를 처리해 고정 크기의 벡터로 출력하고, 디코더는 이 벡터를 입력으로 받아 출력 시퀀스르 생성합니다.

순환 신경망의 다대다 구조

![image (19).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/4746d133-3fc6-454d-8272-5e3418efe244/image_(19).png)

#### 양방향 순환 시경망

양방향 순환신경망(Bidirectional Recurrent Neural Network, BiRNN)은 기본적인 순환신경망에서 시간 방향을 양방향으로 처리할 수 있도록 고안된 방식입니다.

순환 신경망은 현재 시점의 입력값을 처리하기 위해 이전 시점$(t-1)$의 은닉 상태를 이용하는데, 양방향 순환 신경망에서는 이전 시점$(t-1)$의 은닉 상태 뿐만 아니라 이후 시점$(t+1)$의 은닉 상태도 함께 이용합니다.

eg) “인생은 B와 _사이의 C다.” 라는 문장에서 _에 입력될 단어를 예측해야 한다면 앞의 문장만이 아니라 뒤의 문장에도 영향을 받는 다는 것을 알 수 있습니다.

빈칸 앞의 ‘인생은 B와’만 봐서는 다음에 어떤 단어가 올지 예측하기 어렵습니다.

하지만 빈칸 뒤의 ‘사이의 C이다’를 통해 빈칸에 ‘D’라는 단어가 적절하게 들어갈 수 있다는 것을 알 수 있습니다.

이처럼 양방향 순환 신경망은 $t$시점 이후의 데이터도 $t$시점의 데이터를 예측하는 데 사용될 수 있습니다.

이러한 방법은 입력 데이터를 순방향으로 처리하는 것만 아니라 역방향으로 거꾸로 읽어 들여 처리하는 방식으로 이루어져있습니다.

연속형 데이터는 이전 시점 데이터 뿐만아니라 이후 시점의 데이터와 큰 상관관계를 갖고 있습니다.

그러므로 양방향 순환 신경망은 양방향적인 정보를 모두 고려해 현재 시점의 출력값을 계산합니다.

양방향 순과 신경망 구조

![image (20).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/d32e7f38-b2fb-49f1-b653-8693a2c295a9/image_(20).png)

#### 다중 순환 신경망

다중 순환 신경망(Stacked Recurrent Neural Network)은 여러 개의 순환 신경망을 연결하여 구성한 모델로 각 순환 신경망이 서로 다른 정보를 처리하도록 설계되어 있습니다.

다증 퍼셉트론을 보면 더 복잡한 문제 해결을 위해 단층 퍼셉트론을 여러 개 쌓아 해결했습니다.

동일한 방식으로 순환 신경망도 여러 층을 쌓아 활용할 수 있습니다.

다중 순환 신경망은 여러 개의 순환 신경망 층으로 구성되어있으며, 각 층에서의 출력값은 다음 층으로 전달되어 처리됩니다.

이렇게 여러 개의 층으로 구성된 RNN은 데이터의 다양한 특징을 추출할 수 있어 성능이 향상될 수 있습니다.

층이 깊어질수록 더 복잡한 패턴을 학습할 수 있다는 장점도 있습니다.

하지만 순환 신경망 층이 많아질술고 학습 시간이 오래 걸리고, 기울기 소실 문제가 발생할 가능성도 높습니다.

다중 순환 신경망 구조

![image (21).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/ee392b9e-ce5d-4348-a7e1-e83d3ade499d/image_(21).png)

#### 순환 신경망 클래스

순환 신경망 클래스는 일대일 구조 순환 신경망부터 다중 순환 신경망까지 쉽게 구현할 수 있습니다.

```python
rnn = torhc.nn.RNN(
	input_size,
	hidden_size,
	uum_layers = 1,
	nonlinearity = "tanh",
	bias = Fasle,
	batch_first = True,
	dorp out = 0,
	bidirectional = False
)
```

순환 신경망 클래스(RNN)는 입력 특성 크기(input_size)와 은닉 상태 크기(hidden_size)를 입력해 순환 신경망을 구성할 수 있습니다.

입력 특성은 입력값 $x$를 의미하며, 은닉 상태는 $h$를 의미합니다.

계층 수(num_layers)는 순환 신경망의 층수를 의미합니다.

2 이상이면 다중 순환 신경망을 구성합니다.

활성화 함수(nonlinearity)는 순환 신경망에서 사용되는 활성화 함수 $\sigma$의 종류를 설정합니다.

tanh와 relu를 적용할 수 있습니다.

편향(bias)은 편향 값 사용 유무를 설정합니다.

배치우선(batch_first)은 입력 배치 크기를 첫 번째 차원으로 사용할지 여부를 설정합니다.

참이라면 [배치 크기, 시퀀스 길이, 입력 특성 크기]의 형태로 입력해야 하며, 거짓 값이라면[시퀀스 길이, 배치 크기, 입력 특성 크기]로 입력합니다.

드롭아웃(dropout)은 과대적합 방지를 위한 드롭아웃 확률을 설정합니다.

마지막 양방향 순환 신경망(bidirectional)은 순환 신경망 구조가 양방향으로 처리할지 여부를 설정합니다.

순환 신경망 클래스를 활용해 다양한 구조의 순환 신경망을 구성할 수 있습니다.

다음 코드는 양방향 다층 신경망 코드입니다.

```python
import torch
from torch import nn

input_size = 128
output_size = 256
num_layers = 3
bidirectional = True

model = nn.RNN(input_size = input_size, hidden_size = output_size, num_layers = num_layers, nonlinearity = "tanh", batch_first = True, bidirectional = bidirectional)

batch_size = 4
sequence_len = 6

inputs = torch.randn(batch_size, sequence_len, input_size)
h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)

outputs, hidden = model(inputs, h_0)
print(outputs.shape)
print(hidden.shape)
```

```python
# 결과

torch.Size([4, 6, 512])
torch.Size([6, 4, 256])
```

순환 신경망 클래스는 순전파 연산 시 입력값(input)과 초기 은닉 상태(h_0)로 순방향 연산을 수행해 출력값(output)과 최종 은닉 상태(hidden)를 반환합니다.

먼저 입력값 차원은 [배치 크기, 시퀀스 길이, 입력 특성 크기]로 전달합니다.

초기 은닉 상태는 순방향 계층 수와 양방향 구조에 따라 전달해야 하는 크기가 달라집니다.

[계층 수 $\times$ 양방향 여부 + 1, 배치 크기, 은닉 상태 크기]로 구성되어 있습니다.

예제의 구조는 [3 $\times$ 2, 4, 256] 형태로 전달됩니다.

순방향 연산 결과는 출력값과 최종 은닉 상태가 반환됩니다.

출력값은 [배치크기, 시퀀스 길이, (양방향 여부 + 1) $\times$ 은닉 상태 크기]가 됩니다.

그러므로 [4, 6, 2 $\times$ 256]의 형태로 반환됩니다.

최종 은닉 상태는 초기 은닉 상태와 동일한 차원으로 반환됩니다.

그러므로  [3 $\times$ 2, 4, 256] 형태로 반환됩니다.

순환 신경망 입출력 차원은 배치 우선 매개변수에 따라 차원의 형태가 달라지므로 매개변수 설정에 주의 해야합니다.

### 장단기 메모리

장단기 메모리(Long Short-Term Memory, LSTM)란 순환 신경망이 갖고 있던 기억력 부족과 기울기 소실 문제를 해결한 모델입니다.

일반적인 순환 신경망은 특정 시점에서 이전 입력 데이터의 정보를 이용해 출력값을 예측하는 구조이므로 시간적으로 먼 과거의 정보는 잘 기억하지 못 합니다.

하지만 어떤 연속형 데이터는 다음 시점의 데이터를 유추하기 위해 훨씬 먼 시점의 데이터에서 힌트를 얻어야 합니다.

eg) 우리는 시내에 있는 중식당에 갔다.

식당의 분위기는 괜찮았고 식사는 맛있었다.

그녀와의 대화도 즐거웠다.

우리는 함께 대화를 더 나누기 위해 _을 떠나 카페로 이동했다.

순환 신경망의 경우 시간적으로 연속된 데이터를 다룰 수 있다는 장점이 있지만, 앞선 시점에서의 정보를 끊임없이 반영하기에 학습 데이터의 크기가 커질수록 앞서 학습한 정보가 충분히 전달되지 않는다는 단점이 있습니다.

이러한 단점으로 인해 장기 의존성 문제(Long-term dependencise)가 발생할 수 있습니다.

활성화 함수로 사용되는 하이퍼볼긱 탄젠트 함수나 ReLU 함수의 특성으로 인해 역전파 과정에서 기울기 소실이나 기울기 폭주가 발생할 가능성이 있습니다.

이러한 문제를 해결하기 위해 장단기 메모리를 사용합니다.

장단기 메모리는 순환 신경망과 비슷한 구조를 가지지만, 메모리 셀(Memory cell)과 게이트(Gate)라는 구조를 도입해 장기 의존성 문제와 기울기 소실 문제를 해결합니다.

장단기 메모리 구조

![화면 캡처 2024-08-01 162739.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/193dc6b8-a54c-436d-b177-9b34cdd06290/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_162739.png)

장단기 메모리는 셀 상태(Cell state)와 망각 게이트(Forget gate), 기억 게이트(Input gate), 출력 게이트(Output gate)로 정보의 흐름을 제어합니다.

셀 상태는 정보를 저장하고 유지하는 메모리 역할을 하며 출력 게이트와 망각 게이트에 의해 제어됩니다.

망각 게이트는 장단기 메모리에서 이전 셀 상태에서 어떠한 정보를 삭제할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태를 입력으로 받아 어떤 정보를 삭제할지 결정합니다.

입력 게이트는 새로운 정보를 어던 부분에 추가할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태를 입력으로 받아 어떤 정보를 추가할지 결정합니다.

출력 게이트는 셀 상태의 정보 중 어떤 부분을 출력할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태, 그리고 새로 추가된 정보를 입력으로 받아 출력할 정보를 결정합니다.

#### 장단기 메모리 구조

장단기 메모리 신경망 내에서 셀 상태와 게이트가 어떻게 동작하는지 보겠습니다.

메모리 셀 상태

![화면 캡처 2024-08-01 163518.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/fc0dbdb7-ce7e-4416-9594-bad7d7714b60/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_163518.png)

메모리 셀은 순환 신경망의 은닉 상태와 유사하게 현재 시점의 입력과 이전 시점의 은닉 상태를 기반으로 정보를 계산하고 저장하는 역할을 합니다.

하지만 순환 신경망에서 은닉 상태는 출력값을 계산하는 데 사용되지만, 장단기 메모리에 메모리 셀은 출력값 계산에 직접 사용되지 않습니다.

대신 장단기 메모리는 망각 게이트, 입력 게이터, 출력 게이트를 통해 어떤 정보를 버릴지, 어떤 정보를 기억할지, 어떤 정보를 출력할지를 결정하는 추가적인 연산을 수행합니다.

세가지 게이트 모두 활성화 함수로 시그모이드를 사용합니다.

시그모이드 함수는 입력값을 0과 1사이의 값으로 변환하므로 게이트의 출력값은 각각 0에서 1 사이의 값으로 결정됩니다.

이 값이 해당 게이트가 입력값에 대해 얼마나 많은 정보를 통과시킬지 결정합니다.

eg) 망각 게이트의 출력값이 1이면 이전 시점의 기억 상태가 현재 시점의 기억 상태에 완전히 유지됩니다.

출력값이 0이면 이전 시점의 기억 상태는 현재 시점의 기억 상태에 전혀 반영되지 않습니다.

망각 게이트는 현재 시점의 입력과 이전 시점의 은닉 상태를 입력으로 받아 시그모이드 함수를 거친 값과 메모리 셀을 곱한 값으로 이전 시점의 메모리 셀을 업데이트 합니다.

시그모이드 함수를 거치며 0과 1사이의 값이 출력되며, 이 값은 어떤 정보를 유지할 것인지, 아니면 망각할 것인지를 결정합니다.

기억 게이트는 현재 시점의 입력과 이전 시점의 은닉 상태를 입력으로 받아 시그모이드 함수를 거친 값과 하이퍼볼릭 탄젠트 함수를 거친 값의 곱으로 새로운 기억 값을 계산합니다.

시그모이드 함수는 입력값의 중요도를 결정하고, 하이퍼볼릭 탄젠트 함수는 입력값을 -1과 1사이의 값으로 변환합니다.

기억 게이트는 어떤 정보를 얼마나 추가할지를 결정합니다.

출력 게이트는 현재 시점의 입력과 이전 시점의 은닉 상태, 그리고 새로 업데이트된 메모리 셀을 입력으로 받아 현재 시점의 출력값을 계산합니다.

출력 게이트는 어떤 정보를 출력할지를 결정합니다.

이러한 게이트들은 입력값에 대한 가중치를 동적으로 조절하면서, 적절한 정보만을 기억하고 유지할 수 있다는 장점을 가지고 있습니다.

망각 게이트

![화면 캡처 2024-08-01 165151.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/db37b8d7-8efa-4201-a775-36ea272f9e8e/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_165151.png)

망각 게이트는 이전 시점의 은닉 상태 $h_{t-1}$과 현재 시점의 입력값 $x_t$를 통해 연산을 수행합니다.

망각 게이트 수식

$$
f_t = \sigma(W^{(f)}_xx_t + W^{(f)}_hh_{t-1} + b^{(f)})
$$

망각 게이트는 이전 시점의 메모리 셀과 현재 시점의 입력을 바탕으로 어떤 정보를 삭제할지를 결정합니다.

망각 게이트의 출력값 $f_t$는 시그모이드 활성화 함수를 사용해 계산됩니다.

수식에서 $W^{(f)}_x$와 $W^{(f)}_h$는 입력값과 은닉 상태를 위한 가중치를 의미하며 $b^{f}$는 망각 게이트의 편향을 의미합니다.

현재 시점의 입력값 $x_t$와 $t-1$ 시점의 은닉 상태 $h_{t-1}$을 사용해 망각 게이트의 출력값을 계산하게 됩니다.

망각 게이트는 두 가중치를 통해 망각 게이트 출력값을 최적화 합니다.

망각 게이트 출력값은 메모리 셀을 계산하기 위한 가중치로 사용됩니다.

망각 게이트의 출력값은 0에서 1사이의 값으로 이 값이 1에 가까우면 더 많은 정보를 유지하고 반대로 0에 가까우면 더 많은 정보를 삭제합니다.

출력값이 정확히 1이면 아무런 정보를 삭제하지 않으며, 0이면 모든 정보를 삭제합니다.

메모리 셀을 계산하기 위해 기억 게이트 출력값을 더해 최종 메모리 셀 값을 계산합니다.

기억 게이트

![화면 캡처 2024-08-01 174702.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/623bfb43-d97f-493c-a869-2df802e137f4/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_174702.png)

기억 게이트 수식

$$
g_i = tanh(W^{(g)}_{x}x_t + W^{(g)}_{x}h_{t-1} + b^{(g)})
\\
i_i = sigmoid(W^{(i)}_{x}x_t + W^{(i)}_{x} + b^{(i)})
$$

기억 게이트는 $g_i$와 $i_i$로 구성돼 있으며 망각 게이트와 동이랗 연산으로 계산됩니다.

$g_i$는 활성화 함수로 하이퍼볼릭 탄젠트를 사용하며, $i_i$는 시그모이드를 사용합니다.

$g_i$는 -1에서 1사이의 값을 가지므로 이전 시점의 은닉 상태와 현재 시점의 입력값 모두 [-1, 1] 범위안에 존재합니다.

$g_i$만으로는 새로운 은닉 상태를 계산하기 위해 이 은닉 상태를 얼마나 기억할지 제어하기 어렵습니다.

그러므로 $i_i$값으로 새로운 은닉 상태 기억을 제어합니다.

$i_i$는[0, 1]의 값을 가지므로 현재 시점에서 얼마나 많은 정보를 기억할 것인지를 결정하는 가중치 역할을 합니다.

$i_i$가 1에 가까울수록 기억할 정보가 많아집니다.

반대로 0에 가까워질수록 정보를 망각하게 됩니다.

메모리 셀은 망각 게이트와 기억 게이트의 정보로 현재 시점의 메모리 셀 값을 계산합니다.

다음 수식은 최종 메모리 셀 계산 방법입니다.

$$
c_t = f_t \bigodot c_{t-1} + g_i \bigodot i_t
$$

$f_t$는 망각 게이트 출력값이며, $c_{t-1}$은 이전 시점의 메모리 셀 값을 나타냅니다.

이 값을 원소별 곱셈 연산을 의미하는 아다마르 곱(Hadamard Product, $\bigodot$)하면 현재 시점의 메모리 셀 값이 계산됩니다.

망각 게이트 값 $f_t$가 0에 가까울수록 이전 시점의 메모리 셀 값이 현재 시점의 메모리 셀 값에 영향을 미치지 않게 됩니다.

기억 게이트도 아다마르 곱을 통해 계산되며 망각 게이트와 합산됩니다.

망각 게이트는 이전 시점의 메모리 셀을 얼마나 유지할지 결정하며, 기억 게이트는 현재 시점의 새로운 정보를 얼마나 받아들일지를 결정합니다.

이렇게 메모리 셀이 계산된 후 어떤 정보를 출력할지를 결정합니다.

출력 게이트가 어떤 정보를 출력할지를 결정합니다.

출려 게이트

![화면 캡처 2024-08-01 174750.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/9aadcea4-2940-42e7-a3fc-ef24833f171e/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_174750.png)

출력 게이트 수식

$$
o_t = \sigma(W^{(o)}_xx_t + W^{(o)}_xh_{t-1} + b^{(o)}
$$

출력 게이트는 현재 시점의 은닉 상태를 제어합니다.

출력 게이트도 망각 게이트에서 사용된 수식과 동일한 수식을 사용하며, 활성화 함수로 시그모이드를 사용합니다.

시그모이드 함수는 입력값을 [0, 1] 범위로 변환하므로 현재 시점의 은닉 상태 값을 얼마나 사용할지 결정합니다.

출력 게이트와 현재 메모리 셀을 연산한다면 새로운 은닉 상태를 계산할 수 있습니다.

은닉 상태 갱신 수식

$$
h_t = o_t \bigodot tanh(c_t)
$$

현재 시점의 은닉 상태 $h_t$는 출력 게이트와 하이퍼볼릭 탄젠트를 적용한 메모리 셀 값으로 계산됩니다.

출력 게이트는 [0, 1]의 범위를 가지며, 하이퍼볼릭 탄젠트가 적용된 메모리 셀은 [-1, 1]범위를 갖습니다.

두 값을 아다마르 곱 연산하면 현재 시점의 은닉 상태가 이전 시점의 은닉 상태에서 얼마나 영향을 받는지 계산할 수 있습니다.

이러한 방법으로 장단기 메모리는 현재 은닉 상태에서 이전 은닉 상태의 정보의 중요도를 제어할 수 있습니다.

#### 장단기 메모리 클래스

장단기 메모리도 파이토치에서 쉽게 구현할 수 있도록 메모리 클래스를 제공합니다.

```python
lstm = torch.nn.LSTM(
	input_size,
	hidden_size,
	num_layer = 1,
	bias = False,
	batch_first = True,
	dropout = 0,
	bidirectional = False,
	proj_size = 0
)
```

장단기 메모리 클래스는 순환 신경망 클래스와 거의 유사한 구조를 갖습니다.

장단기 메모리 클래스는 활성화 함수를 명확하게 정의해 사용하므로 순환 신경망에서 사용하는 활성화 함수(nonlinearity) 매개 변수를 사용하지 않습니다.

그러므로 투사 크기(proj_size) 매개변수를 사용해 장단기 메모리 계층의 출력에 대한 선형 투사(Linear Projection) 크기를 결정합니다.

투사 크기가 0보다 큰 경우, 은닉 상태를 선형 투사를 통해 다른 차원으로 매핑합니다.

이 값을 통해 출력차원을 줄이거나 다른 차원으로 변환할 수 있습니다.

투사 크기가 0이라면 은닉 상태의 차원을 변환하지 않고 그대로 유지합니다.

장단기 메모리 클래스는 출력 차원을 조절할 때 투사 크기 매개변수로 모델의 크기와 복잡도를 변경할 수 있습니다.

그러므로 투사크기는 은닉 상태 크기(hidden_size)보다 작은 값으로 설정합니다.

LSTM 예제

```python
import torch
from torch import nn

input_size = 128
output_size = 256
num_layers = 3
bidirectional = True
proj_size = 64

model = nn.LSTM(
    input_size = input_size,
    hidden_size = output_size,
    num_layers = num_layers,
    batch_first = True,
    bidirectional = bidirectional,
    proj_size = proj_size
)

batch_size = 4
sequence_len = 6

inputs = torch.randn(batch_size, sequence_len, input_size)
h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, proj_size if proj_size > 0 else output_size)
c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)

outputs, (h_n, c_n) = model(inputs, (h_0, c_0))

print(outputs.shape)
print(h_n.shape)
print(c_n.shape)
```

```python
# 결과

torch.Size([4, 6, 128])
torch.Size([6, 4, 64])
torch.Size([6, 4, 256])
```

장단기 메모리 클래스는 순전파 연산시 입력값(inputs)과 초기 은닉 상태(h_0), 초기 메모리 상태(c_0)로 순방향 연산을 수행해 출력값(outputs)과 최종 은닉 상태(h_n), 최종 메모리 셀 상태(c_n)를 반환합니다.

먼지 입력값 차원은 앞선 순환 신경망 클래스에서 사용하던 입력값 차원 형태와 동일한 형태로 [배치 크기, 시퀀스 길이, 입력 특성 크기]를 전달합니다.

초기 은닉 상태도 순환 신경망 클래스와 동일한 구조를 갖지만, 장단기 메모리는 선형 투사를 통해 출력층의 차원을 변경할 수 있으므로 투사 크기가 0보다 큰 경우 출력층의 차원을 투사 크기로 입력합니다.

초기 메모리 셀은 순환 신경망 클래스의 초기 은닉 상태와 동일한 구조인 [계층 수 $\times$ 양방향 여부 + 1, 배치 크기, 은닉 상태 크기]를 갖습니다.

초기 메모리 셀은 선형 투사를 적용하더라도 은닉 상태 크기를 사용합니다.

순방향 연산 결과는 출력값과 최종 은닉 상태, 최종 메모리 셀 상태가 반환됩니다.

출력값은 수식도 순환 신경망과 동일한 수식을 사용하지만 선형 투사가 적용됐으므로 [배치 크기, 시퀀스 길이, (양방향 여부 + 1) $\times$ 투사 크기(또는 은닉 상태 크기)]로 구성되어 있습니다.

예제와 같은 구조라면 [4, 6, 2 $\times$ 64]의 형태로 반환됩니다.

최종 은닉 상태는 초기 은닉 상태와 동일한 차원인 [3 $\times$ 2, 4, 256] 형태로 반환됩니다.

순환 신경망 입출력 차원은 배치 우선 매개변수에 따라 차원의 형태가 달라지므로 매개변수 설정에 주의해야 합니다.

장단기 메모리는 순환 신경망과 비슷한 구조를 갖지만, 투사 크기가 적용되는 경우 차원 계산 방식이 달라집니다.

또한 메모리 셀의 상태도 입력으로 전달해야 하며, 은닉 상태와 메모리 셀 상태를 튜플로 묶어 사용하므로 사용에 주의해야합니다.

### 모델 실습

긍/부정 모델 분류

![화면 캡처 2024-08-01 182512.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/ec0282e9-88d7-4332-876f-cc0797f540c8/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_182512.png)

임베딩은 임베딩 벡터를 가져오는 임베딩 계층을 의미합니다.

임베딩 계층은 [어휘 사전의 크기 $\times$ 임베딩 벡터의 크기]로 순람표 구조를 갖습니다.

임베딩 계층은 입력 텍스트를 정수 인코딩한 뒤 해당 색인 값에 해당하는 임베딩을 가져오는 역하을 합니다.

초깃값으로는 무작위 값을 할당하고 학습을 통해 임베딩 계층이 최적화되게 만들거나, 사전 학습된 임베딩 벡터를 가져와 사용할 수 있습니다.

문장 분류 모델 코드

```python
from torch import nn

class SentenceClassifier(nn.Module):
    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_layers, dropout = 0.5, bidirectional = True, model_type = "lstm"):
        super().__init__()

        self.embedding = nn.Embedding(num_embeddings = n_vocab, embedding_dim = embedding_dim, padding_idx = 0)

        if model_type == "rnn":
            self.model = nn.RNN(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)
        elif model_type == "lstm":
            self.model = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)

        if bidirectional:
            self.classifier = nn.Linear(hidden_dim * 2, 1)
        else:
            self.classifier = nn.Linear(hidden_dim, 1)

        self.dropout = nn.Dropout(dropout)

    def forward(self, inputs):
        embeddings = self.embedding(inputs)
        output, _ = self.model(embeddings)
        last_output = output[:, -1, :]
        last_output = self.dropout(last_output)
        logits = self.classifier(last_output)
        return logits
```

SentenceClassifier 클래스는 임베딩 층을 구성할 때 사용하는 단어 사전의 크기(n_vocab)와 순환 신경망 클래스와 장단기 메모리 클래스에서 사용하는 매개변수를 입력으로 전달 받습니다.

또한, 모델 종류(model_type) 매개변수로 순환 신경망을 사용할지, 장단기 메모리를 사용할지 설정합니다.

초기화 메서드에서는 SentenceClassifier 클래스에 입력된 함수의 매개변수에 따라 모델 구조를 미세조정합니다.

분류기 계층은 모델을 양방향으로 구성한다면 전달되는 입력 채널 수가 달라지므로 분류기 계층을 현재 모델 구조에 맞게 변경합니다.

순방향 메서드에서는 입력받은 정수 인코딩을 임베딩 계층에 통과시켜 이베딩 값을 얻습니다.

얻은 임베딩 값을 모델에 입력하여 출력값을 얻습니다.

출력값(output)의 마지막 시점만 활용할 예정이므로 [:, -1, :]으로 마지막 시점의 결과값만 분리해 분류기 계층에 전달합니다.

SentenceClassifier 클래스를 선언했다면 모델 학습에 사용할 데이터세트를 불러옵니다.

```python
import pandas as pd
from Korpora import Korpora

corpus = Korpora.load("nsmc")
corpus_df = pd.DataFrame(corpus.test)
```

```python
# 결과

Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을
    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.

    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.
    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,
    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.

    # Description
    Author : e9t@github
    Repository : https://github.com/e9t/nsmc
    References : www.lucypark.kr/docs/2015-pyconkr/#39

    Naver sentiment movie corpus v1.0
    This is a movie review dataset in the Korean language.
    Reviews were scraped from Naver Movies.

    The dataset construction is based on the method noted in
    [Large movie review dataset][^1] from Maas et al., 2011.

    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/

    # License
    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
    Details in https://creativecommons.org/publicdomain/zero/1.0/

[Korpora] Corpus `nsmc` is already installed at C:\Users\user\Korpora\nsmc\ratings_train.txt
[Korpora] Corpus `nsmc` is already installed at C:\Users\user\Korpora\nsmc\ratings_test.txt
```

```python
train = corpus_df.sample(frac = 0.9, random_state = 42)
test = corpus_df.drop(train.index)

print(train.head(5).to_markdown())
print("Training Data Size :", len(train))
print("Testing Data Size :", len(test))
```

```python
# 결과

|       | text                                                                                     |   label |
|------:|:-----------------------------------------------------------------------------------------|--------:|
| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라.      |       1 |
|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                       |       0 |
|   199 | 신날 것 없는 애니.                                                                        |       0 |
| 12447 | 잔잔 격동                                                                                 |       1 |
| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                           |       1 |
Training Data Size : 45000
Testing Data Size : 5000
```

학습 데이터세트의 크기가 작은 테스트 데이터세트를 활용해 실습을 진행합니다.

corpus.test로 테스트 세트를 불러오고, 이 데이터세트를 학습 데이터와 테스트 데이터로 분리합니다.

학습 데이터세트와 테스트 데이터 세트로 분리했다면 토큰화 및 단어 사전을 구축합니다.

Okt 토크나이저를 사용해 데이터세트를 토큰화하고 단어 사전을 추구하는 방법을 알아보겠습니다.

```python
from konlpy.tag import Okt
from collections import Counter

def build_vocab(corpus, n_vocab, special_tokens):
    counter = Counter()
    for tokens in corpus:
        counter.update(tokens)
    vocab = special_tokens
    for token, count in counter.most_common(n_vocab):
        vocab.append(token)
    return vocab

tokenizer = Okt()
train_tokens = [tokenizer.morphs(review) for review in train.text]
test_tokens = [tokenizer.morphs(review) for review in test.text]

vocab = build_vocab(corpus = train_tokens, n_vocab = 5000, special_tokens = ["<pad>", "<unk>"])
token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for idx, token in enumerate(vocab)}

print(vocab[:10])
print(len(vocab))
```

```python
# 결과

['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']
5002
```

Okt 토크나이저를 사용해 단어를 토큰화하고 단어 사전을 구축합니다.

build_vocab 함수는 위에 있던 단어 사전구축 함수와 동일합니다.

단, 문장의 길이를 맞추기 위해 <pad> 토큰을 special_tokens에 추가합니다.

예제에서는 단어 사전의 최대 길이를 5000으로 입력했습니다.

특수 토큰은 현재 2개이므로 단어 사전은 5002개로 구성됩니다.

토큰화 및 단어 사전을 추국했다면 파이토치의 임베딩 층을 사용하기 위해 토큰을 정수로 변환합니다.

다음은 단어 사전을 통해 토큰을 정수 인코딩하고 패딩하겠습니다.

```python
import numpy as np

def pad_sequences(sequences, max_length, pad_value):
    result = list()
    for sequence in sequences:
        sequence = sequence[: max_length]
        pad_length = max_length - len(sequence)
        padded_sequence = sequence + [pad_value] * pad_length
        result.append(padded_sequence)
    return np.asarray(result)

unk_id = token_to_id["<unk>"]
train_ids = [[token_to_id.get(token, unk_id) for token in review] for review in train_tokens]
test_ids = [[token_to_id.get(token, unk_id) for token in review] for review in test_tokens]

max_length = 32
pad_id = token_to_id["<pad>"]
train_ids = pad_sequences(train_ids, max_length, pad_id)
test_ids = pad_sequences(test_ids, max_length, pad_id)

print(train_ids[0])
print(test_ids[0])
```

```python
# 결과

[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13
 4839    1    1    1    2    0    0    0    0    0    0    0    0    0
    0    0    0    0]
[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51
    3    1 4684    6    0    0    0    0    0    0    0    0    0    0
    0    0    0    0]
```

학습 데이터세트와 테스트 데이터세르를 단어 사전에 있는 정수로 인코딩합니다.

정수로 인코딩하면 모델이 텍스틀 처리하기 쉬워집니다.

컴퓨터는 텍스트를 처리하는 것보다 숫자를 처리하는 것이 훨씬 빠르고 효율적입니다.

텍스트 데이터를 수자로 변환함으로써 모델이 데이터를 더 빠르게 처리할 수 있게 됩니다.

최대 길이(max_length)는 입력 텍스트의 길이를 고려해 결정합니다.

너무 큰 값으로 설정하면 입력 행렬의 크기가 커져 많은 리소스를 필요로 하게 되며, 너무 짧은 경우는 문장을 제대로 반영하지 못 해 모델의 성능이 저하 될 수 있습니다.

pad_sequences 함수는 너무 긴 문장은 최대 길이로 줄이고, 너무 작은 길이라면 최대 길이와 동일한 크기로 변환합니다.

그러므로 pad_sequences 함수는 시퀀스를 최대 길이로 잘라내고, 시퀀스 길이가 작으면 <pad> 토큰을 시퀀스 뒤에 이어 붙여 동일한 길이로 변경합니다.

출력 결과에서 확인할 수 있듯이, OOV의 경우 1(<unk>)로 인코딩 되며, 시퀀스 길이가 짧은경우 최대 길이(32)가 될 수 있게 0(<pad>) 토큰이 추가가 됩니다.

정수 인코딩및 패딩이 완료되면 파이초이 데이터로더에 적용합니다.

```python
import torch
from torch.utils.data import TensorDataset, DataLoader

train_ids = torch.tensor(train_ids)
test_ids = torch.tensor(test_ids)

train_labels = torch.tensor(train.label.values, dtype = torch.float32)
test_labels = torch.tensor(test.label.values, dtype = torch.float32)

train_dataset = TensorDataset(train_ids, train_labels)
test_dataset = TensorDataset(test_ids, test_labels)

train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)
test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)
```

텐서 데이터 세트(TensorDataste) 클래스는 파이토치 텐서 형태를 입력값으로 받습니다.

따라서 정수 인코딩과 라벨값을 파이토치 텐서 형태로 변환합니다.

변환된 데이터세트는 데이터로더 클래스에 적용합니다.

데이터로더까지 적용됐다면 모델 학습을 위해 손실 함수와 최적화 함수를 선언합니다.

```python
from torch import optim

n_vocab = len(token_to_id)
hidden_dim = 64
embedding_dim = 128
n_layers = 2

device = "cuda" if torch.cuda.is_available() else "cpu"
classifier = SentenceClassifier(n_vocab = n_vocab, hidden_dim = hidden_dim, embedding_dim = embedding_dim, n_layers = n_layers).to(device)
criterion = nn.BCEWithLogitsLoss().to(device)
optimizer = optim.RMSprop(classifier.parameters(), lr = 0.001)
```

학습에 적용하려는 하이퍼파라미터를 선언합니다.

은닉 상태 크기는 64, 임베딩 벡터의 크기는 128로 적용하고 신경망을 두 개의 층으로 구성합니다.

현재 모델은 문장의 긍/부정을 분류하므로 손실 함수는 이진 교차 엔트로피 함수를 적용합니다.

BCEWithLogitsLoss 클래스는 BCELoss 클래스와 Sigmoid 클래스가 결합된 형태입니다.

BCELoss 클래스를 사요하는 경우, crierion(torhc.sigmoid(output), target)과 동일한 형태로 간주할 수 있습니다.

최적화 함수는 RMSProp(Root Mean Sqaure Propagation)을 적용합니다.

RMSProp은 모든 기울기를 누적하지 않고, 지수 가중 이동 평균(Exponentially Weighted Moving Average, EWMA)을 사용해 학습률을 조절합니다.

기울기 제곱 값의 평균값이 자아지며 학습률을 증가시키고, 그 반대일 경우 학습률을 감소시켜서 불필요한 지역 최솟값에 빠지는 것을 방지합니다.

RMSprop은 기울기의 크기가 큰 경우에는 빠른 수렴을 보이며, 작은 경우에는 더 작은 학습률을 유지함으로써 더 안정적으로 최적화를 수행할 수 있습니다.

손실 함수와 쵲거화 함수까지 선언했다면 모델 학습과 모델 평가 코드를 작성합니다.

모델 학습 중간에 학습이 잘 이뤄지고 있는지 확인하기 위해 일정 배치를 학습한 후 테스트 데이터세트로 손실값을 확인해봅니다.

```python
def train(model, datasets, criterion, optimizer, device, interval):
    model.train()
    losses = list()

    for step, (input_ids, labels) in enumerate(datasets):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = model(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % interval == 0:
            print(f"Train Loss {step} : {np.mean(losses)}")

def test(model, datasets, criterion, device):
    model.eval()
    losses = list()
    corrects = list()

    for step, (input_ids, labels) in enumerate(datasets):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = model(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())
        yhat = torch.sigmoid(logits)>.5
        corrects.extend(torch.eq(yhat, labels).cpu().tolist())

    print(f"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}")

epochs = 5
interval = 500

for epoch in range(epochs):
    train(classifier, train_loader, criterion, optimizer, device, interval)
    test(classifier, test_loader, criterion, device)
```

```python
# 결과

Train Loss 0 : 0.6936471462249756
Train Loss 500 : 0.693555216351431
Train Loss 1000 : 0.6799667379656038
Train Loss 1500 : 0.671034049086377
Train Loss 2000 : 0.6597666360478828
Train Loss 2500 : 0.6455000843204817
Val Loss : 0.5854670436808858, Val Accuracy : 0.7118
Train Loss 0 : 0.41112029552459717
Train Loss 500 : 0.5546985305473952
Train Loss 1000 : 0.5432005482209431
Train Loss 1500 : 0.5302174012217022
Train Loss 2000 : 0.5190471136855936
Train Loss 2500 : 0.5091006162284804
Val Loss : 0.44975384955589004, Val Accuracy : 0.7864
Train Loss 0 : 0.36678919196128845
Train Loss 500 : 0.42058366468685593
Train Loss 1000 : 0.4144672000786284
Train Loss 1500 : 0.41059464453995664
Train Loss 2000 : 0.4084507605765594
Train Loss 2500 : 0.40620398653990647
Val Loss : 0.4092938023539016, Val Accuracy : 0.803
Train Loss 0 : 0.7258260250091553
Train Loss 500 : 0.34512316864615905
Train Loss 1000 : 0.3494388313999364
Train Loss 1500 : 0.35010109212668955
...
Train Loss 1500 : 0.31984244648870985
Train Loss 2000 : 0.3215242485045225
Train Loss 2500 : 0.3212532159157011
Val Loss : 0.3992777406312406, Val Accuracy : 0.8248
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```

에폭마다 테스트 데이터세트로 모델의 검증 손실(Validation Loss)과 검증 정확도(Validation Accuracy)를 확인합니다.

검증 손실과 검증 정확도는 모델이 이전에 본 적이 없는 테스트 데이터세트로, 모델이 새로운 데이터를 얼마나 잘 예측하는지를 평가합니다.

출력 결과를 보면 데스트 데이터스에 대해 손실이 감소하며, 정확도가 상승하는 것을 확인할 수 있습니다.

모델 학습 과정에서 임베딩 계층을 비롯한 순환 신경망내의 여러 가중치가 최적화됩니다.

학습된 임베딩 계층의 가중치를 동일한 단어 사전을 사용하여 토큰의 임베딩 값으로 사용할 수도 있습니다.

다음 코드는 학습된 임베딩 계층에서 단어 사전의 임베딩 값을 추출하는 방법입니다.

```python
token_to_embedding = dict()
embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()

for word, emb in zip(vocab, embedding_matrix):
    token_to_embedding[word] = emb

token = vocab[1000]
print(token, token_to_embedding[token])
```

```python
# 결과

보고싶다 [ 0.29195192  0.07734357  0.21900596  0.208875    1.6980429  -0.60247594
  0.48469818  1.0964513   0.32399485  0.5273235  -0.49249032 -2.0570672
  0.2175732  -0.09554552 -1.460692    0.6207085   0.92655885 -0.8524688
 -1.0831031   0.63737863 -0.98294353 -3.2568374  -1.748478   -0.00537326
 -1.1126487  -0.5375635  -0.43660617  0.5276603  -1.8889349  -0.57526594
  0.9641603  -1.8456868  -1.1332587   0.56807715 -0.30232394  0.85906875
 -0.16163981  2.2777991   0.57848835  0.23577769 -0.33707178  0.8481684
 -0.4062612  -0.19665645  0.47921476  0.42694888  0.9631627   0.09467673
  0.35254973  0.59979534 -0.93250316 -0.52503943 -0.7277629   0.88731205
 -1.1003376  -0.656939    1.8419139   0.09664328  0.53495467  0.22263312
  1.6676962  -0.89426    -0.9639165   1.3346835  -1.6018201  -0.53803355
  0.98053575 -1.2546101   0.47262082 -0.0283177   1.6540885  -0.0413489
 -0.2534752  -0.7714252  -0.9710474   0.22851752  0.3020327  -1.0602528
  0.84030545 -1.4089813  -0.9714093  -0.16344643 -0.10205424  0.9217093
  0.78081304  1.0899807  -1.7331579   0.30227324  1.7415738  -0.27996987
 -0.7647882   1.9623847  -0.1892851   1.7307378  -0.08447721  0.5829974
 -0.33097792 -2.648708    0.8113952   1.7481744   1.1323535   0.10786305
 -0.39029574  0.63642025 -0.37435612  0.4045528  -1.4623537   0.14528565
 -0.73656785 -1.8010896  -0.6100502   2.485716    0.91768646 -0.26182017
  0.08681174 -0.55386645 -0.04040005  0.37809858  1.5102594  -0.23949634
 -0.9209239  -1.2474521   1.3977511   0.3281062  -0.27090222  0.132494
  0.61763215 -3.1052167 ]
```

학습된 모델의 임베딩 계층을 각 단어에 대한 임베딩으로 사용할 수 있습니다.

하지만 긍/부정 분류는 임베딩 계층이 아닌 순환 신경망의 연산이 더 중요하게 동작됩니다.

모델이 복잡할수록 계층이 토큰의 의미 정보를 학습하기는 더 어렵습니다.

사전 학습된 임베딩 값을 이용하여 임베딩 계층을 초기화는 코들 보겠습니다.

```python
from gensim.models import Word2Vec

word2vec = Word2Vec.load("models/word2vec.model")
init_embeddings = np.zeros((n_vocab, embedding_dim))

for index, token in id_to_token.items():
    if token not in ["<pad>", "<unk>"]:
        init_embeddings[index] = word2vec.wv[token]

embedding_layer = nn.Embedding.from_pretrained(torch.tensor(init_embeddings, dtype = torch.float32))
```

사전 학습된 모델로 임베딩 계층을 초기화하는 경우 넘파이 배열로 초기화할 수 있습니다.

이때 <pad> 토큰과 <unk>토큰은 초기화에서 제외됩니다.

임베딩 계층은 파이토치의 임베딩 클래스의 from_pretrained 메서드로 초기화할 수 있습니다.

이 값을 SentenceClassifier 클래스의 self.embedding 값으로 적용합니다.

```python
import torch
from torch import nn

class SentenceClassifier(nn.Module):
    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_layers, dropout = 0.5, bidirectional = True, model_type = "lstm", pretrained_embedding = None):
        super().__init__()
        if pretrained_embedding is not None:
            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype = torch.float32))
        else:
            self.embedding = nn.Embedding(num_embeddings = n_vocab, embedding_dim = embedding_dim, padding_idx = 0)

        if model_type == "rnn":
            self.model = nn.RNN(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)
        elif model_type == "lstm":
            self.model = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)

        if bidirectional:
            self.classifier = nn.Linear(hidden_dim * 2, 1)
        else:
            self.classifier = nn.Linear(hidden_dim, 1)

        self.dropout = nn.Dropout(dropout)

    def forward(self, inputs):
        embeddings = self.embedding(inputs)
        output, _ = self.model(embeddings)
        last_output = output[:, -1, :]
        last_output = self.dropout(last_output)
        logits = self.classifier(last_output)
        return logits
```

사전 학습된 임베딩(pretrained_embedding) 매개변수가 None이 아니면 전달된 값을 임베딩 계층으로 초기화합니다.

임베딩 층을 초기화하고 모델을 학습한 코드를 보겠습니다.

```python
from torch import optim

device = "cuda" if torch.cuda.is_available() else "cpu"
classifier = SentenceClassifier(n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers, pretrained_embedding=init_embeddings).to(device)
criterion = nn.BCEWithLogitsLoss().to(device)
optimizer = optim.RMSprop(classifier.parameters(), lr = 0.001)

epochs = 5
interval = 500

for epoch in range(epochs):
    train(classifier, train_loader, criterion, optimizer, device, interval)
    test(classifier, test_loader, criterion, device)
```

```python
# 결과

Train Loss 0 : 0.6985633373260498
Train Loss 500 : 0.6878796851563597
Train Loss 1000 : 0.6673794631715064
Train Loss 1500 : 0.641378102999064
Train Loss 2000 : 0.6173441011657839
Train Loss 2500 : 0.5973675453045615
Val Loss : 0.5139461361562101, Val Accuracy : 0.7546
Train Loss 0 : 0.5052717924118042
Train Loss 500 : 0.5279346922200597
Train Loss 1000 : 0.5285314464723909
Train Loss 1500 : 0.5197436976321612
Train Loss 2000 : 0.5131213341666007
Train Loss 2500 : 0.5057853771204569
Val Loss : 0.47390519553860916, Val Accuracy : 0.7762
Train Loss 0 : 0.45509421825408936
Train Loss 500 : 0.4739141018090848
Train Loss 1000 : 0.4729011731815862
Train Loss 1500 : 0.4710543490028
Train Loss 2000 : 0.4674724537706089
Train Loss 2500 : 0.46295987073181627
Val Loss : 0.4391772489959059, Val Accuracy : 0.7942
Train Loss 0 : 0.2982790470123291
Train Loss 500 : 0.44646073229417593
Train Loss 1000 : 0.4449532794339078
Train Loss 1500 : 0.444117646140706
...
Train Loss 1500 : 0.43100419906597787
Train Loss 2000 : 0.43113475958297753
Train Loss 2500 : 0.4308873453810185
Val Loss : 0.43869204571643194, Val Accuracy : 0.7736
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```

사전 학습된 임베딩을 사용하는 것은 모델성능을 개선할 수 있는 방법중 하나입니다.

하지만 학습데이터의 양이 충분히 많다면 모델의 목적에 맞게 새로운 임베딩 층을 학습하는 것이 더 좋은 결과를 얻을 수 도 있습니다.

사전 학습된 임베딩을 사용하더라도 해당 언어와 문제에 맞는 임베딩을 선택하는것이 중요합니다.

eg)  한국어 자연어 처리에는 문맥 정보를 고려한 임베딩 방법이 더 성능이 좋을 수 있습니다.

모델의 목적과 데이터의 특성을 고려하여 적절한 임베딩 방법을 선택하는 것이 중요합니다.

## 합성곱 신경망

합성곱 신경망(Convolutional Neural Network, CNN)은 주로 이밎 인식과 같은 컴퓨터비전 분야의 데이터를 분석하기 위해 사용되는 인공 신경망의 한 종류 입니다.

합성곱 신경망은 입력 데이터의 지역적인 특징을 추출하는데 특화된 구조를 가지고 있으며 이를 위해 합성곱(Convolution) 연산을 사용합니다.

합성곱 연산은 이미지의 특정 영역에서 입력값의 분포 또는 변화량을 계산해 출력 노드를 생성합니다.

특정 영역 안에서 연산을 수행하므로 지역 특징(Local Features)을 효과적으로 추출할 수 있습니다.

이미지 데이터는 고정된 프레임 내에 객체들의 위치와 형태가 자유분방하므로 여러 영역의 지역 특징을 조합해 입력 데이터의 전반적인 전역 특징(Global Features)을 파악할 수 있습니다.

합성곱 신경망은 원래 컴퓨터비전을 위해 고안된 인공 신겨망이지만, 자연어 처리 작업에서도 우수한 성능을 보입니다.

앞선 순환 신경망은 이전 시점$(t-1)$의 상태를 기억해 현재상태$(t)$를 계산하는 데 유용한 구조지만, 연산 순서의 제약으로 병렬 처리가 어렵다는 단점이 있습니다.

입력 데이터의 길이가 길어질수록 처리 속도가 느려지고 많은 수의 가중치를 사용하면 학습이 어려워지는 문제가 있어 깊은 신경망을 구성하기가 어렵습니다.

하지만 2014년 사전 학습된 임베딩과 합성곱 신경망을 이용한 문장 분류모델의 등장으로 자연어 처리에서도 합성곱 신경망이 사용되기 시작했습니다.

입력 데이터의 길이에 상관없이 병렬 처리가 가능하고, 학습에 필요한 가중치 수를 줄여 깊은 신경망을 구성할 수 있게 됐습니다.

합성곱 신경망을 활용해 자연어 처리를 수행하는 방법을 알아보겠습니다.

### 합성곱 계층

합성곱 계층은 입력 데이텨와 필터를 합성곱해 출력 데이터를 생성하는 계층입니다.

합성곱 계층은 이미지나 음성 데이터와 같은 고차원 데이터를 처리하는데 주로 사용됩니다.

합성곱 계층은 필터를 사용해 데이터의 특징을 추출하므로 데이터의 지역적인 패턴을 인식할 수 있으며, 입력 데이터의 모든 위치에서 동일한 필터를 사용하므로 모델 매개변수를 공유합니다.

모델의 매개변수를 공유함으로써 모델이 학습해야 할 매개변수 수가 감소해 과대적합을 방지합니다.

또한 입력 데이터에서 특징을 추출할 때, 해당 특징이 이미지 내 다른 위치에 존재하더라도 필터를 사용해 특징을 추출하므로 특징이 어디에 있어도 동일하게 추출할 수 있습니다.

합성곱 계층을 여러 겹 쌓아 모델을 구성하며, 합성곱 계층이 많아질수록 모델의 복잡도가 증가하므로 더 다양한 특징을 추출해 학습할 수 있습니다.

#### 필터

합성곱 계층은 입력 데이터에 필터(Filter)를 이용해 합성곱 연산을 수행하는 계층입니다.

필터는 커널(Kernelr) 또는 윈도(Window)로 불리기도 하며, 일반적으로 $3 \times 3, 5 \times 5$와 같은 작은 크기의 정방형으로 구성됩니다.

이 필터를 일정 간격으로 이동하면서 입력 데이터와 합성곱 연산을 수행해 특징 맵을 설정합니다.

필터 영역마다 합성곱 연산이 수행되며, 필터의 가중치가 이 모델 학습 과정에서 갱신됩니다.

합성곱 신경망은 보통 여러 개의 필터를 사용해 다양한 특징을 추출하며, 이를 통해 입력 이미지의 다양한 특징을 인식하고 분류하 수 있습니다.

다음 그림은 $4 \times 4$크기의 이미지에 $3 \times 3$크기의 필터를 1간격만큼 이동하며 특징 맵을 추출하는 과정입니다.

![화면 캡처 2024-08-01 194102.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/8f95cd7e-b13c-484e-bee0-3c9bcfedd7c0/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_194102.png)

합성곱 연산을 통해 $2 \times 2$ 특징 맵을 추출 과정을 보여줍니다.

$4 \times 4$입력 이미지의 각 픽셀 값에 $3 \times 3$필터를 1간격만큼 이동시키며 합성곱 연산을 수행합니다.

입력값은 이미지 픽셀값을 의미하며, 필터의 값은 학습을 통해 변경되는 가중치로 임의의 초깃값을 설정합니다.

합성곱 연산은 이미지에서 필터와 대응하는 부분을 곱한 후, 모두 더한 값을 특징 맵의 한 원소로 사용합니다.

필터는 입력 이미지 왼쪽 상단부터 오른쪽 하단으로 이동하면서 합성곱 연산을 반복하며 ㅇ특징 맵 전체를 계산합니다.

특징맵 (1, 1) 위치의 값을 계산하면, $(8 \times 3) + (5 \times 0) + (13 \times 1) + (6 \times 2) + \cdots + (7 \times 3)$이 되어 총합은 $209$가 됩니다.

똑같이 (1, 2), (2, 1), (2, 2) 위치도 반복해 124, 158, 87의 값을 $2 \times 2$ 특징 맵에 할당합니다.

이러한 특징 맵은 다음 합성곱 계층의 입력으로 사용되어 동일한 합성곱 연산 과정을 반복합니다.

각 계층에서 하나의 필터가 여러 번 사용되고 이를 공용 가중치로 공유함으로써 이미지 내에서 어느 위치에서도 동일한 패턴을 학습할 수 있게 됩니다.

#### 패딩

합성곱 신경망의 기존 방식대로 일반적으로 합성곱 연산을 수행하면 출력값인 특징 맵의 크기가 작아집니다.

입력값의 크기가 $4 \times 4$이었지만, $2 \times 2$크기로 갑소합니다.

이는 합성곱 신경망을 더 깊게 쌓는 데 제약사항이 될 수 있습니다.

합성곱 신경망 성능에 악영향을 끼칠 수 있습니다.

또한, 이미지의 가장자리에 있는 정보는 다른 위치에 있는 정보에 비해 학습하기 더 어렵습니다.

합성곱 신경망의 기존 방식대로 계산을 하게 되면 (1, 1) 위치의 픽셀값 8은 특징 맵을 계산하기 위해 한 번만 수식에 포합되어 영향력이 낮으며 (2, 2) 위치의 픽셀값 4는 네 번의 연산에 관여합니다.

이와 같이 가장자리 부분의 정보가 학습되는 데 제한이 있을 수 있습니다.

이러한 현상을 방지하기위해 입력 이미지나 입력으로 사용되는 특징 맵 가장자리에 특정 값을 덧 붙이는 패딩(Padding)을 추가합니다.

가장자리에 덧붙이는 패딩 값은 0으로 할당하는데, 이를 제로 패딩(Zeor padding)이라고 합니다.

다음 이미지는 $4 \times 4$크기의 이미를 제로 패딩한 뒤 $3 \times 3$ 크기의 필터를 통해 $4 \times 4$ 크기의 특징 맵을 계산하는 방법을 보여줍니다.

![화면 캡처 2024-08-01 194346.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/aa921dd6-8a5e-4ebf-bfa4-067007abe5f1/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_194346.png)

패딩을 추가하면 출력값의 크기가 작아지지 않고 입력값 크기와 동일한 $4 \times 4$크기로 특징 맵이 계산됩니다.

합성곱 신경망에서는 패딩을 적절히 사용해 입력 데이터의 크기를 조정하고, 작은 크기의 필터로도 이미지 전체에 대한 정보를 반영할 수 있습니다.

#### 간격

간격(Stride)이란 필터가 한 번에 움직이는 크기를 의미합니다.

간격의 크기가 1이면 필터가 한 픽셀씩 이동하면서 합성곱 연산을 수행하며, 간격의 크기가 2 이상이면 필터가 더 큰 간격으로 이동하면서 계산을 수행합니다.

간격의 크기를 조절함으로써 출력 데이터의 크기를 조절 할 수 있습니다.

eg) 패딩에서 예시를 들었던 그림은 간겨을 1로 설정해 필터가 한 픽셀씩 이동하면서 출력 데이터를 생성하므로 입력 데이터와 동일한 크기의 출력 데이터가 생성됩니다.

간격의 크기를 2 이상으로 할당했다면 필터가 더 큰 간격으로 이동하므로 출력 데이터의 크기는 입력 데이터보다 작아집니다.

간격을 조정함으로써 입력 데이터의 공간적인 정보를 유지하거나 감소시킬 수 있습니다.

입력 데이터의 공간적인 정보는 픽셀 간의 상대적인 위치나 거리에 대한 정보를 의미합니다.

eg) 이미지에서 픽셀 간의 상대적인 위치나 거리에 대한 정보는 이미지의 형태나 구조를 나타내는데 중요한 역할을 합니다.

간격을 작게 설정하면 입력 데이터의 공간적인 정보를 보존할 수 있으며, 간격을 크게 설정하면 입력 데이터의 공간적인 정보를 감소시킬 수 있습니다.

간겨을 조절해 합성곱 신경망이 학습해야 하는 모델 매개변수의 수를 감소시킬 수 있으며, 이를 통해 모델의 복잡도를 낮추고 과대적합을 방지할 수 있습니다.

#### 채널

입력 데이터와 필터 간의 연산은 채널(Channel) 에서 수행됩니다.

채널은 입력 데이터와 필터가 3차원으로 구성되어 있을 떄 같은 위치의 값끼리 연산되게 합니다.

이를 통해 입력 데이터의 공간 정보를 유지하면서 추출되는 특징을 확장할 수 있습니다.

eg) 입력 데이터가 RGB 이미지라면 각각의 R, G, B 채널마다 동일한 필터가 조내하며, 각 필터는 해당 체널의 정보를 추출해 특징 맵을 생성합니다.

특징 맵의 개수는 채널의 개수만큼 존재합니다.

만약 패딩의 그림의 입력디 RGB 이미지였다면, 하나의 체널에 대해 연산한 결과가 됩니다.

채널 개수는 일반적으로 합성곱 계층에서 설정되며, 이는 모델의 구조나 목적에 따라 달라집니다.

채널의 개수가 많아질술고 학습할 수 있는 특징의 다양성이 증가해 모델의 표현력(Representational Power)이 높아지는 효과를 가져옵니다.

출력 채널의 많은 경우 각 채널은 입력 데이터에서 서로다른 특징을 학습할 수 있습니다.

따라서 모델은 더 많은 종류의 특징을 학습하게 되며 그로인해 더 복잡한 문제를 해결할 수 있는 능력을 갖추게 됩니다.

그러나 출력 채널이 많을수록 모델의 매개변수가 많아지므로 학습 시간과 메모리 사용량이 증가하는 단점을 갖게 됩니다.

모델이 과대적합 되는 위험이 발생할 수도 있습니다.

#### 팽창

팽창(Dilation)이란 합성곱 연산을 수행할 때 입력 데이터에 더 넗은 범위의 영역을 고려할 수 잇게 하는 기법입니다.

팽창은 필터와 입력 데이터 사이에 간격을 두는 방버입니다.

일반적으로 합성곱 계층에서 팽창은 1로 사용해 간격을 한 칸만 띄워 사용합니다.

이 값이 커질수록 필터가 입력 데이터를 바라보는 범위가 넓어집니다.

![화면 캡처 2024-08-01 201925.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/b377f7ee-d3b2-442b-b2f8-b14f37b0bbd4/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_201925.png)

위 그림은 $6 \times 6$크기의 이미지에서 $3 \times 3$ 크기의 필터를 1 간격만큼 이동하고 패딩이 0일 때 특징 맵을 추출하는 과정을 시각화한 것입니다.

팽창 값이 1인 경우 패딩의 합성곱 연산 방식과 동일합니다.

팽창 값이 2인 경우 입력 데이터를 한칸 씩 건너뛰면서 합성곱 연산을 수행합니다.

그러므로 입력 데이터의 각 픽셀이 출력에 미치는 영향이 강화되는 효과를 갖습니다.

팽창은 필터의 크기를 키우지 않고 입력 데이터에 더 넓은 영역을 고려할 수 있게 해 더 깊고 복잡한 모델을 구성할 수 있게 합니다.

팽창을 적절히 적용하면 모델의 매개변수 수를 줄일 수 있어 메모리 사용량을 줄일 수 있습니다.

하지만 필터가 바라봐야 하는 입력 데이터의 범위가 커지므로 오히려 연산량이 늘어날 수도 있습니다.

또한 팽창 크기가 너무 크다면 인접한 픽셀값을 고려하지 않게 되므로 공간적인 정보가 보존되지 않아 특징 추출의 효과가 떨어질 수 있습니다.

#### 합성곱 계층 클래스

파이토치에서는 합성곱 계층 클래스를 사용해 간단하게 신경망을 구현할 수 있습니다.

다음은 2D 합성곱 계층클래스입니다.

```python
conv = torch.nn.Conv2D(
	in_channels,
	out_channels,
	kernel_size,
	stride = 1,
	padding = 0,
	dilation = 1,
	groups = 1,
	bias = True,
	padding_mode = "zeros"
)
```

합성곱 계층 클래스는 입력 데이터 채널 크기(in_channels)와 출력 데이터 채널 크기(out_channels)를 통해 채널을 생성합니다.

앞선 선형 변환 클래스의 입력 데이터 차원 크기(in_features)와 출력 데이터 차원 크기(out_features) 매개변수와 동일한 역할을 합니다.

커널 크기(kernel_size)는 합성곱 연산의 필터 크기를 의미하며, 간격(stride), 패딩(padding), 팽창(dilation)은 앞서 설명한 필터의 속성을 의미합니다.

그룹(groups)은 입력 체널과 출력 채널을 하나의 그룹으로 묶는 것을 의미합니다.

그룹을 2 이상의 값으로 설정하면 입력 채널과 출력 채널을 더 작은 그룹으로 나눠 각각 합성곱 연산을 수행합니다.

그룹을 사용하면 합성곱 연산 시 모델의 매개변수 수를 줄일 수 있습니다.

그룹의 값이 2 이상이면 입력 채널과 출력 채널의 수가 그룹수로 나누어 떨어야합니다.

입력 채널이나 출력 채널이 그룹 수의 배수여야합니다.

편향 설정(bias)은 계층에 편향 값 포함 여부를 설정합니다.

패딩 모드(padding_mode)는 패딩 영역에 할당되는 값을 설정합니다.

zeros는 제로 패딩을 의미하며, reflect는 입력 데이터의 가장자리를 거울처럼 반사해 값을 할당합니다.

replicate는 가장자리의 값을 복사해 값을 할당합니다.

합성곱 계층은 필터를 통해 입력 데이터의 공간적인 정보를 유지하거나 감소시킵니다.

출력 크기를 합성곱 계층에서 제어할 수 있습니다.

다음 수식은 합성곱 계층 출력 크기 입니다.

$$
L_{out} = \left\lfloor \frac{L_{{in}} + 2 \times padding - dilation \times (kernel\_size - 1) - 1}{stride} + 1 \right\rfloor

$$

합성곱 계층을 통화는 입력값의 크기$(L_{in})$는 위의 수식과 같은 형태로 출력 크기$(L_{out})$가 계산됩니다.

팽창의 dilation 1을 계산하게 된다면 다음 수식과 같아집니다.

$$
\begin{matrix}
L_{\text{in}} &=& 6, \;
kernel\_size = 3, \;
stride = 1, \;
padding = 0, \;
dilation = 1 \\
L_{out} &=& \left\lfloor \frac{6 + 2 \times 0 - 1 \times (3 - 1) - 1}{1} + 1 \right\rfloor = 4
\end{matrix}
$$

수식에서 확인할 수 있듯이 갑에 따라 소수점이 발생할 수 있습니다.

위 수식의 간격이 1이 아닌 2라면 출력 크기는 2.5가 됩니다.

이미지의 픽셀 위치는 소수점으로 할당할 수 없어 정수형으로 변환합니다.

그래서 소수점은 버림 처리되어 2의 값이 반환됩니다.

### 활성화 맵

활성화 맵(Activation Map)은 합성곱 계층의 특징 맵에 활성화 함수를 적용해 얻어진 출력값을 의미합니다.

합성곱 계층에서 입력 이미지와 필터의 합성곱 연산을 통해 특징 맵을 추추랗면 이 값에 비선형성을 추가하기 위해 활성화 함수를 적용합니다.

일반저긍로 합성곱 신경망에서는 ReLU 함수가 적용되며, 이를 통해 특징 맵의 값이 0보다 크면 그 값을 그대로 출력하고, 0이하일 경우에는 0을 출력합니다.

이렇게 얻은 활성화 맵은 다음 계층의 입력값으로 사용됩니다.

다음 계층이 합성곱 계층이라면 동일한 방식의 연산이 수행돼 합성곱 연산과 활성화함수를 거치게 됩니다.

합성곱 연산과 활성화 함수를 여러 번 반복하여 신경망을 구성하면 입력 이미지에서 추출된 추상적인 특징을 학습할 수 있게 됩니다.

합성곱 계층의 출력값에 활성화 함수를 적용하지 않으면 합성곱 연산 결괏값이 그대로 다음 계층으로 전달됩니다.

이 경우 모델이 선형적인 결합만 수행하게 되어 복잡한 패턴이나 추상적인 특징을 학습하는 것이 어려워집니다.

따라서 활성화 함수를 적용함으로써 모델이 비선형성을 가지게 되어 입력 데이터에서 다양한 추상적인 특징을 학습할 수 있게 됩니다.

활성화 맵은 합성곱 모델을 분석하는데 매우 중요한 정보를 제공합니다.

활성화 맵을 시각화하여 확인하면 이미지에서 모델이 어떤 특징을 학습하는지, 어떤 부분이 활성화되어 잇는지 등을 이해할 수 있습니다.

### 풀링

풀링(Pooling)은 특징 맵의 크기를 줄이는 연산으로 합성곱 계층 다음에 적용됩니다.

풀링은 특징 맵의 크기를 줄여 연산량을 감소시키고 입력 데이터의 정보를 압축하는 효과를 가집니다.

풀링은 합성곱 연산과 비슷하게 필터와 간격을 이용합니다.

일정한 크기의 필터 내 특정 값을 선택합니다.

선택하느 방버에는 크게 최댓값 풀링(Max Pooling)과 평균값 풀링(Average Pooling)이 있습니다.

최댓값 풀링은 특정 크기의 필터 내 원솟값 중 가장 큰 값을 선택해 특징 맵의 크기를 감소시키며, 평균값 풀링은 필터 내 원솟값의 평균값으로 특징 맵의 크기를 감소시킵니다.

다음 이미지는 최댓값 풀링 과정입니다.

![화면 캡처 2024-08-01 205415.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/73be633e-a41e-427f-9c97-f0e3f907f149/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_205415.png)

위 그림은 $4 \times 4$ 크기의 특징 맵에 $2 \times 2$크기의 최댓값 풀링을 2간격으로 적용했을 때 결과값을 보여줍니다.

풀링은 입력 데이터의 공적 크기를 줄이기 때문에 계산 비용을 감소시킬 수 있으며, 입력 데이터의 특징 위치가 변경되더라도 인근 영역에 대한 연산을 적용하기 때문에 공간적 정보를 유지할 수 있습니다.

하지만 데이터의 위치 정보를 일부 손실시키기 때문에 세밀한 위치 정보가 필요한 작업에서는 성능 저하를 초래할 수 있습니다.

이로 인해 최근에는 풀링을 이용해 공간적 크리르 감소하는 방법보다 연산량이 더 많더라도 합성곱 계층의 간격을 설정해 이력 데이터의 공간적인 크기를 줄이는 방법을 사용합니다.

#### 풀링 클래스

파이토치에서는 풀링 클래스로 간단히 풀링을 적용할 수 있습니다.

2차원 최댓값 풀링 클래스

```python
pool = torch.nn.MaxPool2d(
	kernel_size,
	stride = None,
	padding = 0,
	dilation = 1
)
```

2차원 최댓값 풀링 클래스도 합성곱 계층 클래스에서 사용했던 것과 동일한 매개변수를 사용합니다.

특징 맵 내의 최댓값을 선택하는 방법이므로 패딩 모드가 적용되지 않습니다.

기본적인 구조가 합성곱 계층 클래스와 동일하기 때문에 출력 크기 계산 방식도 합성곱 계층의 수식과 동일합니다.

2차원 평균값 풀링 클래스

```python
pool = torch.nn.AvgPool2d(
	kernel_size,
	stride = None,
	padding = 0,
	count_include_pad = True
)
```

2차원 평균값 풀링 클래스도 합성곱 계층 클래스에서 사용했던 매개변수와 거의 동일합니다.

하지만 평균값으로 풀링해야 하므로 팽창은 지원하지 않습니다.

팽창의 경우 원소 간의 거리가 멀어지므로 주변 영역의 특징계산이 어려워집니다.

패딩 포함(count_include_pad)은 패딩 영역의 값을 평균 계산에 포함할지 여부를 설정합니다.

참값으로 사용하면 제로 패딩의 값이 평균값 풀링 연산에 포함됩니다.

다음 수식은 2D 평균값 풀링 클래스의 출력 크리 계산 방법입니다.

$$
L_{out} = \left\lfloor \frac{L_{in} + 2 \times padding - kernel\_size}{stride} + 1 \right\rfloor
$$

### 완전 연결 계층

완전 연결 계층(Fully Connected Layer, FC)은 각 입력 노드가 모든 출력 노드와 연결된 상태를 의미합니다.

이를 통해 완전 연결 계층은 입력과 출력 간의 모든 가능한 관계를 학습할 수 있습니다.

이 계층은 출력 노드의 수를 조절할 수 있으므로 모델의 복잡성과 용량을 조절하는데 사용합니다.

eg) 이전 계층의 출력 데이터가 2차원 배열 형태인 경우 1차원 백터 형태로 변경해 출력할 수 있습니다.

일반적으로 합성곱 신경망에서는 합성곱 계층과 풀링 계층을 거친 결과물인 특징 맵을 입력으로 받습니다.

입력된 3차원 특징 맵은 평탄화 작업을 통해 1차원 벡터로 변경하고 완전 연결 계층의 가중치와 내적 연산을 수행해 출력값을 계산합니다.

전체 입력 특징 맵과 가중치 간의 내적 연산을 수행해 출력값을 계산하므로 이전 계층에서 추출한 특성 맵의 공간 정보가 무시되고 모든 입력을 독맂벅으로 처리해 계산합니다.

합성곱 신경망에서는 특성 맵의 공간 정보를 보존하기 위해 합성곱 계층으로 구성된 네트워크를 구성하고 이후에 완전 연결 계층을 추가하여 고수준 작업을 수행합니다.

완전 연결 계층은 이전 계층에서 추출된 특징을 활용하여 최정적인 분류 작업을 수행합니다.

최정값은 소프트맥스나 시그모이드와 같은 활성화 함수를 적용해 분류 모델로 구성할 수 있습니다.

따라서 완전 연결 계층은 합성곱 신경망의 최종 출력을 결정하는 역할을 합니다.

다음 그림은 간단한 합성곱 신경망의 모델 구조입니다.

![화면 캡처 2024-08-01 211411.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/c3f436a8-5fe9-4545-a3f4-080a6b03ed18/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_211411.png)

입력 이미지 → 합성곱 계층, ReLU, 최댓값 풀링을 2번 반복하고 3차원 배열을 1차원 베터로 펼치는 flatten 연산을 수행한 것을 나타냈습니다.

이 1차원 벡터를 완전 연결 계층으로 전달해 10개의 출력 벡터를 반환합니다.

10개의 출력 벡터에 소프트맥스나 시그모이드를 적용하면 이미지를 입력했을때 클래스를 분류하는 모델로 구축할 수 있습니다.

코드로 구현하게 된다면 다음과 같습니다.

```python
import torch
from torch import nn

class CNN(nn.Module):
    def __init__(self):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 2, padding = 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2)
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2)
        )

        self. fc = nn.Linear(32 * 32 * 32, 10)
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = torch.flatten(x)
        x = self.fc(x)
        return x
```

완전 연결 계층은 선형 변환으로 구성되며 입력 벡터와 가중치를 곱으로 계산됩니다.

완전 연결 계층의 입력 데이터 차원 크기는 마지막 특징 맵의 높이 $\times$ 너비 $\times$ 채널로 계산됩니다.

출력 데이터 차원의 크기가 분류 모델의 클래스 개수가 됩니다.

### 모델 실습

수직/수평 방향으로 이동하며, 합성곱 연산을 수행하는 2차원 합성곱(2-Dimensional Convolution)에 대해 알아봤습니다.

하지만 텍스트 데이터의 임베딩 값은 순서를 제외하면 입력값의 위치가 의미를 가지지 않습니다.

그러므로 텍스트 데이터에서도 이미지 데이터와 같이 2차원 합성곱 필터를 사용하면 텍스트의 정보를 제대로 학습할 수 없습니다.

다음 이미지는 이미지 데이터와 텍스트 데이터에 2차원 합성곱을 수행했을 때의 예시입니다.

![화면 캡처 2024-08-01 211525.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/8aed6fad-c5fe-401e-82d1-ea8bed180283/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_211525.png)

텍스트 데이터는 2차원 합성곱이 아닌, 1차원 합성곱(1-Dimensional Convolution)을 적용해야 합니다.

1차원 합성곱은 입력 데이터가 1차원 베터인 경우에 대한 합성곱 연산을 수행합니다.

1차원 합성곱은 일반적으로 텍스트 데이터 처리에서 많이 사용됩니다.

텍스트 데이터는 문장을 단어 단위로 분리하여 각 단어를 임베딩하여 나온 1차원 벡터 데이터를 입력값으로 사용합니다.

1차원 합성곱은 이러한 입력값에서 수평 방향으로 이동하지 않고 수직 방향으로만 이동하는 필터를 적용하여 해당 문장의 특징을 추출합니다.

1차원 필터의 크기는 필터의 높이에만 영향을 미치며, 필터의 너비는 입력 이베딩의 크기가 됩니다.

다음 그림은 텍스트 임베딩 입력값에 크기가 3인 1차원 합성곱 연산을 1씩 이동하며 수행하는 것을 보여줍니다.

![화면 캡처 2024-08-01 211616.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/74ae8612-3c0a-44a7-94bb-58e743ed92e7/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_211616.png)

필터 크기가 3인 합성곱 임베딩을 수행하면 인접한 3개의 토큰에 대해 연산을 수행합니다.

이는 앞서 다룬 N-gram과 유사한 개념입니다.

텍스트 순서에 따른 정보를 학습할 수 있습니다.

1차원 합성곱을 이용한 신경망에서는 다양한 크기의 합성곱 필터를 사용하여 여러 종류의 정보를 추출할 수 있습니다.

1차원 합성곱을 수행하면 1차원 벡터를 출력값으로 얻게 되므로 풀링을 적용하면 하나의 스칼라 값이 도출 됩니다.

크기가 다른 여러 개의 합성곱 필터를 사용하면 여러 개의 스칼라값을 얻을 수 있고 이러한 다양한 크기의 출력 벡터를 모아 하나의 벡터로 연결하여 하나의 특징 벡터로 만들 수 있습니다.

이 특징 벡터를 이용해 분류, 예측 등의 작업을 수행할 수 있습니다.

다음 그림은 크기가 각각 2, 3, 4인 필터를 2개씩 사용하여 출력 특징 벡터를 얻는 것을 보여줍니다.

![화면 캡처 2024-08-01 213233.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f55d134-2041-43fe-9a55-eef541222464/478913e2-5389-42f8-9422-c9b9cfe1544b/%ED%99%94%EB%A9%B4_%EC%BA%A1%EC%B2%98_2024-08-01_213233.png)

순환 신경망과 동일하게 출력값에 완전 연결 계층을 추가해 분류 모델을 구성합니다.

합성곱 신경망을 이용한 문장 분류는 사전 학습된 임베딩 벡터를 사용하는 것이 일반적이며, 그렇지 않을 경우 분류 성능이 저하될 수 있습니다.

따라서 이미 학습한 모델로 임베딩 층을 초기화해 학습합니다.

다음 코드는 합성곱 기반 문장 분류 모델 정의 방법을 보여줍니다.

```python
import torch
from torch import nn

class SentenceClassifier(nn.Module):
    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout = 0.5):
        super().__init__()

        self.embedding = nn.Embedding.from_pretrained(
            torch.tensor(pretrained_embedding, dtype = torch.float32)
        )
        
        embedding_dim = self.embedding.weight.shape[1]

        conv = []

        for size in filter_sizes:
            conv.append(
                nn.Sequential(
                    nn.Conv1d(in_channels = embedding_dim, out_channels = 1, kernel_size = size),
                    nn.ReLU(),
                    nn.MaxPool1d(kernel_size = max_length - size - 1)
                )
            )
        
        self.conv_filters = nn.ModuleList(conv)

        output_size = len(filter_sizes)
        self.pre_classifier = nn.Linear(output_size, output_size)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(output_size, 1)

    def forward(self, inputs):
        embeddings = self.embedding(inputs)
        embeddings = embeddings.permute(0, 2, 1)

        conv_outputs = [conv(embeddings) for conv in self.conv_filters]
        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim = 1)

        logits = self.pre_classifier(concat_outputs)
        logits = self.dropout(logits)
        logits = self.classifier(logits)
        
        return logits
```

SentenceClassifier 클래스는 사전 학습된 임베딩 벡터(pretrained_embedding), 필터 크기 리스트(filter_sizes), 최대 길이(max_length)를 입력 받아 모델을 구성합니다.

합성곱 계층 구성 시 위 그림과 같이 크기가 다른 여러 개의 합성곱 필터를 구성하기 위해 필터 크기 리스트만큼 시퀀셜을 생성합니다.

각 시퀀스는 1차원 합성곱 계층, ReLU, 최댓값 풀링으로 구성됩니다.

합성곱 계층의 출력 채널 수는 1로 설정합니다.

최댓값 풀링의 커널 크기는 앞선 합성곱 연산의 영향을 받으므로 최대 길이 - 필터 크기 - 1로 설정합니다.

ModuleList 클래스는 여러 개의 서브 모듈을 리스트 형태로 저장하는 기능을 제공합니다.

conv 리스트에는 시퀀스 모듈이 담겨 있으므로 ModuleList 클래스를 적용합니다.

순방향 메서드에서는 입력값(inputs)을 임베딩 계층에 통과시켜 임베딩 벡터를 얻습니다.

이후 permute 메서드르 통해 차원을 변경하고 앞서 설정한 여러 개의 합성곱 필터에 통과 시킵니다.

cat 메서드를 통해 각 필터의 출력값을 이어 붙여 하나의 벡터로 만들고 분류를 위한 네트워크를 통과시켜 최종 출력값을 얻습니다.

```python
import pandas as pd
from Korpora import Korpora

corpus = Korpora.load("nsmc")
corpus_df = pd.DataFrame(corpus.test)
```

```python
# 결과

    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을
    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.

    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.
    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,
    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.

    # Description
    Author : e9t@github
    Repository : https://github.com/e9t/nsmc
    References : www.lucypark.kr/docs/2015-pyconkr/#39

    Naver sentiment movie corpus v1.0
    This is a movie review dataset in the Korean language.
    Reviews were scraped from Naver Movies.

    The dataset construction is based on the method noted in
    [Large movie review dataset][^1] from Maas et al., 2011.

    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/

    # License
    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
    Details in https://creativecommons.org/publicdomain/zero/1.0/

[Korpora] Corpus `nsmc` is already installed at C:\Users\user\Korpora\nsmc\ratings_train.txt
[Korpora] Corpus `nsmc` is already installed at C:\Users\user\Korpora\nsmc\ratings_test.txt
```

```
train = corpus_df.sample(frac = 0.9, random_state = 42)
test = corpus_df.drop(train.index)

print(train.head(5).to_markdown())
print("Training Data Size :", len(train))
print("Testing Data Size :", len(test))
```

```python
# 결과

|       | text                                                                                     |   label |
|------:|:-----------------------------------------------------------------------------------------|--------:|
| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |
|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |
|   199 | 신날 것 없는 애니.                                                                       |       0 |
| 12447 | 잔잔 격동                                                                                |       1 |
| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |
Training Data Size : 45000
Testing Data Size : 5000
```

```python
from konlpy.tag import Okt
from collections import Counter

def build_vocab(corpus, n_vocab, special_tokens):
    counter = Counter()
    for tokens in corpus:
        counter.update(tokens)
    vocab = special_tokens
    for token, count in counter.most_common(n_vocab):
        vocab.append(token)
    return vocab

tokenizer = Okt()
train_tokens = [tokenizer.morphs(review) for review in train.text]
test_tokens = [tokenizer.morphs(review) for review in test.text]

vocab = build_vocab(corpus = train_tokens, n_vocab = 5000, special_tokens = ["<pad>", "<unk>"])
token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for idx, token in enumerate(vocab)}

print(vocab[:10])
print(len(vocab))
```

```python
# 결과

['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']
5002
```

```python
import numpy as np

def pad_sequences(sequences, max_length, pad_value):
    result = list()
    for sequence in sequences:
        sequence = sequence[: max_length]
        pad_length = max_length - len(sequence)
        padded_sequence = sequence + [pad_value] * pad_length
        result.append(padded_sequence)
    return np.asarray(result)

unk_id = token_to_id["<unk>"]
train_ids = [[token_to_id.get(token, unk_id) for token in review] for review in train_tokens]
test_ids = [[token_to_id.get(token, unk_id) for token in review] for review in test_tokens]

max_length = 32
pad_id = token_to_id["<pad>"]
train_ids = pad_sequences(train_ids, max_length, pad_id)
test_ids = pad_sequences(test_ids, max_length, pad_id)

print(train_ids[0])
print(test_ids[0])
```

```python
# 결과

[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13
 4839    1    1    1    2    0    0    0    0    0    0    0    0    0
    0    0    0    0]
[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51
    3    1 4684    6    0    0    0    0    0    0    0    0    0    0
    0    0    0    0]
```

```python
from torch.utils.data import TensorDataset, DataLoader

train_ids = torch.tensor(train_ids)
test_ids = torch.tensor(test_ids)

train_labels = torch.tensor(train.label.values, dtype = torch.float32)
test_labels = torch.tensor(test.label.values, dtype = torch.float32)

train_dataset = TensorDataset(train_ids, train_labels)
test_dataset = TensorDataset(test_ids, test_labels)

train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)
test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)
```

```python
from torch import optim

n_vocab = len(token_to_id)
hidden_dim = 64
embedding_dim = 128
n_layers = 2

device = "cuda" if torch.cuda.is_available() else "cpu"
filter_sizes = [3, 3, 4, 4, 5, 5]
classifier = SentenceClassifier(pretrained_embedding = init_embeddings, filter_sizes = filter_sizes, max_length = max_length).to(device)

criterion = nn.BCEWithLogitsLoss().to(device)
optimizer = optim.RMSprop(classifier.parameters(), lr = 0.001)
```

```python
def train(model, datasets, criterion, optimizer, device, interval):
    model.train()
    losses = list()

    for step, (input_ids, labels) in enumerate(datasets):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = model(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % interval == 0:
            print(f"Train Loss {step} : {np.mean(losses)}")

def test(model, datasets, criterion, device):
    model.eval()
    losses = list()
    corrects = list()

    for step, (input_ids, labels) in enumerate(datasets):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = model(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())
        yhat = torch.sigmoid(logits)>.5
        corrects.extend(torch.eq(yhat, labels).cpu().tolist())

    print(f"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}")

epochs = 5
interval = 500

for epoch in range(epochs):
    train(classifier, train_loader, criterion, optimizer, device, interval)
    test(classifier, test_loader, criterion, device)
```

```python
# 결과

Train Loss 0 : 0.7657092213630676
Train Loss 500 : 0.5726251596224284
Train Loss 1000 : 0.5440806578565668
Train Loss 1500 : 0.5264195766967587
Train Loss 2000 : 0.5201702850392793
Train Loss 2500 : 0.5156028683795209
Val Loss : 0.46590388087799756, Val Accuracy : 0.7796
Train Loss 0 : 0.4878423511981964
Train Loss 500 : 0.4755618774486397
Train Loss 1000 : 0.4795945630623744
Train Loss 1500 : 0.48329347814821066
Train Loss 2000 : 0.48343782004775077
Train Loss 2500 : 0.48326645239216476
Val Loss : 0.47045569055187053, Val Accuracy : 0.7662
Train Loss 0 : 0.47427207231521606
Train Loss 500 : 0.4742126032680333
Train Loss 1000 : 0.471283372837704
Train Loss 1500 : 0.4764130814503543
Train Loss 2000 : 0.4745145508687178
Train Loss 2500 : 0.47591523339347047
Val Loss : 0.45641728197804654, Val Accuracy : 0.7868
Train Loss 0 : 0.2702144384384155
Train Loss 500 : 0.4665652296083892
Train Loss 1000 : 0.46610977150105337
Train Loss 1500 : 0.4696070882159817
...
Train Loss 1500 : 0.46240111963102454
Train Loss 2000 : 0.46429794318583056
Train Loss 2500 : 0.4651765892835485
Val Loss : 0.45128062986337336, Val Accuracy : 0.7928
```

필터 크기는 위 그림과 동일한 구조인 [3, 3, 4, 4, 5, 5]의 구조를 입력합니다.

필터 크기 변수의 값과 개수를 조절해 모델의 구조를 변경하거나 확장할 수 있습니다.

최적화 함수는 순환 신경망에서 사용한 RMSProp이 아닌 Adam(Adaptive Moment Estimation)을 사요합니다.

Adam은 경사 하강법 알고리즘을 개선한 방법으로 모멘텀과 RMSProp을 결합한 방법입니다.

Adam은 이전 기울기 값의 지수 이동 평균과 이전 기울기 값의 지수 이동 제곱 평균을 사용해 현재 기울기 값을 갱신합니다.

모멘텀과 마찬가지로 지난 기울기 값의 영향을 일정 부분 유지하면서 새로운 기울기 값을 반영하기 때문에 기울기가 빠르게 변화하는 구간에서 더 안정적인 수렴을 보입니다.

RMSProp과 같이 학습률을 조절해 발산하는 경우도 방지합니다.

Adam은 학습률, 모멘텀, RMSProp의 하이퍼파라미터를 모두 자동으로 조절하면서 최적의 값으로 수렴합니다.

따라서 하이퍼파라미터를 조정하는 수고를 덜어줄 뿐 아니라, 학습률과 모멘텀의 조절 문제를 해결하고 SGD보다 더 빠른 수렴을 보이는 경우가 많습니다.

텍스트 데이터를 합성곱 신경망으로 처리하면, 각 단어를 지역적인 특징으로 인식할 수 있어 문장 내부의 구조적인 정보를 잘 파악할 수 있습니다.

또한 합성곱 신경망은 계산 속도가 빠르기 때문에 대규모 데이터세트에서도 빠르게 학습할 수 있습니다.

합성곱 신경망은 이미지와 유사한 구조를 가진 2차원 데이터에 더 적합하므로 텍스트를 합성곱 신경망으로 처리한다면 모델의 구조 및 목적을 충분히 고려해야 합니다.