<aside>
💡 Text Preprocessing(전처리)

Text Collection

</aside>

Text Preprocessing

- API를 통해 데이터 가져오기
- Noise 처리 1. 띄어쓰기 2. 맞춤법
- Tokenization 단어(공백)/음절(Vocab Size는 고정, 의미 사라짐)/**서브워드 단위(음절 단위로 쪼갠 뒤 확률을 고려해 Subword로 형성)**
- Lexical Analysis (재미있다, 재미있었다, 재미있을지도 모른다 - 동일하게 처리)(품사 연결)
    - 품사태깅 (통계 기반) N states, transition probability, initial probability
    - Markov chain을 기반으로 함 - 이전의 모든 확률을 사용해 계산하는 것봐 바로 직전의 확률만 계산해서 실행하는 것의 결과가 같다고 가정
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/a7407ed8-bdc5-4093-b0af-f8f6f73bf0a3/Untitled.png)
    
    - 0.1 * 0.6 * 0.6 *0.6
    - 0.7(이전이 없을때는 initial probability 사용) * 0.1  *  0.1 * 0.1
    - Hidden Markov Model - sequence of observation likelihood 이 추가 조건
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/76aa7083-f215-497e-b5c2-c0ecbc6de43f/Untitled.png)
        
- Stopwords 의미가 영향이 없는 단어는 제거

Text Collection

- 과거에는 텍스트 데이터를 사전, 책 등을 이용

: Google Books N-gram corpus - 구(phrase) 형태를 주로 데이터화

English-Chinese Parallel Corpora - 전문가가 직접 번역하여 구성되는 데이터 셋이 많아짐

ui - 단어 하나

bi - 단어 옆 하나

tri - 단어 3개

: DBLP (CS papers) Corpora - 컴퓨터 사이언스 한정에서 관련된 논문이나 자료들을 모아둠

: AI Hub - 대규모 데이터를 사용하고 싶을때

Text Embedding

- **One-hot Encoding** : 표현하고자 하는 **단어 인덱스는 1**, 나머지 0
    - 1. 단어 수 늘어나면, 필요 공간이 늘어남 2. 유사도 표현이 어려움
- **BoW** : 순서는 고려하지 않지만, **빈도수** 기반 표현
    - Stopword를 이용해 필요한 부분만 꺼낼 수 있음
- **DTM** : **문서**에서 등장하는 **빈도** → 행렬로 표현
    - 많은 문서 벡터가 0(Sparse Vector/Matrix),  불용어
- **TF-IDF** : 단어에 weight를 부여 - 빈도를 사용
    - TF : 문서d 안에 t의 등장 횟수 - DTM, BoW등과 비슷한 결과
    - DF(t) : 특정 단어 t가 등장한 문서 수
    - IDF : DF에 반비례(역수-로그 이용) - 가중치
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/3acadc7c-a485-4766-8928-f95b9ea9bb34/Untitled.png)
    
- **Word2Vec** : Distribute하게 표현(Vec의 크기 정할 수 있음) ↔ 희소 표현(벡터의 값 대부분이 0으로 되는 문제점)
    - Cosine similarity : 각도(세타) 두개의 벡터 차이 계산 가능
    - Jaccard similarity : 교집합 len를 이용
    - Euclidean distance :  두 점 간 직선 거리
    - Manhattan distance : |두 벡터 간의 차이|
    - Minkowski distance : 특이 케이스
    
    |  | 삼성 | 스마트폰 | 갤럭시 | 애플 | 아이폰 |
    | --- | --- | --- | --- | --- | --- |
    | x1 | 1 | 1 | 1 | 0 | 0 |
    | x2 | 0 | 1 | 0 | 1 | 1 |
    - 코사인 유사도
        - 1/3
    - 자카드 유사도
        - 1/5
    - 유클리안 거리
        - 2
    - 맨하튼 거리
        - 4
    - CBoW : 주변어(사용자가 범위 지정)를 가지고 중심어를 예측
        - 주변어들이 input이므로 하나로 모아주는 과정(Avg를 사용)이 필요함(size에 따라 연산 변경)
    - Skip-gram Model : 중심어를 통해 주변어 예측
        - 학습된 데이터는 기본적으로 One-hot Encoding
        - 학습해야 하는 size는 2 * V * D
        - 1x7  7(앞에서 계산할 수 있어야 함)x4(사용자 지정) 1x4 4x7 1x7
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/3bb773da-1dd6-48cd-8622-c54e10e78052/Untitled.png)
            
        - 타켓팅하는 부분에 대한 내적값(분자), 타켓팅하는 Wh와 나머지의 내적값(분모)를 최대화 하는 것이 목적
        - 확률값의 계산이니까 곱으로
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/9c89498f-e2e1-48a1-9604-5c9019d374ca/Untitled.png)
            
        - loss function : log 씌워서 쉽게 계산
        - gradient를 구하기
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/c34718d8-01c1-4d6c-a6b9-4ef88a670a99/Untitled.png)
        
        업데이트를 하나씩 해도 됨(한번에 같이 할 필요는 없음)
        
        분모에 있는 걸 negative sampling으로 하나만 하는 경우도 있음
        
- **LSA** : SVD(사각행렬에 대한 특이값 분해)
    - SVD는 AI수학에서 배운 개념,,,해가 없는 경우는 세로로 긴 A의 경우 list squared방법을 배운거,,(이게 뭔소리지?)
    - Truncated SVD : 유사한 값으로 분해할 수 있는 방법(알아둬야 함) n → p로 축소
    - 시그마까지 연결해서 계산하면, Dimension(Topic이라고 볼 수도 있음)과 Terms의 **관계성**이 나옴
    - 시그마랑 연결 Documents와 Dimension 길이가 줄여진 형태로 embedding이 됨**(축소된 p dimension**)
    - 시그마를 결합해서 weight를 고려해줘야 함
    - 1.73만 쓸 때는 Truncated
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/64258247-7a8c-4358-9e7f-dfb058dd5115/Untitled.png)
    
- **GloVe** : LSA & Word2Vec의 단점 극복하여 활용 - 임베딩된 중심&주변단어 내적 **동시등장 확률**
    - 문장마다 주변어와의 관계가 겹치는 경우를 빈도로 나타낸 것이 동시 등장확률, window size에 따라 바뀔 수도 있음 - 확률은 조건부 확률로 계산
    - 단어간의 연관성 확인 가능
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/0a5e2096-49f4-4586-a914-2ce19b6d927a/Untitled.png)
    
    - F(주변단어, 주변단어, 중심단어) = F(벡터간 뺄셈 후 내적)
    - Wi ↔ Wk 서로 row가 같음(주변어가 될수도 있고, 중심어가 될 수도 있음)
    - 대칭행렬의 형태를 띄므로 X↔X^T는 당연히 나와야 함
    - 준동형 사상 : 인풋에서의 계산과 아웃풋에서의 계산이 달라야 함
    ex ) exp(a+b) = exp(a) x exp(b)
    - 주변어와 중심어가 바뀌는 경우를 bias로 지정해서 학습하자
    
    ### Topic modeling
    
    모델링에 따른 모음이 나왔을때 요약하는 명명은 우리가 해야함
    
    1. LSA : 의미 분석은 앞과 비슷, 단점은 새로운 데이터가 들어오면 다시 해야함
    2. **LDA : 토픽을 구성하는 분포, 각토픽 안에 term이 나올 확률, 토픽이 랜덤 맵핑하기**
    K는 토픽의 개수, 네모는 for문 반복 의미함. 
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/150af5ca-0791-4681-94a7-f4a2d23e69f9/0daabc02-1657-463a-ada2-5cba33024565/Untitled.png)
        
        ![Untitled (9)](https://github.com/user-attachments/assets/5e771e36-7264-4686-8100-9debe55a9e5c)
        
    3. BERTopic :
