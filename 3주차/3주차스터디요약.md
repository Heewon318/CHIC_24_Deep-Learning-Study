## ❓질문1

동적 임베딩과 정적 임베딩은 어떨 때 활용될까? 

### 답변 정리

정적 임베딩은 간단한 NLP 작업(텍스트 분류, 유사성 측정 등 단어 수준의 작업)에 주로 사용되고,동적 임베딩은 복잡한 NLP 작업(문장 완성, 번역, 질문 응답 등 문맥을 깊이 이해해야 하는 작업)에 주로 사용된다.

## ❓질문2

합성곱 신경망이 순환 신경망에 비해서 왜 가중치를 줄일 수 있지?

### 답변 정리

가중치를 줄인다는 것이 수식적으로 그런 게 아니라 그냥 선택과 집중 느낌으로 3X3으로 조그마한 칸을 공부해나가는 느낌??

## ❓질문3

N-gram이 N-1개의 토큰만 사용한다는게 정확히 무슨 뜻일까? 
 
### 답변 정리

문장이 "I love machine learning and natural language processing"일 때, 각 단어마다 앞의 모든 단어를 고려하면 계산이 매우 복잡해진다. 대신, N-1개의 단어만 고려하면 훨씬 간단해진다. 예를 들어, 3-gram 모델은 "I love machine"과 "love machine learning" 등과 같이 세 단어씩 묶어서 계산한다.

## ❓질문

자기호쉬 모델과 Greedy Serch, Beem Search의 수식이 비슷한데 과연 어떤 관계일까

### 답변 정리

- Autoregressive Language Model은 각 단어의 조건부 확률을 계산하여 텍스트를 생성합니다.
- Greedy Search는 매 단계에서 가장 높은 확률을 가진 단어를 선택하여 시퀀스를 확장합니다.
- Beam Search는 여러 후보 시퀀스를 동시에 고려하여 더 나은 텍스트 생성을 목표로 합니다.

## ❓질문
TF-IDF에서 문서에서 단어들이 많이 나온다고 중요하지 않고 많이 안나온다고 중요하지 않나요?

### 답변 정리

이러한 오류가 있어 더 좋은 모델들이 나왔었다.
단어 임베딩, 문장 임베딩, 등 여러 모델들이 있습니다.

## ❓질문 
FC layer에서 3차원으로 입력을 받지 않고 flatten을 하는 이유는?

### 답변정리
FC layer의 역활로 모든 뉴런이 연결돼야 하는데 3차원 데이터를 FC하는 과정에서 관리하는 자원이 들고 복잡해지는 문제가 있는데 flatten을 사용해 1차운 데이터로 변환하면 그 자체로 모든 뉴런이 자연스럽게 연결이 되고 불필용한 중복 연결이 제거되 계산이 효율적으로 됩니다.