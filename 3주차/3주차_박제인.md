## 2부 - 자연어 처리
### 1.토큰화
#### - 토큰화는 "컴퓨터가 자연어를 이해할 수 있게 나누는 과정"이다.
#### - 토큰화를 위해 '토크나이저'를 사용한다. (*토크나이저 : 텍스트문자열을 토큰으로 나누는 소프트웨어나 알고리즘)
##### - 토크나이저 구축 방법 : ⓐ공백 분할 ⓑ정규표현식 적용 ⓒ어휘 사전 적용 ⓓ머신러닝 활용
#### - 입력된 텍스트 데이터를 단어나 글자 단위로 나누는 기법으로 '①단어 토큰화'와 '②글자 토큰화'가 있다.
##### - ①단어 토큰화 : 공백을 기준으로 분리, ②글자 토큰화 : 글자 단위로 문장 나눔(공백도 토큰으로 나뉨)
##### - ③형태소 토큰화 : 텍스트를 형태소 단위로 나눔.(*형태소 : 자립형태소와 의존형태소로 나뉨), 일반적으로 형태소 어휘 사전에는 각 형태소가 어떤 품사에 속하는지와 해당 품사의 뜻 등의 정보도 함께 제공된다.(품사 태깅)
#### - 1) (한국어) 자연어 처리 라이브러리  : KoNLPy -> 명사 추출, 형태소 분석, 품사 태깅 등의 기능 제공, Okt, 꼬꼬마, 코모란, 한나눔 등의 형태소 분석기를 지원함.
```python
!pip install konlpy
#Okt 토큰화
from konlpy.tag import Okt
okt = Okt()
sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."
nouns = okt.nouns(sentence)
phrases = okt.phrases(sentence)
morphs = okt.morphs(sentence)
pos = okt.pos(sentence)
print("명사 추출 : ", nouns)
print("구 추출 : ", pharses)
print("형태소 추출 : ", morphs)
print("품사 태깅 : ", pos)
```
#### 2. 자연어 처리 라이브러리 : NLTK -> 토큰화, 형태소 분석, 구문 분석, 개체명 인식, 감성 분석 등의 기능 제공
```python
#패키지 다운로드 및 모델 다운로드
!pip install nltk
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")
```
#### 3. 자연어 처리 라이브러리 : spaCy -> 사이썬 기반 오픈 소스 라이브러리,빠른 속도와 높은 정확도 제공
```python
#spaCy 설치
!pip install spacy
python -m spacy download en_core_web_sm
```
#### ④하위 단어 토큰화 : 하나의 단어가 빈번하게 사용되는 하위 단어의 조합으로 나눔.(하위단어 토큰화 방법 -> 바이트 페어 인코딩, 워드피스, 유니그램 모델 등)

### 2. 임베딩
#### - 컴퓨터는 토큰화만으로는 모델을 학습할 수 없기 때문에(텍스트 자체를 이해할 수 없으므로) 텍스트를 숫자로 변환하는 텍스트 벡터화 과정이 필요하다.
#### - 텍스트 벡터화 : 텍스트를 숫자로 변환하는 과정을 의미
##### - 기초적인 텍스트 벡터화로 원-핫 인코딩과 빈도 벡터화 등이 있음. -> 벡터의 희소성이 크고 컴퓨팅 비용 증가와 차원의 저주라는 문제 존재 => 워드 임베딩
##### 1) 원-핫 인코딩 : 문서에 등장하는 각 단어를 고유한 색인 값으로 맵핑한 후 해당 색인 위치를 1로 표시하고 나머지 위치는 모두 0으로 표시하는 방식
##### 2) 빈도 벡터화 : 문서에서 단어의 빈도수를 세어 해당 단어의 빈도를 벡터로 표현하는 방식
##### 3) 워드 임베딩 : 단어를 고정된 길이의 실수 벡터로 표현하는 방법, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어간의 관계 추론
##### 4) 동적 임베딩 : 단어의 의미가 문맥에 따라 변할 수 있다는 점을 반영해 단어를 벡터로 표현하는 방법, 인공 신경망 활용, 문맥에 따라 단어의 벡터 표현 달라짐. -> BERT, GPT, ELMo 등

#### 언어 모델
##### - 입력된 문장으로 각 문장을 생성할 수 있는 확률을 계산하는 모델
##### 1) 자기회귀 언어 모델 - 조건부 확률의 연쇄법칙 적용(chain rule)
##### 2) 통계적 언어 모델 - 마르코프 체인 이용(빈도 기반의 조건부 확률 모델)
###### - 가장 기초적인 통계적 언어 모델 : N-gram 
###### - Q. N-gram이 N-1개의 토큰만 사용한다는게 정확히 무슨 뜻일까? -> 문장이 "I love machine learning and natural language processing"일 때, 각 단어마다 앞의 모든 단어를 고려하면 계산이 매우 복잡해진다. 대신, N-1개의 단어만 고려하면 훨씬 간단해진다. 예를 들어, 3-gram 모델은 "I love machine"과 "love machine learning" 등과 같이 세 단어씩 묶어서 계산한한다.
##### 3) GPT
##### 4) BERT



