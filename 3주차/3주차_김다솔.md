# 자연어
자연어 처리를 위해 해결해야하는 문제
- 모호성
- 가변성
- 구조

해결 과정
1. 토큰화
- 토크나이저 사용(공백 분할, 정규표현식 적용, 어휘 사전 적용- OOV 문제 고려, 머신러닝 활용)

🤩 OOV 문제 해결 - BPE 알고리즘
- 서브 워드 분리
- 하나의 단어를 더 작은 서브 워드들의 조합으로 구성
- 신조어, 희귀 단어 문제 해결
- 참고링크: https://f7project.tistory.com/286

### 단어 토큰화
- 단어 단위로 토큰화(띠어쓰기, 문장 부호, 대소문자 기준)
- 가장 일반적임
- split method 사용
```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized = review.split()
print(tokenized)
#공백을 기준으로 분리함
```
1. 최고! vs 최고!!

느낌표 하나 차이로 다른 의미

2. cg vs cg.

cg가 단어 사전에 있더라도 cg., cg는, cg도는 OOV가 됨.
- 한국어는 접사, 문장 부호, 오타, 띄어쓰기 오류 등에 취약

### 글자 토큰화
글자 단위로 문장을 나눔(비교적 작은 단어 사전 구축) -> 컴퓨터 자원 SAVE

```
review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
tokenized2 = list(review)
print(tokenized2)
#한 글자씩으로 나뉨.
```
list를 사용하면 한글의 경우 한 글자씩, 영어의 경우 알파벳 단위로 나뉨

### 자소 단위 토큰화

```
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
```
자음 모음 단위로 분리됨.
이러면 단어 토큰화의 단점인 "cg." vs "cg" 문제도 해결할 수 있고, 단어 사전도 줄일 수 있다. 

🤨단점
1. 개별 토큰은 의미 없기에 LLM이 각 토큰의 의미를 조합해서 결과를 도출해야함. 이 때 문장 생성이나 개체명 인식 등을 할 때 다의어나 동음이의어가 많은 도메인에서의 구별 어려움
2. 모델 입력 시퀀스의 길이가 길어질수록 연산량 증가

예시> 앞서 사용된 review의 경우
- 단어 토큰화: 13개
- 글자 토큰화: 53개
- 자소 단위 토큰화: 101개

### 형태소 토큰화
- 품사 태깅을 통해 문맥을 고려해 더 정확한 분석 가능
1. KoNLPy
- Okt, Kkma, Komoran, Hannanum 등
#### Okt
okt.nouns, okt.phrases, okt.morphs, okt.pos
#### NLTK
영어 자연어 처리에 최적화됨.

### 하위 단어 토큰화
😊장점
인간이 자연어를 이해하는 방식과 가장 유사함.

🤨단점
전문용어, 고유어 많은 데이터의 취약

# 임베딩
컴퓨터는 텍스트 벡터화 과정이 필요
- 원-핫 인코딩
: 문서에 등장하는 각 단어를 고유한 색인 값으로 매핑, 해당 색인 위치는 1, 나머지는 0
- 빈도 벡터화
: 해당 단어의 빈도로 표시

장점: 단어/문장을 벡터 형태로 변환 쉽고 간단

단점: 벡터가 sparse함, 차원의 저주, 의미를 내포하고 있지 않음.

🙄차원의 저주??
- 관측치보다 변수의 수가 더 많아지는 경우 발생
- 차원이 늘어나면서 sparse해짐.(빈공간이 늘어남)
- knn 알고리즘에는 특히 안 좋음.
---
- 워드 임베딩
: 단어를 고정된 길이의 실수 벡터로 표현하는 방법, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계를 추론
- 동적 임베딩: 다의어나 문맥 정보를 다루기 쉽게 함, 인공 신경망 활용

### 언어 모델
- 자기회귀 언어 모델
:입력된 문장들의 조건부 확률을 이요해 다음에 올 단어 예측
- 통계적 언어 모델
: 언어의 통계적 구조를 이용해 문장이나 단어의 시퀀스를 생성 또는 분석함.
### N-gram
텍스트에서 n개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정함.(가장 기초적인 통계적 언어 모델)
- 입력 텍스트를 하나의 토큰 단위로 분석하지 않고 N개의 토큰을 묶어서 분석함.
- 유니그램, 바이그램, 트라이그램, N-gram
### TF-IDF
텍스트 문서에서 특정 단어의 중요도를 계산하는 방법, 문서 내 단어의 중요도를 평가하는 데 사용되는 통계적인 가중치를 의미함.

: BoW(문서나 문장을 단어의 집합으로 표현하는 방법)에 가중치를 부여하는 방법

#### 단어 빈도 TF
: 문서 내에서 특정 단어의 빈도수를 나타내는 값
- TF값이 높을 수록 해당 단어가 특정 문서에서 중요한 역할을 한다고 생각할 수도 있지만, 단어 자체가 특정 문서 내에서 자주 사용되는 단어이므로 전문 용어나 관용어로 간주할 수 있음.
-문서 길이가 길어질수록 TF 값도 높아짐.

#### 문서 빈도 DF
: 한 단어가 얼마나 많은 문서에 나타나는지를 의미함.
- 특정 단어가 많은 문서에 나타나면 문서 집합에서 단어가 나타나는 횟수를 계산함.
- DF 값이 높으면 특정 단어가 많은 문서에서 등장한다고 볼 수 있음. 해당 단어는 일반적으로 널리 사용되며, 중요도가 낮을 수 있음.
- DF 값이 낮으면 적은 수의 문서에만 등장한다는 뜻. 특정 문맥에서만 사용되는 단어일 가능성 높음 -> 중요도 높을 수 있음

#### 역문서 빈도 IDF
: 전체 문서 수를 문서 빈도로 나눈 다음에 로그를 취한 값
- 문서 내에서 특정 단어가 얼마나 중요한지를 나타냄.
- 문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는 의미가 됨.
->문서 빈도의 역수를 취해 단어의 빈도수가 적을수록 IDF 값이 커지게 보정함.
- 문서에서 특정 단어의 등장 횟수가 적으면 IDF는 상대적으로 커짐

#### TF-IDF
: 문서 빈도와 역문서 빈도를 곱한 값
TF x IDF

- 문서 내에 단어가 자주 등장하지만 전체 문서 내에 적게 등장하는 경우 TF-IDF 값은 커진다. -> 특이한 단어일 경우 값이 커짐
- 전체 문서에서 자주 등장할 확률이 높은 관사나 관용어 등의 가중치는 낮아짐.
- 사이킷런 사용함.

### 빈도 기반 벡터화의 단점
- 문장의 순서나 문맥 고려 X
-> 문장 생성과 같이 순서가 중요한 작업에는 부적합함

---
언어 모델의 역할은?
1. 예측: 감정 분석 등
2. 생성: 모델에 절이나 구 입력 시 다음 단어 예측, 문장 생성

통계적 언어 모델 vs 자기회귀 언어 모델
- 두개가 같은 거임.

자기회귀 언어 모델 vs N-gram
- 자기회귀는 이전에 등장한 모든 단어를 고려하며, 확률을 차례로 곱해나감.
- 엔그램은 모든 토큰을 사용하지 않고 사용자가 지정한 n - 1개의 토큰을 고려함
---
# Word2Vec
- 2013년 구글에서 공개한 임베딩 모델, 단어 간의 유사성을 측정하기 위해 분포가설을 기반으로 개발됨.
- 분포 가설: 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높음.
- 즉, 단어 간의 동시 발생 확률 분포를 이용해 단어 간의 유사성을 측정함.
### 단어 벡터화
1. 희소 표현
- 원-핫 인코딩, TF-IDF 등 빈도 기반 방법
- data가 sparse할 수밖에 없는 듯.
- 공간적 낭비 발생, 단어간의 유사성 반영 못 함, 벡터 간의 유사성 계산 많은 비용 발생
2. 밀집 표현
- Word2Vec
- 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기는 커지지 않음.
- 효율적인 공간 활용이 가능함.
- 단어 간의 거리를 효과적으로 계산할 수 있음
-CBoW, Skip-gram 사용

#### CBoW
- 주변에 있는 단어를 가지고 중간에 있는 단어 예측
- 중심 단어: 예측 대상
- 주변 단어: 예측에 사용되는 단어들
- 윈도: 중심 단어 예측을 위해 주변 몇 개의 단어를 고려할지를 정해야하는데 이 범위를 뜻함.
- 슬라이딩 윈도: 학습을 위해 윈도를 이동해 가며 학습하는 것
- CBoW는 슬라이딩 윈도를 통해 한 번의 학습으로 여러 개의 중심 단어와 그에 맞는 주변 단어를 학습할 수 있음.
- 입력 값: 입력 단어의 원-핫 벡터
- 하나의 윈도에서 하나의 학습 데이터가 만들어짐.

#### Skip-gram
- CBoW와 반대
- 중심단어를 입력받아 주변 단어를 예측
- 하나의 윈도에서 여러 학습 데이터가 나옴.
- 이게 더 학습 데이터가 많다보기 CBoW보다는 성능이 좋음.

# 계층적 소프트맥스
:출력층을 이진 트리 구조로 표현하여 연산 수행
- 자주 등장할 수록 상위 노드, 드물게 등장할 수록 하위 노드
- 일반 소프트맥스 연산 보다 더 빠르고 효율적
- 소프트맥스 연산의 시간 복잡도에서 log2를 씌운 값이 됨.
### 네거티브 샘플링
- Word2Vec 모델에서 사용되는 확률적 샘플링 기법
- 전체 단어 집합에서 일부 단어 샘플링하여 오답 단어로 사용
- 학습 윈도 내에 등장하지 않는 단어를 n개 추출 후 정답 단어와 함께 소프트맥스 연산 수행
- 전체 단어의 확률을 계산할 필요가 없어 모델을 효율적으로 학습할 수 있음.
- n은 보통 5~20정도
---
### 소프트맥스 word2vec vs 네거티브 word2vec
- word2vec의 등장 배경: one-hot은 단어 벡터간 유의미한 유사도를 계산할 수 없음. 원핫은 0과 1로만 구성되어서 데이터가 sparse해짐.

->단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 해야함 = word2vec, 확률로 표현함.

- 네거티브는 입력 단어의 임베딩과 해당 단어가맞는지를 판단해 1과 0으로 내적 연산을 수행함. 이 값을 시그모이드 함수에 넣어 확률값으로 변환.
- 레이블이 1이면 확률값을 높이고, 0이면 낮춘다.
 
# 순환 신경망(RNN)
- 순서가 있는 연속적 데이터에 적합
- 각 시점의 데이터가 이전 시점의 데이터와 독립적이지 않다는 특성이 더 효과적으로 만듦.
- 8/1 주가 -> 8/2 주가 -> 8/3 주가
- 자연어 데이터도 이런 특성을 지님!(t번째 단어는 ~t-1 번째까지의 단어에 영향을 받음)
- Xt는 Xt+1에게 은닉 상태값을 전달한다. 
- 출력 데이터는 총 3개의 가중치를 갖는다.(Whh, Wxh, bh)
![title](https://mblogthumb-phinf.pstatic.net/MjAyMjAxMDVfMjA0/MDAxNjQxMzU2MzczMTk5.F24JkPWEnAYA1-Mt9riYyG1xJVeF-Rv4eV8hlHu1KqMg.PVIUFMJhFsGUEBuJF7HONhUMSR31JJT_XJces8r9mEUg.PNG.yygg9800/image.png?type=w800)   
1. 일대다 구조
: 입력 1개, 출력 여러 개
- 자연어 처리) 입력: 문장 -> 출력: 단어 품사 예측
- 이미지 캡셔닝) 입력: 이미지 -> 출력: 이미지 설명
- 출력 시퀀스의 길이를 미리 알고 있어야함.(출력 시퀀스 길이 예측 모델 필요)
2. 다대일 구조
: 입력 여러 개, 출력 1개
- 감정 분류) 입력: 문장 ->출력: 감정
- 자연어 추론) 입력: 두 문장 -> 출력: 관계
- 문장 분류) 입력 시퀀스 -> 출력: 범주
3. 다대다 구조
: 입력 여러 개, 출력 여러 개
예> 번역기, 음성 인식기 등
4. 양방향 순환 신경망
: t 시점의 입력값 처리를 위해 t-1, t+1 시점의 은닉 상태 이용
5. 다중 순환 신경망
: 여러 개의 순환 신경망을 연결
- 층이 깊어질수록 더 복잡한 패턴 학습 가능 but,  시간 오래걸리고 기울기 소실 문제 발생 확률 높아짐
---
### 🙄 기울기 소실 문제
딥러닝 분야에서 layer를 많이 쌓을수록 데이터 표현력이 증가하기에 학습이 더 잘 될 것 같지만 실제로는 그렇지 않음!
- 역전파: 출력층에서 입력층으로 가중치를 업데이트 하는 거
- 역전파를 할 때 출력층에서 멀어질수록 활성화 함수의 gradient 값이 매우 작아지는 현상 -> 점점 0에 수렴하니 가중치 갱신이 이루어지지 않는다고 볼 수 있음.
1. sigmoid
![title](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FyJnMy%2FbtrCKh4ojvw%2FLtlaMwrtEZHC3KtGNWvuLK%2Fimg.png)   
sigmoid 함수의 미분값은 x가 0일 때 가장 크며 0.25이다. 여기서 x 값이 커지거나 작아지면 기울기가 거의 0에 수렴함. 게다가 e는 정확한 값으로 컴퓨터가 계산하지 않고 근사값으로 하기 때문에 점점 학습 오차도 커짐. -> sigmoid 함수를 활용하면 모델 학습이 제대로 이루어지지 않음.
2. tanh 함수
: sigmoid의 단점 개선
![title](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcq4ZGq%2FbtrCJJNA2bI%2FZhOpBc6Rj3061ZMw11AfPK%2Fimg.png)   
출력 값의 범위가 sigmoid에 비해 2배 늘어남. but, 여전히 x 값이 작아지거나 커지면 기울기가 크게 작아짐.
3. ReLU
---
## 순환 신경망 클래스

```
rnn = torch.nn.RNN(
  input_size, #x
  hidden_size, #h
  num_layers=1, #층 수
  nonlinearity="tanh", #tanh, relu 선택 가능
  bias=False, #편향 값 사용 유무
  batch_first=True, 
  dropout=0, #dropout 확률 설정
  bidirectional=False #양방향으로 할지
)
```


# 합성곱 신경망(CNN)
: 주로 이미지 인식 같은 컴퓨터비전 분야의 데이터 분석을 위해 사용됨, 데이터의 지역적인 특징 추출에 특화됨.
- 합성곱 연산: 입력값의 분포/변화량을 계산해 출력 노드 생성, 특정 영역 안에서 연산을 수행하므로 지역 특징을 효과적으로 추출할 수 있음.
- 이미지 데이터: 자유분방함 -> 지역 특징을 조합해 전역 특징 파악
- 원래는 이미지를 위해 고안했지만 자연어 처리에도 뛰어남.
- 병렬 처리 가능, 학습에 필요한 가중치를 줄여 깊은 신경망 구성
---
🙄 왜 가중치를 줄일 수 있지?
- 각 픽셀은 고유의 가중치를 가지고 있고 변하지 않음 -> 파라미터 수 감소
A. 가중치를 줄인다는 것이 수식적으로 그런 게 아니라 그냥 선택과 집중 느낌으로 3X3으로 조그마한 칸을 공부해나가는 느낌??
