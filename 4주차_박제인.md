## 1. Seq2Seq(sequence-to-sequence)

: sequence를 입력받고 sequence를 출력하는 모델.

- ‘입력 sequence’를 ‘source sequence’라고 하며
- ‘정답 sequence’를 ‘target sequence’라고 한다.
- **대표 분야**:
    - 기계 번역(Machine Translation): 번역하려는 문장을 입력받아 번역된 문장을 출력한다.
    - 챗봇: 질문을 입력받아 답변을 출력한다.
    - 내용 요약(Text Summarization), Speech to Text, Image Captioning, 말의 어투를 변경하는 등 입력된 sequence를 다른 도메인의 sequence로 변경되는 곳에 많이 쓰인다.
- **구조**
    - seq2seq를 크게 보면 many-to-many 형태라고 볼 수 있다. 입력된 sequence를 끝까지 읽고 정보를 압축한 뒤 압축된 정보를 바탕으로 출력 sequence를 구성한다. seq2seq에서 입력된 문장을 끝까지 읽고 정보를 압축하는 역할을 하는 모듈을 ‘Encoder’라고 한다. 압축된 정보를 바탕으로 출력 sequence를 만드는 역할을 하는 모듈을 ‘Decoder’라고 한다. 그리고 압축된 정보를 ‘Context vector’라고 부른다.
- **단점**
    - 한정된 길이의 벡터(마지막 시점의 hidden state vector)만을 이용해 Encoder의 입력 sequence 정보를 담기 때문에 sequence의 길이가 길수록 시작 시점에 가까운 정보가 유실될 수 있다. (= seq2seq 구조는 한정된 길이의 벡터를 이용해 입력 sequence의 모든 정보를 담기 때문에 입력 정보의 유실이 일어난다.)
    - RNN 모델이 지닌 구조적인 한계점인 long-term dependency 문제가 있을 수 있다.
    - LSTM도 long-term depedency 문제를 완벽하게 극복한 것은 아니다.
        
  
